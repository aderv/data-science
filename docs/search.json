[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ciencia de Datos",
    "section": "",
    "text": "Presentación",
    "crumbs": [
      "Presentación"
    ]
  },
  {
    "objectID": "index.html#presentación-del-curso",
    "href": "index.html#presentación-del-curso",
    "title": "Ciencia de Datos",
    "section": "Presentación del curso",
    "text": "Presentación del curso\nEl curso de ciencia de datos presenta una introducción a la teoría e implementación de métodos estadísticos para el análisis de datos. Se hace énfasis en los modelos predictivos que buscan aproximar una variable de interés \\(Y\\) (métodos supervisados) en términos de un conjunto de variables predictoras \\((\\textbf{X})\\), usando una función f que debe ser aprendida a través de un conjunto de datos \\((x_i, y_i)\\ i=1,2,...,n\\).\n\n\n\n\n\n\n\nFigura 1: Habilidades necesarias para la Ciencia de Datos\n\n\n\n\n\n\n\nSe estudian las bases conceptuales y matemáticas de los modelos y se evalúan a partir de sus propiedades estadísticas y capacidad de predecir la variable \\(Y\\). Se estudia también la parte práctica de los temas mediante la aplicación y validación de los métodos en bases de datos sintéticas y reales, usando el lenguaje R como software estadístico y de programación.\nLa ciencia de datos requiere de disciplinas y habilidades diversas, pero puede resumirse en 3 grandes aspectos (Figura 1):\n\nConocimiento en estadística y modelos matemáticos\nManejo de algoritmos y lenguajes de programación\nExperticia en algún campo específico de aplicación\n\nEl objetivo principal de este curso es desarrollar de manera formal, pero sin exceso en la teoría matemática, los modelos estadísticos en ciencia de datos. Las habilidades de programación y conocimientos básicos en los campos de aplicación se desarrollarán en la medida de lo necesario.\nResolver problemas de ciencia de datos no es un proceso lineal, pero sí se puede enmarcar en una serie de etapas que ayudan a desarrollar de manera consistente el trabajo o flujo de trabajo.\nEn la Figura 2.1 se muestra un esquema de este proceso sistemático. La primera etapa se conoce como disputa de datos (data wrangling)1 y consiste en la obtención y puesta a punto de los datos que se van a utilizar. Esta fase requiere importar, ordenar y transformar los datos, y necesita habilidades específicas en manejo de bases de datos. Las personas con alta especialización en esta área se conocen somo Ingenieros de Datos (Data Engineers).\n1 Algunos autores lo definen como una pelea con los datos\n\n\n\n\n\n\n\nflowchart LR\nsubgraph DW[Data Wrangling]\n  direction LR\n  A(Importar) --&gt; B(Ordenar)\n  B --&gt; C(Transformar)\nend\nsubgraph EDA[\"Exploratory Data Analysis (EDA)\"]\n  direction RL\n  C2(Transformar) --&gt; D(Visualizar)\n  D --&gt; C2\nend\nsubgraph MO[Modeling]\n  direction RL\n  E(Modelar) --&gt; F(Evaluar)\n  F --&gt; E\nend\nsubgraph CO[Communicate]\n  G(comunicar)\nend\n\nDW --&gt; EDA\nEDA --&gt; MO\nMO --&gt; EDA\nMO --&gt; CO\n\n\n\n\n\n\n\n\nFigura 2: Flujo de trabajo en ciencia de datos\n\n\n\nLa segunda fase es el análisis exploratorio de datos (Exploratory Data Analysis (EDA)) y consiste principalmente en la visualización a través de tablas, resúmenes y gráficas de la información que se tiene en las bases de datos que han sido previamente organizadas. Esta fase puede requerir iterar entre los procesos de transformación y visualización. No todos los problemas de ciencia de datos requieren de técnicas de machine learning y muchos de estos problemas pueden resolverse en esta fase. Las personas que se especializan en esta área se conocen como Analistas de Datos (Data Analysts).\nLa tercera fase es la Modelación y se trata de aplicar modelos estadísticos y de machine learning para obtener ideas e información a partir de los datos. Este proceso también es iterativo y puede requerir aplicar varios tipos de modelos, evaluarlos y compararlos para obtener buenos resultados. Los especialistas en esta área son los Científicos de Datos (Data Scientists).\nLa última fase del proceso es la comunicación de los resultados y hallazgos. Es importante aprender a presentar los resultados y veremos algunas herramientas básicas para lograrlo.",
    "crumbs": [
      "Presentación"
    ]
  },
  {
    "objectID": "index.html#recursos-bibliógraficos",
    "href": "index.html#recursos-bibliógraficos",
    "title": "Ciencia de Datos",
    "section": "Recursos bibliógraficos",
    "text": "Recursos bibliógraficos\nHay muchos recursos disponibles para apoyar este curso. Es fundamental usar lenguajes de programación y en este caso usaremos R, cualquier curso online, libro o tutorial puede servir para aprender los conceptos básicos, el estudiante es libre de escoger y aprender (si aún no sabe) R de una fuente de su preferencia.\nEl libro principal para los conceptos estadísticos será James et al. (2021), ya que maneja un buen balance entre aplicaciones y teoría para el bagaje matemático que se espera de los estudiantes. Este libro también trae código y aplicaciones en R, pero hay mejores fuentes en este aspecto.\nOtros 2 libros interesantes para la parte conceptual y teórica son (irizarry1?; irizarry2?; Huang y Deng, s. f.), que además ofrecen código desarrollado para aplicaciones usando R.\nEl libro de (Wickham, Çetinkaya-Rundel, y Grolemund, s. f.) es excelente para aprender a hacer ciencia de datos en R, sobre todo en la parte de análisis exploratorio de datos, tiene muchos ejemplos y código muy bien escrito. Su desventaja2 es la falta de modelos estadísticos.\n2 Desventaja en el sentido de que el libro no está pensado para hacer modelación por lo que tendremos que usar otras fuentes para esta parteUn libro que requiere algo más de madurez matemática que (James et al. 2021) es (Hastie, Tibshirani, y Friedman 2009), aunque no se usará como referencia principal, es útil para profundizar en algunos temas importantes. De igual manera, se pueden consultar las notas disponibles en línea de 2 cursos de estadística no paramétrica (García-Portugués, s. f.a) y modelos predictivos (García-Portugués, s. f.b).\nLos libros (Alexander 2023; K. Healy 2018; Kabacoff, s. f.) son excelentes referencias para profundizar en aspectos de visualización de datos usando R. También se pueden encontrar toda una serie de recursos valiosos en la web, como (Emaasit, s. f.; C. Healy y Holtz, s. f.; Holtz, s. f.).\nLos libros (James et al. 2021; Hastie, Tibshirani, y Friedman 2009) pueden obtenerse de las bases de datos de la universidad. Los libros (irizarry1?; irizarry2?; Huang y Deng, s. f.; Wickham, Çetinkaya-Rundel, y Grolemund, s. f.; García-Portugués, s. f.a, s. f.b; Alexander 2023; K. Healy 2018; Kabacoff, s. f.) están disponibles en la web y son de libre acceso.",
    "crumbs": [
      "Presentación"
    ]
  },
  {
    "objectID": "index.html#saberes-previos",
    "href": "index.html#saberes-previos",
    "title": "Ciencia de Datos",
    "section": "Saberes previos",
    "text": "Saberes previos\nPara un óptimo desarrollo de los contenidos del curso, se espera que los estudiantes tengan habilidades básicas en programación algorítmica con cualquier lenguaje. Se usará R pero la experiencia en cualquier otro lenguaje es suficiente para aprender rápidamente las herramientas necesarias.\nDe igual forma es necesario el manejo de conceptos matemáticos de funciones, derivadas y optimización en una y varias variables tal como se ve en los cursos de cálculo; conceptos de variables aleatorias, estimadores, intervalos de confianza, pruebas de hipótesis y regresión lineal de los cursos de Probabilidad y Estadística y Modelos de Regresión y Series de Tiempo. Es recomendable que el estudiante se sienta cómodo en la manipulación de matrices y sistemas de ecuaciones lineales como se ve en el curso de Álbegra Lineal.",
    "crumbs": [
      "Presentación"
    ]
  },
  {
    "objectID": "index.html#objetivos",
    "href": "index.html#objetivos",
    "title": "Ciencia de Datos",
    "section": "Objetivos",
    "text": "Objetivos\n\nComprender la teoría y los conceptos en los que se fundamentan los métodos estudiados\nIdentificar problemas en los que se puedan aplicar de forma adecuada modelos predictivos de ciencia de datos\nAdquirir pericia en las diferentes fases de la ciencia de datos: importación, organización, transformación, visualización, modelación y comunicación\nAplicar en problemas reales los conceptos aprendidos, mediante el uso de lenguajes de programación y bases de datos\nAnalizar y comunicar de manera clara los resultados de la aplicación de modelos estadísticos en bases de datos",
    "crumbs": [
      "Presentación"
    ]
  },
  {
    "objectID": "index.html#metodología",
    "href": "index.html#metodología",
    "title": "Ciencia de Datos",
    "section": "Metodología",
    "text": "Metodología\nEl contenido del curso será presentado en 2 sesiones presenciales por semana. Durante las clases se irán desarrollando de manera simultánea conceptos y código en R, por lo que es necesario estar en salas de informática.\nEl estudiante es responsable de trabajar semanalmente en el curso para afianzar los temas dados. se realizarán exámenes cortos3 periódicamente para evaluar la evolución de cada estudiante en la asimilación de los temas. Se dejarán talleres/tareas para estimular la investigación y profundización de los temas vistos durante las clases.\n3 No es necesario que estas evaluaciones rápidas o quices sean programadas o avisadas con anticipación, pueden realizarse en cualquier clase y en cualquier momento de la clase. Si \\(p_k\\) es la probabilidad de haya quiz en la k-ésima clase, entonces \\(p_k&gt;0 \\ \\forall k\\)",
    "crumbs": [
      "Presentación"
    ]
  },
  {
    "objectID": "index.html#plan-de-trabajo-por-semana",
    "href": "index.html#plan-de-trabajo-por-semana",
    "title": "Ciencia de Datos",
    "section": "Plan de trabajo por semana",
    "text": "Plan de trabajo por semana\n\n\n\n\n\nSemana\nInicio\nTemas\n\n\n\n\n1\njulio 15\nIntroducción al curso. Análisis exploratorio, parte I\n\n\n2\njulio 22\nAnálisis exploratorio, parte II\n\n\n3\njulio 29\nManejo de bases de datos. Estadística descriptiva y probabilidad\n\n\n4\nagosto 05\nModelos estadísticos, función de pérdida. Medidas de la calidad de un modelo. Sesgo y varianza. Regresión lineal\n\n\n5\nagosto 12\nModelos lineales de clasificación. Análisis discriminante lineal. Examen 1\n\n\n6\nagosto 19\nRegresión logística\n\n\n7\nagosto 26\nValidación cruzada (CV)\n\n\n8\nseptiembre 02\nIntroducción a modelos no lineales. Regresión polinómica y en bases funcionales. Splines, Smoothing splines\n\n\n9\nseptiembre 09\nMétodos basados en árboles, parte I. Examen 2\n\n\n10\nseptiembre 16\nMétodos basados en árboles, parte II\n\n\n11\nseptiembre 23\nMétodos basados en árboles, parte III\n\n\n12\nseptiembre 30\nMétodos basados en árboles, parte IV\n\n\n13\noctubre 07\nAprendizaje NO supervisado, parte I. Examen 3\n\n\n14\noctubre 14\nAprendizaje NO supervisado, parte II\n\n\n15\noctubre 21\nAprendizaje NO supervisado, parte III\n\n\n16\noctubre 28\nAprendizaje NO supervisado, parte IV",
    "crumbs": [
      "Presentación"
    ]
  },
  {
    "objectID": "index.html#evaluación",
    "href": "index.html#evaluación",
    "title": "Ciencia de Datos",
    "section": "Evaluación",
    "text": "Evaluación\nLa nota final se calculará como el promedio ponderado de 4 notas parciales tal como se detalla en la siguiente tabla\n\n\n\n\n\n\n\n\n\n\nCortes\nComponentes\n\n\n\n\nPrimer corte (25%)\nEvaluaciones de Seguimiento, quices, talleres, exposiciones, trabajos (10%)\nExamen 1 (15%)\n\n\nSegundo corte (25%)\nEvaluaciones de Seguimiento, quices, talleres, exposiciones, trabajos (10%)\nExamen 2 (15%)\n\n\nTercer corte (25%)\nEvaluaciones de Seguimiento, quices, talleres, exposiciones, trabajos (10%)\nExamen 3 (15%)\n\n\nCuarto corte (25%)\nEvaluaciones de Seguimiento, quices, talleres, exposiciones, trabajos (10%)\nExamen final (15%)",
    "crumbs": [
      "Presentación"
    ]
  },
  {
    "objectID": "index.html#sobre-el-reglamento",
    "href": "index.html#sobre-el-reglamento",
    "title": "Ciencia de Datos",
    "section": "Sobre el Reglamento",
    "text": "Sobre el Reglamento\n\nEs necesario que el estudiante participe como mínimo en el 80% del desarrollo del curso (clases). La inasistencia será causal de pérdida del curso con una calificación de cero punto cero cero (0.00), sin posibilidad de cancelación del curso, si la inasistencia fue reportada previamente al sistema de información.\nLa inasistencia por razones de enfermedad o fuerza mayor deberá justificarse dentro de los tres (3) días hábiles siguientes al hecho. Cuando el estudiante requiera ausentarse para eventos académicos, científicos, culturales y deportivos, en representación de la Universidad, deberá reportar este hecho al profesor, como mínimo con ocho (8) días hábiles de anticipación.\nEn cualquiera de las clases se pueden hacer actividades evaluativas de seguimiento (quices, talleres cortos, ejercicios en clase), con o sin programación previa.\nLa no presentación de una evaluación en la fecha y hora fijada por el profesor conlleva una nota de 0.00 (cero punto cero cero).\nCuando, por causas justificadas, no se puedan presentar las evaluaciones parciales o finales en las fechas establecidas por el docente, se podrán solicitar pruebas supletorias ante el director de la Facultad. Estas pruebas tendrán un costo y deberán ser solicitadas dentro de los tres (3) días hábiles siguientes a la programación de la prueba no presentada por el estudiante y, si es autorizada, deberá ser presentada dentro de los 10 días hábiles siguientes a su autorización.\nLos resultados de las evaluaciones deberán ser informados al estudiante, a más tardar, dentro de las dos (2) semanas siguientes a su presentación.\n\n\n\n\n\nAlexander, Rohan. 2023. Telling Stories with Data. CRC Press. https://tellingstorieswithdata.com/.\n\n\nEmaasit, Daniel. s. f. «ggplot2 Extensions». https://exts.ggplot2.tidyverse.org/gallery/.\n\n\nGarcía-Portugués, Eduardo. s. f.a. Notes for Nonparametric Statistics. https://bookdown.org/egarpor/NP-UC3M/.\n\n\n———. s. f.b. Notes for Predictive Modeling. https://bookdown.org/egarpor/PM-UC3M/.\n\n\nHastie, Trevor, Robert Tibshirani, y Jerome Friedman. 2009. The Elements of Statistical Learning. Springer Series en Statistics. New York, NY: Springer. https://doi.org/10.1007/978-0-387-84858-7.\n\n\nHealy, Conor, y Yan Holtz. s. f. «From data to Viz  Find the graphic you need». https://www.data-to-viz.com/data-to-viz.com.\n\n\nHealy, Kieran. 2018. Data Visualization. A Practical Introduction. Princeton University Press. https://socviz.co/index.html#preface.\n\n\nHoltz, Yan. s. f. «The R Graph Gallery – Help and inspiration for R charts». The R Graph Gallery. https://r-graph-gallery.com/index.html.\n\n\nHuang, Shuai, y Houtao Deng. s. f. Data Analytics: A Small Data Approach. CRC Press. https://dataanalyticsbook.info/index.html#cover.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, y Robert Tibshirani. 2021. An Introduction to Statistical Learning: with Applications in R. 2.ª ed. Springer Texts en Statistics. New York, NY: Springer US. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nKabacoff, Rob. s. f. Data Visualization with R. https://rkabacoff.github.io/datavis/.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, y Garrett Grolemund. s. f. R for Data Science (2e). 2.ª ed. O’REILLY. https://r4ds.hadley.nz/.",
    "crumbs": [
      "Presentación"
    ]
  },
  {
    "objectID": "r-intro.html",
    "href": "r-intro.html",
    "title": "1  Introducción al uso de R",
    "section": "",
    "text": "1.1 Instalación\nPara trabajar con R, es necesario instalar el lenguaje en sí y un IDE (por sus siglas en inglés, Integrated Development Environment). Un IDE es una herramienta que facilita el trabajo con el lenguaje de programación, hay varias opciones para R, usaremos Rstudio.\nSiga los siguientes pasos para la instalación:\nSe recomienda instalarlo en el orden dado para que Rstudio reconozca automáticamente la versión de R ya instalada. Cuando esté todo listo, se ejecuta la aplicación de Rstudio, desde donde haremos todo el trabajo.\nEn Rstudio: file -&gt; New File -&gt; R script (atajo: ctrl+shift+N), abrirá un editor en blanco para empezar a codificar. Al hacerlo, debe verse como en la imagen\nGuarde su trabajo desde el menú: File -&gt; Save As; o con: ctrl+S.",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al uso de `R`</span>"
    ]
  },
  {
    "objectID": "r-intro.html#instalación",
    "href": "r-intro.html#instalación",
    "title": "1  Introducción al uso de R",
    "section": "",
    "text": "Descargar e instalar R según el sistema operativo que se tenga.\nDescargar e instalar la versión de Rstudio Desktop\n\n\n\n\n\n\n\n\n\nFigura 1.1: Rstudio IDE",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al uso de `R`</span>"
    ]
  },
  {
    "objectID": "r-intro.html#lo-básico",
    "href": "r-intro.html#lo-básico",
    "title": "1  Introducción al uso de R",
    "section": "1.2 Lo básico",
    "text": "1.2 Lo básico\n\nR es un lenguaje de tipado dinámico y de alto nivel (o sea, fácil de aprender)\nEl operador de asignación es &lt;-. Atajo de teclado: Alt + -. Asignación es el proceso mediante el cual guardamos los objetos que creamos en espacios de memoria o variables\nEs posible usar también el signo igual (=) como operador de asignación, aunque en algunos pocos casos particulares puede generar confusiones, por lo que es recomendable usar &lt;-\nPara agregar comentarios al código use el signo #. Todo lo que escriba en R, después de un signo #, será ignorado. Sirve para comentar el código\nPuede ejecutar una línea o un subconjunto de líneas seleccionadas del código con ctrl + enter. Sitúe el cursor en cualquier parte de la línea a ejecutar o seleccione las líneas deseadas. Dependiendo de las líneas ejecutadas, puede encontrar un resultado impreso en la consola, una gráfica, unas variables agregadas al entorno, etc.\nLos nombres de las variables deben empezar con una letra1 y luego cualquier carácter alfanumérico. Los únicos caracteres especiales permitidos son el punto (.) y el guión bajo (_), útiles cuando se quiere usar nombres compuestos\nLos nombres de variables no deben contener espacios\n\n1 También pueden empezar con un . pero esto crea objetos ocultosEjemplos de asignación de valores a variables:\n\na1 &lt;- 56 # numeric\nb = 12.3 # numeric\nb = b + 1\na &lt;- \"hola\" # character\nx_1 &lt;- 3L # integer\nx2 &lt;- FALSE # logical\nx.3 &lt;- 1 + 3i # complex\nvel_inicial &lt;- 47.31 # Velocidad inicial en km/h\n\nAlgunos consejos:\n\nUsar nombres relacionados con lo que se está codificando para ayudar a entender mejor el código. Por ejemplo, si se está resolviendo un problema en física, donde se necesita almacenar el valor de la velocidad inicial de una partícula, es mejor opción velocidad_incial &lt;- 100, en lugar de var1 &lt;- 100\nNo usar mayúsculas (cuestión de gustos)\nNo usar acentos (tildes)\nComentar comentar comentar, para que cualquiera que lea su código, pueda entenderlo con más facilidad\n\n\nvelocidad &lt;- 80 # velocidad en km/h\nvel_inicial &lt;- 45 # velocidad inicial en km/h",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al uso de `R`</span>"
    ]
  },
  {
    "objectID": "r-intro.html#tipos-de-datos-en-r",
    "href": "r-intro.html#tipos-de-datos-en-r",
    "title": "1  Introducción al uso de R",
    "section": "1.3 Tipos de datos en R",
    "text": "1.3 Tipos de datos en R\n\nCharacter\nNumeric\nInteger\nComplex\nLogical\n\n\nx1 &lt;- 'UPB Montería' # las cadenas de texto deben ir entre comillas simples (') o dobles (\")\nclass(x1)  # devuelve la clase del objeto x1\n## [1] \"character\"\n\npi # pi es una constante pi = 3.1415...\n## [1] 3.141593\n\nx2 &lt;- pi    \nclass(x2)\n## [1] \"numeric\"\nx3 &lt;- \"AB\" # character\nx4 &lt;- 6L # integer\nclass(x4)\n## [1] \"integer\"\nx5 &lt;- 3.4 + 9.3i # complex\nclass(x5)\n## [1] \"complex\"\nlogico &lt;- FALSE # logical\nclass(logico)\n## [1] \"logical\"",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al uso de `R`</span>"
    ]
  },
  {
    "objectID": "r-intro.html#vectores",
    "href": "r-intro.html#vectores",
    "title": "1  Introducción al uso de R",
    "section": "1.4 Vectores",
    "text": "1.4 Vectores\nUn vector es un arreglo unidimensional que puede almacenar datos de un mismo tipo. Podemos tener un vector numérico, por ejemplo, lo que quiere decir que tenemos un arreglo que almacena solo datos de tipo numérico. No se pueden almacenar datos de diferente tipo en un vector, es decir, no puede tener en R un vector con valores numéricos y lógicos mezclados.\nPara crear un vector, se usa la función c() (es una “c” minúscula, R distingue entre mayúsculas y minúsculas). Ejemplos:\n\nvector1 &lt;- c(1,5,7,4) # Vector de solo valores numéricos\nvlog &lt;- c(T, F, F, F, T) # vector de solo valores lógicos\nvchar &lt;- c(\"c\", \"i\", \"e\", \"n\", \"c\", \"i\", \"a\") # vector de caracteres\nvector1; vlog; vchar # imprime los vectores creados\n## [1] 1 5 7 4\n## [1]  TRUE FALSE FALSE FALSE  TRUE\n## [1] \"c\" \"i\" \"e\" \"n\" \"c\" \"i\" \"a\"\n\nSi intentamos almacenar elementos de diferente tipo en un vector, R intentará hacer una coerción a un tipo de dato adecuado.\n\nc(1,2,3,\"hola\") # creará un vector de caracteres\n## [1] \"1\"    \"2\"    \"3\"    \"hola\"\nc(1,2,3,FALSE,TRUE) # creará un vector numérico. T equivale a 1 y F a 0\n## [1] 1 2 3 0 1\n\nHay muchas formas adicionales a la función c() para obtener o crear vectores en R. Algunas formas comunes son:\n\n# una suceción de números enteros desde el 10 hasta el 20 \n# almacenando el resultado en un vector llamado suc1\nsuc1 &lt;- 10:20 \n\n# con la función vector()\nvec1 &lt;- vector(mode = \"numeric\", length = 15)\nvec2 &lt;- vector(\"logical\", 6)\nvec3 &lt;- vector(length=6, mode=\"logical\")\nsuc1; vec1; vec2; vec3\n##  [1] 10 11 12 13 14 15 16 17 18 19 20\n##  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n## [1] FALSE FALSE FALSE FALSE FALSE FALSE\n## [1] FALSE FALSE FALSE FALSE FALSE FALSE\n\nLas funciones como vector pueden usarse sin declarar de forma explícita los argumentos, solo asegúrese de poner los valores en el orden correcto.\nMás ejemplos:\n\n23:15\n## [1] 23 22 21 20 19 18 17 16 15\n# con la función seq() para crear vectores a partir de secuencias\nseq(from = 5, to = 10) # secuencia que empieza en 5 y va hasta el 10 de uno en uno\n## [1]  5  6  7  8  9 10\n\n# una suceción que empieza en 5 y aumenta en pasos de 0.2 hasta llegar a 10\nseq(from = 5, to = 10, by = 0.2)\n##  [1]  5.0  5.2  5.4  5.6  5.8  6.0  6.2  6.4  6.6  6.8  7.0  7.2  7.4  7.6  7.8\n## [16]  8.0  8.2  8.4  8.6  8.8  9.0  9.2  9.4  9.6  9.8 10.0\n\n# desde el 5 hasta el 10, en pasos iguales que permitan un total de 12 números\nseq(from = 5, to = 10, length.out = 12)\n##  [1]  5.000000  5.454545  5.909091  6.363636  6.818182  7.272727  7.727273\n##  [8]  8.181818  8.636364  9.090909  9.545455 10.000000\n\n# con la función rep() para repetir un objeto, n cantidad de veces\nrep(1.45, 6) # repite el número 1.45 6 veces y genera un vector\n## [1] 1.45 1.45 1.45 1.45 1.45 1.45\nrep(c(1,3), 7) # repite el vector c(1,3) 7 veces\n##  [1] 1 3 1 3 1 3 1 3 1 3 1 3 1 3\n\nMuchas funciones y procedimientos en R, devuelven sus resultados en vectores. Por ejemplo, es posible generar números aleatorios de forma muy sencilla y guardarlos en un vector\n\nx &lt;- runif(n=100) # runif() genera números uniformemente distribuidos (entre 0 y 1 por defecto)\n\n# 20 números uniformes entre -3.7 y -1\ny &lt;- runif(n=100, min=-3.7, max=-1) # por supuesto: min &lt; max\n\n# números con distribución normal\nz &lt;- rnorm(n = 100, mean = 2.1, sd = 0.4)\n\nplot(x, y) # gráfico de dispersión\n\n\n\n\n\n\n\nhist(z) # histograma\n\n\n\n\n\n\n\n\nPara acceder a los elementos de un vector se usan los corchetes []\n\nx &lt;- rnorm(n=20, mean = 0, sd = 2) # vector con 20 números\nx # imprime el vector\n##  [1]  0.31111021 -1.28932691 -0.63783126  2.73281460 -3.24308477  0.61290631\n##  [7] -4.60754847 -2.89751587  0.83773948 -0.01509477 -0.73674283 -1.23449014\n## [13] -2.65829987 -0.70493001 -1.39230755 -0.09070774  1.38974548  0.27562085\n## [19] -0.32799147 -1.36002823\nx[17] # elemento 17\n## [1] 1.389745\nx[3:10]  # elementos del 3 al 10 inclusive \n## [1] -0.63783126  2.73281460 -3.24308477  0.61290631 -4.60754847 -2.89751587\n## [7]  0.83773948 -0.01509477\nx[-5]  # todos los elementos menos el quinto\n##  [1]  0.31111021 -1.28932691 -0.63783126  2.73281460  0.61290631 -4.60754847\n##  [7] -2.89751587  0.83773948 -0.01509477 -0.73674283 -1.23449014 -2.65829987\n## [13] -0.70493001 -1.39230755 -0.09070774  1.38974548  0.27562085 -0.32799147\n## [19] -1.36002823\nx[c(1,6,4)]  # elementos 1, 6 y 4\n## [1] 0.3111102 0.6129063 2.7328146\nx[-c(1,6,4)]  # todos los elementos excepto el 1, 6 y 4\n##  [1] -1.28932691 -0.63783126 -3.24308477 -4.60754847 -2.89751587  0.83773948\n##  [7] -0.01509477 -0.73674283 -1.23449014 -2.65829987 -0.70493001 -1.39230755\n## [13] -0.09070774  1.38974548  0.27562085 -0.32799147 -1.36002823",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al uso de `R`</span>"
    ]
  },
  {
    "objectID": "r-intro.html#matrices",
    "href": "r-intro.html#matrices",
    "title": "1  Introducción al uso de R",
    "section": "1.5 Matrices",
    "text": "1.5 Matrices\nUna matriz es un arreglo bidimensional para almacenar datos de un mismo tipo (igual que un vector, pero ahora en filas y columnas). Hay varias formas de crear matrices. Se puede usar la función matrix()\n\n# matriz de 3 filas y columnas, si no se asignan valores\n# se llenará cada posición con NA\nmm &lt;- matrix(nrow = 3, ncol = 4)\n\n## Matriz de 3 filas y 4 columnas, cuyos valores están en un vector\n## de números aleatorios uniformemente distribuidos\nmm1 &lt;- matrix(data=runif(12,0,1), nrow = 3, ncol = 4)\nmm1\n##           [,1]      [,2]        [,3]      [,4]\n## [1,] 0.8957262 0.8582433 0.301976190 0.9588218\n## [2,] 0.7463191 0.2704070 0.004278304 0.7268614\n## [3,] 0.9618773 0.3274588 0.399206386 0.1620095\n\n# Por defecto se llenará la matriz por columnas\nmm2 &lt;- matrix(data = c(1,2,3,4,5,6,7,8,9), nrow = 3)\nmm2\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n# note que no especificamos el número de columnas, pero R es \"inteligente\"\n# y sabe que necesita 4 columnas para crear una matriz de 3 filas con 12 elementos\n\n# Si desea \"armar\" la matriz por filas, agregue byrow = TRUE\nmm3 &lt;- matrix(data = c(1,2,3,4,5,6,7,8,9), nrow = 3, byrow = TRUE)\nmm3\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    4    5    6\n## [3,]    7    8    9\n\n# Para acceder a los elementos, indique el número de la fila y/o columna\nmm3[3,2]  #elemento (3,2): fila 3, columna 2\n## [1] 8\nmm3[2,]  # toda la fila 2 (un vector)\n## [1] 4 5 6\nmm3[,3]  # toda la columna 3 (un vector)\n## [1] 3 6 9\n\nSe pueden combinar vectores por filas o por columnas para formar matrices\n\nsuc1 &lt;- 1:9\nsuc2 &lt;- 2:10\nmm4 &lt;- cbind(suc1, suc2) # se combina por columnas\nmm5 &lt;- rbind(suc1, suc2) # se combina por filas\nmm4; mm5 # Imprime las matrices en la consola\n##       suc1 suc2\n##  [1,]    1    2\n##  [2,]    2    3\n##  [3,]    3    4\n##  [4,]    4    5\n##  [5,]    5    6\n##  [6,]    6    7\n##  [7,]    7    8\n##  [8,]    8    9\n##  [9,]    9   10\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\n## suc1    1    2    3    4    5    6    7    8    9\n## suc2    2    3    4    5    6    7    8    9   10",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al uso de `R`</span>"
    ]
  },
  {
    "objectID": "r-intro.html#data-frame",
    "href": "r-intro.html#data-frame",
    "title": "1  Introducción al uso de R",
    "section": "1.6 Data frame",
    "text": "1.6 Data frame",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al uso de `R`</span>"
    ]
  },
  {
    "objectID": "r-intro.html#listas",
    "href": "r-intro.html#listas",
    "title": "1  Introducción al uso de R",
    "section": "1.7 Listas",
    "text": "1.7 Listas\nLos vectores y matrices están formados por un único tipo de datos, las listas no tienen esta limitación y se pueden crear con la función list(). Los elementos de una lista pueden ser: numéricos, enteros, lógicos, texto; pueden contener otras estructuras como vectores, matrices, y otras listas.\n\n# una lista que contiene un número entero, un caracter, un número real\n# un vector, una matriz, y otra lista. \nlista1 &lt;- list(3L, \"DF\", pi, c(1.34,2,3), mm1, list(2,\"ss\"))\nlista1\n## [[1]]\n## [1] 3\n## \n## [[2]]\n## [1] \"DF\"\n## \n## [[3]]\n## [1] 3.141593\n## \n## [[4]]\n## [1] 1.34 2.00 3.00\n## \n## [[5]]\n##           [,1]      [,2]        [,3]      [,4]\n## [1,] 0.8957262 0.8582433 0.301976190 0.9588218\n## [2,] 0.7463191 0.2704070 0.004278304 0.7268614\n## [3,] 0.9618773 0.3274588 0.399206386 0.1620095\n## \n## [[6]]\n## [[6]][[1]]\n## [1] 2\n## \n## [[6]][[2]]\n## [1] \"ss\"\n\nPara acceder a sus elementos se usan dobles corchetes [[]]\n\nlista1[[2]] # segundo elemento de la lista\n## [1] \"DF\"\n\n# Primer elemento del vector que está el el lugar 4 de la lista (¡reloco!)\nlista1[[4]][1]  \n## [1] 1.34\n\nR es un lenguaje muy versátil y trae por defecto muchas opciones de cálculo. Además de eso, es posible ampliar la gama de operaciones o actividades que se pueden realizar con la instalación de paquetes o librerías adicionales. Es un lenguaje bastante popular y con una comunidad muy activa. Si tiene algún problema con algún código, muy seguramente otra persona ya tuvo el mismo problema antes y se puede encontrar la solución en línea. Cuando tenga un resultado inesperado puede, copiar y pegar el error en Google para buscar soluciones.",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al uso de `R`</span>"
    ]
  },
  {
    "objectID": "r-intro.html#valores-faltantes",
    "href": "r-intro.html#valores-faltantes",
    "title": "1  Introducción al uso de R",
    "section": "1.8 Valores faltantes",
    "text": "1.8 Valores faltantes\nUn vector de valores lógicos puede usarse para extraer elementos de otro vector. Se extraerá cada elemento en el que haya un valor TRUE\n\nxx &lt;- 20:26 # vector con 7 elementos\nind &lt;- c(T, T, F, F, T, T, T)\nxx[ind]  # extrae las posiciones donde hay TRUE: 1,2,5,6,7\n## [1] 20 21 24 25 26\n\nTipos de datos especiales:\n\nNA: Not Available\nNaN: Not a Number\n\n\n2/0\n## [1] Inf\n0/0\n## [1] NaN\n\nCon alguna combinación de funciones podemos sacar datos NA y/o NaN de un vector\n\nzz &lt;- c(1,2,NA,Inf,NA, NaN, NA, 4, NA)\n\n# devuelve un vector con elementos TRUE donde hay valores NA o NaN\nss &lt;- is.na(zz) \n\n# Vector con elementos TRUE donde hay valores NaN\nss1 &lt;- is.nan(zz) \n\n# con el operador ! cambiamos TRUE por FALSE y viceversa\nssneg &lt;- !ss # en ssneg hay TRUE donde no hay valores NA o NaN\n\n# extrae valores donde ssneg es TRUE, es decir donde no hay valores\n# NA o NaN en zz (¡qué enredo tan bueno!)\nzzlimpio &lt;- zz[ssneg] \nzz; zzlimpio # imprimir y comparar\n## [1]   1   2  NA Inf  NA NaN  NA   4  NA\n## [1]   1   2 Inf   4",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al uso de `R`</span>"
    ]
  },
  {
    "objectID": "r-intro.html#paquetes-o-librerías",
    "href": "r-intro.html#paquetes-o-librerías",
    "title": "1  Introducción al uso de R",
    "section": "1.9 Paquetes o librerías",
    "text": "1.9 Paquetes o librerías\nHay paquetes para hacer casi cualquier cosa en R. Aquí puede econttrar una lista de los paquetes disponibles para instalación directa desde la consola de R (hay más de 20.000).\nPara instalar y luego cargar paquetes:\n\ninstall.packages(\"nombre_paquete\"): solo se hace una vez, por lo que no es necesario incluirlo en el editor (la parte donde se escribe el código) sino que se puede realizar una única vez en la consola (la parte inferior izquierda de Rstudio donde se imprimen los resultados)\nlibrary(nombre_paquete): debe hacerse siempre que se inicie una nueva sesión y se vaya a usar el paquete, normalmente se cargan las librerías a usar al comienzo del código",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al uso de `R`</span>"
    ]
  },
  {
    "objectID": "r-intro.html#más-vectores",
    "href": "r-intro.html#más-vectores",
    "title": "1  Introducción al uso de R",
    "section": "1.10 Más vectores",
    "text": "1.10 Más vectores\n\nx1 &lt;- rep(0,20)\nx2 &lt;- seq(from=0, to=2, length.out=5)\nx3 &lt;- seq(from=0, to=2, by=0.2)\nx4 &lt;- 3:12\n\n# La función sample() es bastante útil\n# Extrae una muestra tamaño 3, del vector x4\nx5 &lt;- sample(x4, size = 3, replace = F) # sin reemplazo\nx6 &lt;- sample(x4, size = 15, replace = T) # con reemplazo\nx1; x2; x3; x4; x5; x6\n##  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n## [1] 0.0 0.5 1.0 1.5 2.0\n##  [1] 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0\n##  [1]  3  4  5  6  7  8  9 10 11 12\n## [1]  7 10 12\n##  [1]  9 11  8  4 12  6  6 12  4 11  5  3 11  6 12\n\nCuando se trabaja con números aleatorios es posible establecer una semilla, para poder reproducir los mismos resultados en diferentes computadores\n\n# se establece la semilla, esto permite que los números \n# generados sean iguales en otros computadores, \n# siempre que tengan la misma semilla establecida\nset.seed(234) \nx7 &lt;- rnorm(1000, mean = 0, sd=1)\nhist(x7)\n\n\n\n\n\n\n\nx8 &lt;- c(\"c\", \"i\", \"e\", \"n\", \"c\", \"i\", \"a\")\nx9 &lt;- sample(x8, size = 15, replace = T)\nx9\n##  [1] \"c\" \"i\" \"e\" \"i\" \"c\" \"c\" \"e\" \"e\" \"c\" \"n\" \"i\" \"i\" \"e\" \"i\" \"c\"",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al uso de `R`</span>"
    ]
  },
  {
    "objectID": "r-intro.html#algunas-funciones-básicas-para-usar-con-vectores",
    "href": "r-intro.html#algunas-funciones-básicas-para-usar-con-vectores",
    "title": "1  Introducción al uso de R",
    "section": "1.11 Algunas funciones básicas para usar con vectores",
    "text": "1.11 Algunas funciones básicas para usar con vectores\n\n\nmin_x7 &lt;- min(x7) # devuelve el valor mínimo en el vector\nmax_x7 &lt;- max(x7) # devuelve el valor máximo en el vector\n\n# devuelve la posición del vector donde se encuentra el valor mínimo\ndonde_min &lt;- which.min(x7) \n# devuelve la posición del vector donde se encuentra el valor máximo\ndonde_max &lt;- which.max(x7)\n\nmin_x7; donde_min\n## [1] -3.03609\n## [1] 8\nmax_x7; donde_max\n## [1] 3.096502\n## [1] 528\n\nPara borrar objetos de la memoria:\n\nrm(ind, logico, max_x7) # borra los objetos ind y max_x7\nrm(list = ls()) # borra todo",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al uso de `R`</span>"
    ]
  },
  {
    "objectID": "r-intro.html#funciones",
    "href": "r-intro.html#funciones",
    "title": "1  Introducción al uso de R",
    "section": "1.12 Funciones",
    "text": "1.12 Funciones\nR está optimizado para trabajar con funciones. Una función es un conjunto de líneas de código que se guardan en memoria y que se puede llamar siempre que sea necesario en otras partes del código, sin necesidad de volver a escribir todo lo que hace dicha función\nLa sintaxis básica para crear una función en R es\nnombre_funcion &lt;- function(&lt;argumentos&gt;){\n  Hacer algo con los argumentos\n  devuelva algún resultado\n}\nPor ejemplo\n\nfuncion_1 &lt;- function(x) x^4 + 6*x + 3\nfuncion_1(3)\n## [1] 102\nplot(funcion_1, 0, 10) #grafica la función en el intervalo especificado\n\n\n\n\n\n\n\n\nSi las operaciones que hace la función se declaran en una sola línea, no es necesario usar las llaves {}. Si las declaraciones dentro de la función ocupan varias líneas, entonces sí es necesario usar las llaves2\n2 Casi siempre es necesario, las funciones no suelen ser tan simples para declararse en una sola línea de códigoFunciones de 2 variables\n\nfuncion_2 &lt;- function(x,y){\n  return(x*y + 2*x + log(abs(x+y+1)))\n}\nfuncion_2(2,3)\n## [1] 11.79176\n\nEl concepto de función en R no está limitado a funciones matemáticas tal como estamos acostumbrados, los argumentos de las funciones pueden ser: números, vectores, matrices, listas, otras funciones\n\n## f  una función\n## x es un vector\n## y es una matriz\n\nfuncion_3 &lt;- function(f,x,y){\n  return(f(x) + x*y[,1]) # el resultado debe ser un vector (¿por qué?)\n}\n\nxxxx &lt;- 1:5\nyyyy &lt;- matrix(data = 1:30, nrow = 5, byrow = TRUE)\nfuncion_3(f = funcion_1, x = xxxx, y = yyyy)\n## [1]  11  45 141 359 783",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al uso de `R`</span>"
    ]
  },
  {
    "objectID": "r-intro.html#sec-rgl",
    "href": "r-intro.html#sec-rgl",
    "title": "1  Introducción al uso de R",
    "section": "1.13 Ejemplo de la instalación y uso de un paquete",
    "text": "1.13 Ejemplo de la instalación y uso de un paquete\nEn R hay muchos paquetes que nos ayudan a hacer gráficas en 3D. Instalemos y usemos el paquete rgl. Note que cada paquete trae nuevas funciones que debemos aprender si es que queremos usarlo, lo bueno es que podemos encontrar documentación en muchos sitios de internet. Aquí encontramos un buen tutorial para usar rgl\nGráfica 3D dinámica con el paquete ‘rgl’\n\nlibrary(rgl) # ya debe estar instalado: install.package(\"rgl\")\nx &lt;- seq(-5.12, 5.12, length.out=200)\ny &lt;- x\n\n# Función rastrigin muy usada en optimización\nrast &lt;- function(x,y){\n  20 + x^2 -10*cos(2*pi*x) + y^2 - 10*cos(2*pi*y)\n}\nrast(0,0) # evalúa la función en el punto (0,0)\n## [1] 0\n\nz &lt;- outer(x,y,FUN=rast)\nrgl::persp3d(x,y,z, col=\"orange2\", main=\"Rastrigin\")",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al uso de `R`</span>"
    ]
  },
  {
    "objectID": "r-intro.html#importar-datos",
    "href": "r-intro.html#importar-datos",
    "title": "1  Introducción al uso de R",
    "section": "1.14 Importar datos",
    "text": "1.14 Importar datos\nPara obtener y cambiar el directorio de trabajo\ngetwd()\nsetwd(\"colocar la ruta de windows, cambiar \\ por /\")\n\n# El archivo Auto.txt debe estar en el directorio de trabajo\nautos &lt;- read.table(file = \"Auto.txt\")\nsummary(autos) # hace un resumen de cada variable en la base de datos autos\n##       mpg          cylinders      displacement     horsepower        weight    \n##  Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n##  1st Qu.:17.00   1st Qu.:4.000   1st Qu.:105.0   1st Qu.: 75.0   1st Qu.:2225  \n##  Median :22.75   Median :4.000   Median :151.0   Median : 93.5   Median :2804  \n##  Mean   :23.45   Mean   :5.472   Mean   :194.4   Mean   :104.5   Mean   :2978  \n##  3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:275.8   3rd Qu.:126.0   3rd Qu.:3615  \n##  Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n##   acceleration        year           origin          name          \n##  Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:392        \n##  1st Qu.:13.78   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n##  Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n##  Mean   :15.54   Mean   :75.98   Mean   :1.577                     \n##  3rd Qu.:17.02   3rd Qu.:79.00   3rd Qu.:2.000                     \n##  Max.   :24.80   Max.   :82.00   Max.   :3.000\nmpg_auto &lt;- autos$mpg  #guarda la variable mpg en el objeto mpg_auto\nclass(autos)\n## [1] \"data.frame\"",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al uso de `R`</span>"
    ]
  },
  {
    "objectID": "r-intro.html#estructuras-de-control",
    "href": "r-intro.html#estructuras-de-control",
    "title": "1  Introducción al uso de R",
    "section": "1.15 Estructuras de control",
    "text": "1.15 Estructuras de control\nif, for, while, repeat; break, next, return\n\n1.15.1 Condicionales\n\n## if(&lt;condicion&gt;){\n##  haga algo\n## } else if(&lt;condicion2&gt;){\n##  haga esto otro\n## } else{\n##  haga esto último\n## }\nx &lt;- 10\nif(x&lt;5){\n  \"X es menor que 5\"\n} else if(x==5){\n  \"X es igual a 5\"\n} else{\n  \"X es mayor que 5\"\n}\n## [1] \"X es mayor que 5\"\n\n\n\n1.15.2 Ciclos\n\n## for(i in &lt;conjunto&gt;){\n##  haga algo por cada elemento\n## }\n\nj &lt;- 1\nfor(i in 1:10){\n  if(i&lt;6){\n    j &lt;- 2*j+i\n    print(log(j))\n  } else{\n    j &lt;- 2*j+i\n    print(cos(j))\n  }\n}\n## [1] 1.098612\n## [1] 2.079442\n## [1] 2.944439\n## [1] 3.73767\n## [1] 4.488636\n## [1] -0.2151347\n## [1] -0.4080545\n## [1] -0.640098\n## [1] -0.240842\n## [1] 0.9960638\n\n\nvv &lt;- c(\"primer\", \"programa\", \"en R\", \"Con ciclo for\")\nfor (i in vv){\n  print(i)\n}\n## [1] \"primer\"\n## [1] \"programa\"\n## [1] \"en R\"\n## [1] \"Con ciclo for\"\n\nSe pueden anidar\n\nmm &lt;- matrix(data = seq(from=exp(1), to=pi, length.out=16), nrow = 4)\n\nfor(i in 1:dim(mm)[1]){\n  for(j in 1:dim(mm)[2]){\n    cat(\"El elmento (\", i, \",\", j, \") es: \", mm[i,j], \"\\n\", sep = \"\")\n  }\n}\n## El elmento (1,1) es: 2.718282\n## El elmento (1,2) es: 2.831165\n## El elmento (1,3) es: 2.944048\n## El elmento (1,4) es: 3.05693\n## El elmento (2,1) es: 2.746503\n## El elmento (2,2) es: 2.859385\n## El elmento (2,3) es: 2.972268\n## El elmento (2,4) es: 3.085151\n## El elmento (3,1) es: 2.774723\n## El elmento (3,2) es: 2.887606\n## El elmento (3,3) es: 3.000489\n## El elmento (3,4) es: 3.113372\n## El elmento (4,1) es: 2.802944\n## El elmento (4,2) es: 2.915827\n## El elmento (4,3) es: 3.02871\n## El elmento (4,4) es: 3.141593",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción al uso de `R`</span>"
    ]
  },
  {
    "objectID": "uso-ggplot2.html",
    "href": "uso-ggplot2.html",
    "title": "2  Uso de ggplot2",
    "section": "",
    "text": "2.1 Abstración del problema\nAntes de comenzar con EDA, hagamos una conceptualización del tipo de problemas que queremos resolver con lo que conocemos como Ciencia de Datos (En el tablero)",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uso de `ggplot2`</span>"
    ]
  },
  {
    "objectID": "uso-ggplot2.html#análisis-exploratorio",
    "href": "uso-ggplot2.html#análisis-exploratorio",
    "title": "2  Uso de ggplot2",
    "section": "2.2 Análisis exploratorio",
    "text": "2.2 Análisis exploratorio\nEl análisis exploratorio se lleva a cabo a través de transformaciones y visualizaciones de los datos.1 El proceso puede ser iterativo y ayuda a comprender mejor el tipo de datos con los que se está trabajando, muchos problemas en Ciencia de Datos se resuelven en esta fase.\n1 Hablar de EDA es una forma de organizar las ideas, algunas personas o autores pueden considerar que el EDA incluye la limpieza de datos e incluso alguna modelación. Por sencillez, vamos a considerar que el EDA está compuesto principalmente por transformación de variables y visualizaciones\n\n\n\n\n\n\n\nflowchart LR\nsubgraph DW[Data Wrangling]\n  direction LR\n  A(Importar) --&gt; B(Ordenar)\n  B --&gt; C(Transformar)\nend\nsubgraph EDA[\"Exploratory Data Analysis (EDA)\"]\n  direction RL\n  C2(Transformar) --&gt; D(Visualizar)\n  D --&gt; C2\nend\nsubgraph MO[Modeling]\n  direction RL\n  E(Modelar) --&gt; F(Evaluar)\n  F --&gt; E\nend\nsubgraph CO[Communicate]\n  G(comunicar)\nend\n\nDW --&gt; EDA\nEDA --&gt; MO\nMO --&gt; EDA\nMO --&gt; CO\n\n\n\n\n\n\n\n\nFigura 2.1: Flujo de trabajo en ciencia de datos",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uso de `ggplot2`</span>"
    ]
  },
  {
    "objectID": "uso-ggplot2.html#cómo-hacer-eda-en-r",
    "href": "uso-ggplot2.html#cómo-hacer-eda-en-r",
    "title": "2  Uso de ggplot2",
    "section": "2.3 ¿Cómo hacer EDA en R?",
    "text": "2.3 ¿Cómo hacer EDA en R?\nHay muchos paquetes en R pensados para facilitar el trabajo del científico de datos. Nos basaremos en el ecosistema de paquetes optimizados para análisis de datos, conocido como tidyverse\nEl core tidyverse es el conjunto principal de paquetes de este ecosistema:\n\nggplot2: Para hacer gráficas\ndiplyr: Para manipulación de datos\ntidyr: Para organizar bases de datos en formato tidy\nreadr: Para importar datos\ntibble: para manejo de bases de datos con muchos datos\nstringr: Para manipular strings\nforcats: Para manipular variables categóricas\n\npodemos cargar cada paquete de forma individual (library(ggplot2)) o cargarlos de forma conjunta con library(tidyverse). Nos interesan principalmente ggplot2 y dplyr.\nPara poder hacer análisis de cualquier tipo, primero necesitamos tener disponible un conjunto de datos o data frame (o en formato del tidyverse un tibble2). Hay varias formas de obtener datos en R\n2 Investigue qué es un tibble\nDatos sintéticos (simulados)\nBases de datos instaladas con R o con alguno de sus paquetes\nImportarlo desde una fuente externa, ya sea de forma local o desde la web\n\nUsaremos una base de datos que viene en el paquete palmerpenguins (hay que instalar y cargar la librería) que es bastante adecuada para iniciar un análisis exploratorio\n\nlibrary(palmerpenguins)\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nOtra herramienta que usaremos con frecuencia en nuestros códigos es el operador pipe. Este operador viene en dos versiones:\n\n|&gt;: Versión base o nativa de R, no requiere la instalación de ningún paquete\n%&gt;%: Versión del tidyverse, requiere cargar alguno de los paquetes dplyr, magrittr, tidyverse\n\nEl atajo de teclado para el operador pipe es . Por defecto se genera %&gt;%, para generar el operador nativo |&gt;, debe ingresar a Tools -&gt; Global Options -&gt; Code -&gt; Editing y activar la casilla use native pipe operator\nEl operador pipe nos permite escribir código de forma lineal y más fácilmente legible de izquierda a derecha, en lugar de funciones anidadas.\n\nsin(log(sum(c(1,2,3))))\n\n[1] 0.9756868\n\n# usando el operador pipe\nc(1,2,3) |&gt; sum() |&gt; log() |&gt; sin()\n\n[1] 0.9756868\n\n\nEl operador permite pasar el resultado de la izquierda como primer argumento de la función siguiente a la derecha.\nEl tipo de gráfica que debemos crear depende del tipo de variables que tengamos y de la forma en que las queramos analizar, por lo que es importante elegir las gráficas adecuadas. Antes de empezar a construir gráficas, recordemos una definición de los tipos de variables que podemos tener\n\nNuméricas (cuantitativas)\n\nDiscretas\nContinuas\n\ncategóricas (cualitativas o factores)\n\nOrdinales\nNominales\n\n\nejecute ?penguins para estudiar y entender los datos en penguins\nDividamos el trabajo por casos",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uso de `ggplot2`</span>"
    ]
  },
  {
    "objectID": "uso-ggplot2.html#caso-1.-una-variable-numérica",
    "href": "uso-ggplot2.html#caso-1.-una-variable-numérica",
    "title": "2  Uso de ggplot2",
    "section": "2.4 Caso 1. Una variable numérica",
    "text": "2.4 Caso 1. Una variable numérica\nAlgunas gráficas para este caso:\n\nHistogramas\nGráficos de densidad\nDiagramas de caja\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nggplot(data = penguins)\n\n\n\n\n\n\n\n\nUna forma alterna, usando el operador pipe3 sería penguins |&gt; ggplot()\n3 Para ggplot2 solo usaremos este operador al aplicar la función ggplot() a la base de datos que estemos usando, el resto de elementos del gráfico se agregarán con el signo ++. Cuando estemos usando dplyr, usaremos con más frecuencia el operador pipeggplot2 implementa lo que se conoce como gramática de gráficos, que en resumen quiere decir que el gráfico se construye al ir agregando capas (layers). Al aplicar la función ggplot() a una base de datos, se crea un lienzo en blanco (gris).\n\npenguins |&gt; ggplot(mapping = aes(x = body_mass_g))\n\n\n\n\n\n\n\n\nAún no se muestra una gráfica, se ha creado un mapeo y puede verse la escala de valores de la variable body_mass_g en el eje X, pero aún falta por agregar el tipo de geometría o tipo de gráfico a construir\n\npenguins |&gt; ggplot(mapping = aes(x = body_mass_g)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nggplot nos informa sobre la presencia de 2 valores no finitos, probablemente sean NA. Por ahora eliminemos los NA de la base de datos4\n4 Esto no debe hacerse a la ligera, más adelante nos ocuparemos de cómo tratar estos NA, por ahora vamos a concentrarnos en hacer gráficas\npenguins &lt;- na.omit(penguins)\n\nVolvemos a intentar la gráfica\n\npenguins |&gt; ggplot(mapping = aes(x = body_mass_g)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nPara hacer gráficos ordenados, es bueno personalizar las etiquetas y los títulos y subtítulos, esto se logra con la función labs() que tiene argumentos como title, subtitle, caption, etc\n\npenguins |&gt; ggplot(mapping = aes(x = body_mass_g)) +\n  geom_histogram() +\n  labs(\n    title = \"Peso de los Pingüinos del Archipiélago de Palmer\",\n    x = \"Peso en gramos\",\n    y = \"Conteo\",\n    caption = \"Datos de 333 pingüinos distribuidos en 3 islas y 3 especies\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nAlgunos argumentos o propiedades, tales como el color (colour) y relleno (fill), son comunes a muchos o todos los tipos de geometrías. Otras propiedades son más específicas, para un histograma por ejemplo, tiene sentido tener propiedades de número de clases (bins) o ancho de las clases (binwidth), pero no tendría mucho sentido hablar de número de clases en un diagrama de cajas.\nEn el siguiente gráfico, modificamos la cantidad de clases a usar, el color del borde y el relleno.\n\npenguins |&gt; ggplot(mapping = aes(x = body_mass_g)) +\n  geom_histogram(fill=\"red\", bins = 20, color = \"black\") +\n  labs(\n    title = \"Peso de los Pingüinos del Archipiélago de Palmer\",\n    x = \"Peso en gramos\",\n    y = \"Conteo\",\n    caption = \"Datos de 333 pingüinos distribuidos en 3 islas y 3 especies\")",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uso de `ggplot2`</span>"
    ]
  },
  {
    "objectID": "uso-ggplot2.html#bonus-colores",
    "href": "uso-ggplot2.html#bonus-colores",
    "title": "2  Uso de ggplot2",
    "section": "2.5 Bonus: Colores",
    "text": "2.5 Bonus: Colores\n\nParte importante de toda gráfica, es el uso de los colores\nEn R podemos manejar colores por nombres (en inglés, por supuesto) o usando sistemas de codificación como el RGB y Hexadecimal\nUna lista de colores en R la podemos obtener con colors()5. La lista completa de colores disponibles en R es\n\n\n\n\n\n\nN\nColores\n\n\n\n\n1\nwhite\n\n\n2\naliceblue\n\n\n3\nantiquewhite\n\n\n4\nantiquewhite1\n\n\n5\nantiquewhite2\n\n\n6\nantiquewhite3\n\n\n7\nantiquewhite4\n\n\n8\naquamarine\n\n\n9\naquamarine1\n\n\n10\naquamarine2\n\n\n11\naquamarine3\n\n\n12\naquamarine4\n\n\n13\nazure\n\n\n14\nazure1\n\n\n15\nazure2\n\n\n16\nazure3\n\n\n17\nazure4\n\n\n18\nbeige\n\n\n19\nbisque\n\n\n20\nbisque1\n\n\n21\nbisque2\n\n\n22\nbisque3\n\n\n23\nbisque4\n\n\n24\nblack\n\n\n25\nblanchedalmond\n\n\n26\nblue\n\n\n27\nblue1\n\n\n28\nblue2\n\n\n29\nblue3\n\n\n30\nblue4\n\n\n31\nblueviolet\n\n\n32\nbrown\n\n\n33\nbrown1\n\n\n34\nbrown2\n\n\n35\nbrown3\n\n\n36\nbrown4\n\n\n37\nburlywood\n\n\n38\nburlywood1\n\n\n39\nburlywood2\n\n\n40\nburlywood3\n\n\n41\nburlywood4\n\n\n42\ncadetblue\n\n\n43\ncadetblue1\n\n\n44\ncadetblue2\n\n\n45\ncadetblue3\n\n\n46\ncadetblue4\n\n\n47\nchartreuse\n\n\n48\nchartreuse1\n\n\n49\nchartreuse2\n\n\n50\nchartreuse3\n\n\n51\nchartreuse4\n\n\n52\nchocolate\n\n\n53\nchocolate1\n\n\n54\nchocolate2\n\n\n55\nchocolate3\n\n\n56\nchocolate4\n\n\n57\ncoral\n\n\n58\ncoral1\n\n\n59\ncoral2\n\n\n60\ncoral3\n\n\n61\ncoral4\n\n\n62\ncornflowerblue\n\n\n63\ncornsilk\n\n\n64\ncornsilk1\n\n\n65\ncornsilk2\n\n\n66\ncornsilk3\n\n\n67\ncornsilk4\n\n\n68\ncyan\n\n\n69\ncyan1\n\n\n70\ncyan2\n\n\n71\ncyan3\n\n\n72\ncyan4\n\n\n73\ndarkblue\n\n\n74\ndarkcyan\n\n\n75\ndarkgoldenrod\n\n\n76\ndarkgoldenrod1\n\n\n77\ndarkgoldenrod2\n\n\n78\ndarkgoldenrod3\n\n\n79\ndarkgoldenrod4\n\n\n80\ndarkgray\n\n\n81\ndarkgreen\n\n\n82\ndarkgrey\n\n\n83\ndarkkhaki\n\n\n84\ndarkmagenta\n\n\n85\ndarkolivegreen\n\n\n86\ndarkolivegreen1\n\n\n87\ndarkolivegreen2\n\n\n88\ndarkolivegreen3\n\n\n89\ndarkolivegreen4\n\n\n90\ndarkorange\n\n\n91\ndarkorange1\n\n\n92\ndarkorange2\n\n\n93\ndarkorange3\n\n\n94\ndarkorange4\n\n\n95\ndarkorchid\n\n\n96\ndarkorchid1\n\n\n97\ndarkorchid2\n\n\n98\ndarkorchid3\n\n\n99\ndarkorchid4\n\n\n100\ndarkred\n\n\n101\ndarksalmon\n\n\n102\ndarkseagreen\n\n\n103\ndarkseagreen1\n\n\n104\ndarkseagreen2\n\n\n105\ndarkseagreen3\n\n\n106\ndarkseagreen4\n\n\n107\ndarkslateblue\n\n\n108\ndarkslategray\n\n\n109\ndarkslategray1\n\n\n110\ndarkslategray2\n\n\n111\ndarkslategray3\n\n\n112\ndarkslategray4\n\n\n113\ndarkslategrey\n\n\n114\ndarkturquoise\n\n\n115\ndarkviolet\n\n\n116\ndeeppink\n\n\n117\ndeeppink1\n\n\n118\ndeeppink2\n\n\n119\ndeeppink3\n\n\n120\ndeeppink4\n\n\n121\ndeepskyblue\n\n\n122\ndeepskyblue1\n\n\n123\ndeepskyblue2\n\n\n124\ndeepskyblue3\n\n\n125\ndeepskyblue4\n\n\n126\ndimgray\n\n\n127\ndimgrey\n\n\n128\ndodgerblue\n\n\n129\ndodgerblue1\n\n\n130\ndodgerblue2\n\n\n131\ndodgerblue3\n\n\n132\ndodgerblue4\n\n\n133\nfirebrick\n\n\n134\nfirebrick1\n\n\n135\nfirebrick2\n\n\n136\nfirebrick3\n\n\n137\nfirebrick4\n\n\n138\nfloralwhite\n\n\n139\nforestgreen\n\n\n140\ngainsboro\n\n\n141\nghostwhite\n\n\n142\ngold\n\n\n143\ngold1\n\n\n144\ngold2\n\n\n145\ngold3\n\n\n146\ngold4\n\n\n147\ngoldenrod\n\n\n148\ngoldenrod1\n\n\n149\ngoldenrod2\n\n\n150\ngoldenrod3\n\n\n151\ngoldenrod4\n\n\n152\ngray\n\n\n153\ngray0\n\n\n154\ngray1\n\n\n155\ngray2\n\n\n156\ngray3\n\n\n157\ngray4\n\n\n158\ngray5\n\n\n159\ngray6\n\n\n160\ngray7\n\n\n161\ngray8\n\n\n162\ngray9\n\n\n163\ngray10\n\n\n164\ngray11\n\n\n165\ngray12\n\n\n166\ngray13\n\n\n167\ngray14\n\n\n168\ngray15\n\n\n169\ngray16\n\n\n170\ngray17\n\n\n171\ngray18\n\n\n172\ngray19\n\n\n173\ngray20\n\n\n174\ngray21\n\n\n175\ngray22\n\n\n176\ngray23\n\n\n177\ngray24\n\n\n178\ngray25\n\n\n179\ngray26\n\n\n180\ngray27\n\n\n181\ngray28\n\n\n182\ngray29\n\n\n183\ngray30\n\n\n184\ngray31\n\n\n185\ngray32\n\n\n186\ngray33\n\n\n187\ngray34\n\n\n188\ngray35\n\n\n189\ngray36\n\n\n190\ngray37\n\n\n191\ngray38\n\n\n192\ngray39\n\n\n193\ngray40\n\n\n194\ngray41\n\n\n195\ngray42\n\n\n196\ngray43\n\n\n197\ngray44\n\n\n198\ngray45\n\n\n199\ngray46\n\n\n200\ngray47\n\n\n201\ngray48\n\n\n202\ngray49\n\n\n203\ngray50\n\n\n204\ngray51\n\n\n205\ngray52\n\n\n206\ngray53\n\n\n207\ngray54\n\n\n208\ngray55\n\n\n209\ngray56\n\n\n210\ngray57\n\n\n211\ngray58\n\n\n212\ngray59\n\n\n213\ngray60\n\n\n214\ngray61\n\n\n215\ngray62\n\n\n216\ngray63\n\n\n217\ngray64\n\n\n218\ngray65\n\n\n219\ngray66\n\n\n220\ngray67\n\n\n221\ngray68\n\n\n222\ngray69\n\n\n223\ngray70\n\n\n224\ngray71\n\n\n225\ngray72\n\n\n226\ngray73\n\n\n227\ngray74\n\n\n228\ngray75\n\n\n229\ngray76\n\n\n230\ngray77\n\n\n231\ngray78\n\n\n232\ngray79\n\n\n233\ngray80\n\n\n234\ngray81\n\n\n235\ngray82\n\n\n236\ngray83\n\n\n237\ngray84\n\n\n238\ngray85\n\n\n239\ngray86\n\n\n240\ngray87\n\n\n241\ngray88\n\n\n242\ngray89\n\n\n243\ngray90\n\n\n244\ngray91\n\n\n245\ngray92\n\n\n246\ngray93\n\n\n247\ngray94\n\n\n248\ngray95\n\n\n249\ngray96\n\n\n250\ngray97\n\n\n251\ngray98\n\n\n252\ngray99\n\n\n253\ngray100\n\n\n254\ngreen\n\n\n255\ngreen1\n\n\n256\ngreen2\n\n\n257\ngreen3\n\n\n258\ngreen4\n\n\n259\ngreenyellow\n\n\n260\ngrey\n\n\n261\ngrey0\n\n\n262\ngrey1\n\n\n263\ngrey2\n\n\n264\ngrey3\n\n\n265\ngrey4\n\n\n266\ngrey5\n\n\n267\ngrey6\n\n\n268\ngrey7\n\n\n269\ngrey8\n\n\n270\ngrey9\n\n\n271\ngrey10\n\n\n272\ngrey11\n\n\n273\ngrey12\n\n\n274\ngrey13\n\n\n275\ngrey14\n\n\n276\ngrey15\n\n\n277\ngrey16\n\n\n278\ngrey17\n\n\n279\ngrey18\n\n\n280\ngrey19\n\n\n281\ngrey20\n\n\n282\ngrey21\n\n\n283\ngrey22\n\n\n284\ngrey23\n\n\n285\ngrey24\n\n\n286\ngrey25\n\n\n287\ngrey26\n\n\n288\ngrey27\n\n\n289\ngrey28\n\n\n290\ngrey29\n\n\n291\ngrey30\n\n\n292\ngrey31\n\n\n293\ngrey32\n\n\n294\ngrey33\n\n\n295\ngrey34\n\n\n296\ngrey35\n\n\n297\ngrey36\n\n\n298\ngrey37\n\n\n299\ngrey38\n\n\n300\ngrey39\n\n\n301\ngrey40\n\n\n302\ngrey41\n\n\n303\ngrey42\n\n\n304\ngrey43\n\n\n305\ngrey44\n\n\n306\ngrey45\n\n\n307\ngrey46\n\n\n308\ngrey47\n\n\n309\ngrey48\n\n\n310\ngrey49\n\n\n311\ngrey50\n\n\n312\ngrey51\n\n\n313\ngrey52\n\n\n314\ngrey53\n\n\n315\ngrey54\n\n\n316\ngrey55\n\n\n317\ngrey56\n\n\n318\ngrey57\n\n\n319\ngrey58\n\n\n320\ngrey59\n\n\n321\ngrey60\n\n\n322\ngrey61\n\n\n323\ngrey62\n\n\n324\ngrey63\n\n\n325\ngrey64\n\n\n326\ngrey65\n\n\n327\ngrey66\n\n\n328\ngrey67\n\n\n329\ngrey68\n\n\n330\ngrey69\n\n\n331\ngrey70\n\n\n332\ngrey71\n\n\n333\ngrey72\n\n\n334\ngrey73\n\n\n335\ngrey74\n\n\n336\ngrey75\n\n\n337\ngrey76\n\n\n338\ngrey77\n\n\n339\ngrey78\n\n\n340\ngrey79\n\n\n341\ngrey80\n\n\n342\ngrey81\n\n\n343\ngrey82\n\n\n344\ngrey83\n\n\n345\ngrey84\n\n\n346\ngrey85\n\n\n347\ngrey86\n\n\n348\ngrey87\n\n\n349\ngrey88\n\n\n350\ngrey89\n\n\n351\ngrey90\n\n\n352\ngrey91\n\n\n353\ngrey92\n\n\n354\ngrey93\n\n\n355\ngrey94\n\n\n356\ngrey95\n\n\n357\ngrey96\n\n\n358\ngrey97\n\n\n359\ngrey98\n\n\n360\ngrey99\n\n\n361\ngrey100\n\n\n362\nhoneydew\n\n\n363\nhoneydew1\n\n\n364\nhoneydew2\n\n\n365\nhoneydew3\n\n\n366\nhoneydew4\n\n\n367\nhotpink\n\n\n368\nhotpink1\n\n\n369\nhotpink2\n\n\n370\nhotpink3\n\n\n371\nhotpink4\n\n\n372\nindianred\n\n\n373\nindianred1\n\n\n374\nindianred2\n\n\n375\nindianred3\n\n\n376\nindianred4\n\n\n377\nivory\n\n\n378\nivory1\n\n\n379\nivory2\n\n\n380\nivory3\n\n\n381\nivory4\n\n\n382\nkhaki\n\n\n383\nkhaki1\n\n\n384\nkhaki2\n\n\n385\nkhaki3\n\n\n386\nkhaki4\n\n\n387\nlavender\n\n\n388\nlavenderblush\n\n\n389\nlavenderblush1\n\n\n390\nlavenderblush2\n\n\n391\nlavenderblush3\n\n\n392\nlavenderblush4\n\n\n393\nlawngreen\n\n\n394\nlemonchiffon\n\n\n395\nlemonchiffon1\n\n\n396\nlemonchiffon2\n\n\n397\nlemonchiffon3\n\n\n398\nlemonchiffon4\n\n\n399\nlightblue\n\n\n400\nlightblue1\n\n\n401\nlightblue2\n\n\n402\nlightblue3\n\n\n403\nlightblue4\n\n\n404\nlightcoral\n\n\n405\nlightcyan\n\n\n406\nlightcyan1\n\n\n407\nlightcyan2\n\n\n408\nlightcyan3\n\n\n409\nlightcyan4\n\n\n410\nlightgoldenrod\n\n\n411\nlightgoldenrod1\n\n\n412\nlightgoldenrod2\n\n\n413\nlightgoldenrod3\n\n\n414\nlightgoldenrod4\n\n\n415\nlightgoldenrodyellow\n\n\n416\nlightgray\n\n\n417\nlightgreen\n\n\n418\nlightgrey\n\n\n419\nlightpink\n\n\n420\nlightpink1\n\n\n421\nlightpink2\n\n\n422\nlightpink3\n\n\n423\nlightpink4\n\n\n424\nlightsalmon\n\n\n425\nlightsalmon1\n\n\n426\nlightsalmon2\n\n\n427\nlightsalmon3\n\n\n428\nlightsalmon4\n\n\n429\nlightseagreen\n\n\n430\nlightskyblue\n\n\n431\nlightskyblue1\n\n\n432\nlightskyblue2\n\n\n433\nlightskyblue3\n\n\n434\nlightskyblue4\n\n\n435\nlightslateblue\n\n\n436\nlightslategray\n\n\n437\nlightslategrey\n\n\n438\nlightsteelblue\n\n\n439\nlightsteelblue1\n\n\n440\nlightsteelblue2\n\n\n441\nlightsteelblue3\n\n\n442\nlightsteelblue4\n\n\n443\nlightyellow\n\n\n444\nlightyellow1\n\n\n445\nlightyellow2\n\n\n446\nlightyellow3\n\n\n447\nlightyellow4\n\n\n448\nlimegreen\n\n\n449\nlinen\n\n\n450\nmagenta\n\n\n451\nmagenta1\n\n\n452\nmagenta2\n\n\n453\nmagenta3\n\n\n454\nmagenta4\n\n\n455\nmaroon\n\n\n456\nmaroon1\n\n\n457\nmaroon2\n\n\n458\nmaroon3\n\n\n459\nmaroon4\n\n\n460\nmediumaquamarine\n\n\n461\nmediumblue\n\n\n462\nmediumorchid\n\n\n463\nmediumorchid1\n\n\n464\nmediumorchid2\n\n\n465\nmediumorchid3\n\n\n466\nmediumorchid4\n\n\n467\nmediumpurple\n\n\n468\nmediumpurple1\n\n\n469\nmediumpurple2\n\n\n470\nmediumpurple3\n\n\n471\nmediumpurple4\n\n\n472\nmediumseagreen\n\n\n473\nmediumslateblue\n\n\n474\nmediumspringgreen\n\n\n475\nmediumturquoise\n\n\n476\nmediumvioletred\n\n\n477\nmidnightblue\n\n\n478\nmintcream\n\n\n479\nmistyrose\n\n\n480\nmistyrose1\n\n\n481\nmistyrose2\n\n\n482\nmistyrose3\n\n\n483\nmistyrose4\n\n\n484\nmoccasin\n\n\n485\nnavajowhite\n\n\n486\nnavajowhite1\n\n\n487\nnavajowhite2\n\n\n488\nnavajowhite3\n\n\n489\nnavajowhite4\n\n\n490\nnavy\n\n\n491\nnavyblue\n\n\n492\noldlace\n\n\n493\nolivedrab\n\n\n494\nolivedrab1\n\n\n495\nolivedrab2\n\n\n496\nolivedrab3\n\n\n497\nolivedrab4\n\n\n498\norange\n\n\n499\norange1\n\n\n500\norange2\n\n\n501\norange3\n\n\n502\norange4\n\n\n503\norangered\n\n\n504\norangered1\n\n\n505\norangered2\n\n\n506\norangered3\n\n\n507\norangered4\n\n\n508\norchid\n\n\n509\norchid1\n\n\n510\norchid2\n\n\n511\norchid3\n\n\n512\norchid4\n\n\n513\npalegoldenrod\n\n\n514\npalegreen\n\n\n515\npalegreen1\n\n\n516\npalegreen2\n\n\n517\npalegreen3\n\n\n518\npalegreen4\n\n\n519\npaleturquoise\n\n\n520\npaleturquoise1\n\n\n521\npaleturquoise2\n\n\n522\npaleturquoise3\n\n\n523\npaleturquoise4\n\n\n524\npalevioletred\n\n\n525\npalevioletred1\n\n\n526\npalevioletred2\n\n\n527\npalevioletred3\n\n\n528\npalevioletred4\n\n\n529\npapayawhip\n\n\n530\npeachpuff\n\n\n531\npeachpuff1\n\n\n532\npeachpuff2\n\n\n533\npeachpuff3\n\n\n534\npeachpuff4\n\n\n535\nperu\n\n\n536\npink\n\n\n537\npink1\n\n\n538\npink2\n\n\n539\npink3\n\n\n540\npink4\n\n\n541\nplum\n\n\n542\nplum1\n\n\n543\nplum2\n\n\n544\nplum3\n\n\n545\nplum4\n\n\n546\npowderblue\n\n\n547\npurple\n\n\n548\npurple1\n\n\n549\npurple2\n\n\n550\npurple3\n\n\n551\npurple4\n\n\n552\nred\n\n\n553\nred1\n\n\n554\nred2\n\n\n555\nred3\n\n\n556\nred4\n\n\n557\nrosybrown\n\n\n558\nrosybrown1\n\n\n559\nrosybrown2\n\n\n560\nrosybrown3\n\n\n561\nrosybrown4\n\n\n562\nroyalblue\n\n\n563\nroyalblue1\n\n\n564\nroyalblue2\n\n\n565\nroyalblue3\n\n\n566\nroyalblue4\n\n\n567\nsaddlebrown\n\n\n568\nsalmon\n\n\n569\nsalmon1\n\n\n570\nsalmon2\n\n\n571\nsalmon3\n\n\n572\nsalmon4\n\n\n573\nsandybrown\n\n\n574\nseagreen\n\n\n575\nseagreen1\n\n\n576\nseagreen2\n\n\n577\nseagreen3\n\n\n578\nseagreen4\n\n\n579\nseashell\n\n\n580\nseashell1\n\n\n581\nseashell2\n\n\n582\nseashell3\n\n\n583\nseashell4\n\n\n584\nsienna\n\n\n585\nsienna1\n\n\n586\nsienna2\n\n\n587\nsienna3\n\n\n588\nsienna4\n\n\n589\nskyblue\n\n\n590\nskyblue1\n\n\n591\nskyblue2\n\n\n592\nskyblue3\n\n\n593\nskyblue4\n\n\n594\nslateblue\n\n\n595\nslateblue1\n\n\n596\nslateblue2\n\n\n597\nslateblue3\n\n\n598\nslateblue4\n\n\n599\nslategray\n\n\n600\nslategray1\n\n\n601\nslategray2\n\n\n602\nslategray3\n\n\n603\nslategray4\n\n\n604\nslategrey\n\n\n605\nsnow\n\n\n606\nsnow1\n\n\n607\nsnow2\n\n\n608\nsnow3\n\n\n609\nsnow4\n\n\n610\nspringgreen\n\n\n611\nspringgreen1\n\n\n612\nspringgreen2\n\n\n613\nspringgreen3\n\n\n614\nspringgreen4\n\n\n615\nsteelblue\n\n\n616\nsteelblue1\n\n\n617\nsteelblue2\n\n\n618\nsteelblue3\n\n\n619\nsteelblue4\n\n\n620\ntan\n\n\n621\ntan1\n\n\n622\ntan2\n\n\n623\ntan3\n\n\n624\ntan4\n\n\n625\nthistle\n\n\n626\nthistle1\n\n\n627\nthistle2\n\n\n628\nthistle3\n\n\n629\nthistle4\n\n\n630\ntomato\n\n\n631\ntomato1\n\n\n632\ntomato2\n\n\n633\ntomato3\n\n\n634\ntomato4\n\n\n635\nturquoise\n\n\n636\nturquoise1\n\n\n637\nturquoise2\n\n\n638\nturquoise3\n\n\n639\nturquoise4\n\n\n640\nviolet\n\n\n641\nvioletred\n\n\n642\nvioletred1\n\n\n643\nvioletred2\n\n\n644\nvioletred3\n\n\n645\nvioletred4\n\n\n646\nwheat\n\n\n647\nwheat1\n\n\n648\nwheat2\n\n\n649\nwheat3\n\n\n650\nwheat4\n\n\n651\nwhitesmoke\n\n\n652\nyellow\n\n\n653\nyellow1\n\n\n654\nyellow2\n\n\n655\nyellow3\n\n\n656\nyellow4\n\n\n657\nyellowgreen\n\n\n\n\n\n\n\n\n5 Ejecute el comando en la consola, a menos que quiera imprimir una lista de colores siempre que ejecute el código en su editor\n\nManejar colores por nombres nos permite recordarlos fácilmente, pero es bastante limitado, a pesar de los más de 600 colores de la lista, no es nada comparado con los más de 16 millones de colores que podemos generar con sistemas como RGB o HEX (aunque tampoco necesitamos tantos colores, ni todas las pantallas pueden generarlos ni nosotros notamos la diferencia)\n\n\n2.5.1 RGB\n\nIniciales de los colores primarios (en inglés): R: Red; G: Green; B: Blue. Se basa en la combinación de diferentes niveles de intensidad para cada uno de estos colores\nSe representa como una terna, donde cada valor está, por lo general, entre 0 y 255 (también en escala de 0 a 1 o en porcentajes de 0 a 100). Equivalencia de los colores primarios:\n\nRojo: rgb(255, 0, 0)\nVerde: rgb(0, 255, 0)\nAzul: rgb(0, 0, 255)\n\nAlgunos colores en sistema RGB (intente comprender el código completo, no se usa ggplot2 sino la función base plot de R, ejecute ?plot en la consola para ver la documentación si quiere aprender a usar esta función base de R)\n\nplot(x = rep(1,3), col = c(rgb(230,50,110, maxColorValue = 255), \n                           rgb(20,50,10, maxColorValue = 100), \n                           rgb(0.5,0.5,0.7)), \n     main = \"Algunos colores con RGB\",\n     xlim = c(0,4),\n     xlab = \"\", ylab=\"\", pch = 20, cex = 10)\n\n\n\n\n\n\n\n\nPor defecto, la función rgb() en R, tomará como valor máximo de la escala el 1, así que si se está trabajando en la escala 0-255 (lo más usual), es necesario establecer maxColorValue=255, en realidad de puede trabajar con cualquier valor positivo, R hará la conversión de forma adecuada.\nlos 256 posibles valores para cada componente (0-255), pueden producir en total \\(256^3=16777216\\), ¡más de 16 millones de colores!\nEs posible agregar un valor \\(\\alpha\\) de opacidad a los colores (estudie y comprenda el siguiente código)\n\nalfa &lt;- seq(5, 255, by = 25) # ¿qué hace esto?\nplot(x = rep(1,length(alfa)), \n     col = rgb(17, 125, 0, maxColorValue = 255, alpha = alfa), \n     cex = 10, pch = 20, xlab = \"\", ylab = \"\", xlim = c(0,12),\n     main = \"Agregar opacidad al color\")\n\n\n\n\n\n\n\n\n\n\n\n2.5.2 Hexadecimal\n\nEl sistema hexadecimal consta de 6 posiciones 👨‍🏫, las primeras 2 para el rojo, las siguientes para el verde y las últimas para el azul. Cada una de estas varía entre 00 y FF. Equivalencia de los colores primarios:\n\nRojo: #FF0000\nVerde: #00FF00\nAzul: #0000FF\n\nHay una correspondencia uno a uno entre ambos sistemas (RGB y HEX). Al igual que el sistema RGB, se puede agregar una propiedad de opacidad al agregar 2 posiciones más (también entre 00 y FF) al sistema Hexadecimal.\nAlgunos colores en sistema Hexadecimal\n\nplot(x = rep(1,7), col = c(\"#AD504E\", \"#5F9EA0\", \"#009ACD\", \"#435EBF\", \n                           \"#2F5A63\", \"#2F5A63A6\", \"#2F5A6355\"),\n     cex = 10, pch = 20, xlab = \"\", ylab = \"\", xlim = c(0,8),\n     main = \"Algunos colores con Hexadecimal\")\n\n\n\n\n\n\n\n\nLa equivalencia de los colores primarios en los sistemas RGB y Hex, se resume en la imagen\n\n\n\n\nTomado de https://www.w3schools.com/colors/default.asp\n\n\n\nLa función rgb() convierte el código RGB a Hexadecimal. También se puede hacer lo contrario con la función col2rgb()\n\nrgb(255, 0, 0, maxColorValue = 255) # De RGB a Hexadecimal\n\n[1] \"#FF0000\"\n\ncol2rgb(\"#FF0000\") # De Hexadecimal a RGB\n\n      [,1]\nred    255\ngreen    0\nblue     0\n\n\nDado que estamos hablando de colores, sin duda alguna lo más natural y sencillo es verlos en lugar de memorizar códigos. Hay muchas herramientas online que sirven como mezcladores de colores y nos devuelven los respectivos códigos en diferentes sistemas. Puede consultar el siguiente tutorial para más detalles\n\n\n\nEn Rstudio podemos instalar el complemento colourPicker con install.packages(\"colourpicker\"). Al hacerlo se activa en el botón Addins un nuevo complemento.",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uso de `ggplot2`</span>"
    ]
  },
  {
    "objectID": "uso-ggplot2.html#volviendo-a-ggplot2",
    "href": "uso-ggplot2.html#volviendo-a-ggplot2",
    "title": "2  Uso de ggplot2",
    "section": "2.6 Volviendo a ggplot2",
    "text": "2.6 Volviendo a ggplot2\n\npenguins |&gt; ggplot(mapping = aes(x = body_mass_g)) +\n  geom_histogram(fill=\"#FA88FC9B\", color = \"darkblue\", binwidth = 200) +\n  labs(\n    title = \"Peso de los Pingüinos del Archipiélago de Palmer\",\n    x = \"Peso en gramos\",\n    y = \"Conteo\",\n    caption = \"Datos de 333 pingüinos distribuidos en 3 islas y 3 especies\")\n\n\n\n\n\n\n\n\nPodemos cambiar la orientación del histograma al cambiar la variable en el argumento mapping y actualizar las etiquetas del gráfico\n\npenguins |&gt; ggplot(mapping = aes(y = body_mass_g)) +\n  geom_histogram(fill=\"#FFD08594\", color = \"darkblue\", binwidth = 200) +\n  labs(\n    title = \"Peso de los Pingüinos del Archipiélago de Palmer\",\n    y = \"Peso en gramos\",\n    x = \"Conteo\",\n    caption = \"Datos de 333 pingüinos distribuidos en 3 islas y 3 especies\")\n\n\n\n\n\n\n\n\nUn gráfico de densidad se obtiene con geom_density()\n\npenguins |&gt; ggplot(mapping = aes(x = body_mass_g)) +\n  geom_density(fill=\"aliceblue\", color = \"black\") +\n  labs(\n    title = \"Peso de los Pingüinos del Archipiélago de Palmer\",\n    x = \"Peso en gramos\",\n    y = \"\",\n    caption = \"Datos de 333 pingüinos distribuidos en 3 islas y 3 especies\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosición del signo “+”\n\n\n\nel signo ++ NO debe ir al comienzo de una línea, puede ir como último elemento de la línea pero no de primero\n\n\nEl siguiente código mostrará un error debido a la posición errada del signo ++\n\npenguins |&gt; ggplot(mapping = aes(x = body_mass_g))\n  + geom_density(fill=\"aliceblue\", color = \"black\") +\n  labs(\n    title = \"Peso de los Pingüinos del Archipiélago de Palmer\",\n    x = \"Peso en gramos\",\n    y = \"\",\n    caption = \"Datos de 333 pingüinos distribuidos en 3 islas y 3 especies\")\n## Error:\n## ! Cannot use `+` with a single argument.\n## ℹ Did you accidentally put `+` on a new line?\n\nSiguiendo la lógica de construcción de las gráficas anteriores, no debe ser difícil ahora crear un diagrama de cajas, la geometría necesaria tiene el nombre que uno esperaría: geom_boxplot()\n\npenguins |&gt; ggplot(aes(y = body_mass_g)) +\n  geom_boxplot() +\n  labs(\n    title = \"Diagrama de Cajas para el peso en gramos de los Pingüinos\",\n    y = \"Peso en gramos\"\n  )",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uso de `ggplot2`</span>"
    ]
  },
  {
    "objectID": "uso-ggplot2.html#caso-2.-una-variable-numérica-y-una-categórica-o-factor",
    "href": "uso-ggplot2.html#caso-2.-una-variable-numérica-y-una-categórica-o-factor",
    "title": "2  Uso de ggplot2",
    "section": "2.7 Caso 2. Una variable numérica y una categórica o factor",
    "text": "2.7 Caso 2. Una variable numérica y una categórica o factor\nAl agregar una variable categórica o factor, podemos obtener información más detallada de la distribución del peso de los pingüinos por especie o isla\n\npenguins |&gt; ggplot(aes(x = body_mass_g, fill=species)) +\n  geom_histogram(color=\"black\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n¿Cómo actúa el argumento fill dentro de aes() y por qué es diferente cuando se usa como argumento dentro de geom_histogram?6\n6 Responder esta pregunta es importante para entender el funcionamiento de ggplot2Note que se agregan leyendas de forma automática y las podemos personalizar. El argumento alpha agrega opacidad (transparencia) al color, debe ser un valor entre 0 (totalmente transparente) y 1 (totalmente opaco)\n\npenguins |&gt; ggplot(aes(x = body_mass_g, fill = species)) +\n  geom_histogram(color = \"darkblue\", binwidth = 200, alpha = 0.7) +\n  labs(\n    title = \"Peso de los Pingüinos del Archipiélago de Palmer\",\n    x = \"Peso en gramos\",\n    y = \"Conteo\",\n    fill = \"Especie\",\n    caption = \"Datos de 333 pingüinos discriminados por especies\")\n\n\n\n\n\n\n\n\nAl igual que con el histograma, podemos combinar la variable categórica para construir gráficos de densidad y gráficos de caja por cada categoría\n\npenguins |&gt; ggplot(aes(x = body_mass_g, fill = species)) +\n  geom_density(alpha = 0.5) +\n  labs(\n    title = \"Distribución del peso por especie\",\n    subtitle = \"Datos de pingüinos del archipiélago de Palmer\",\n    fill = \"Especie\",\n    y = \"\",\n    x = \"Peso en gramos\"\n  )\n\n\n\n\n\n\n\n\nLos gráficos de densidad por categoría sobre la misma escala son muy útiles. El resultado anterior puede ser un poco desalentador ya que los gráficos se superponen, agregamos transparencia al color para tener una mejor lectura, pero una mejor opción es variar un poco la escala en el eje vertical para cada densidad7. ggplot2 no tiene soporte para este tipo de gráficos, pero podemos agregar una extensión que nos permite crearlos fácilmente (existen muchas extensiones para ggplot). La librería que debemos agregar es ggridges, instalamos desde la consola install.packages(\"ggridges\") y cargamos la librería para poder usarla: library(ggridges)\n7 Estos son los llamados ridgeline plots o joyplots\n# install.packages(\"ggridges\") # haga esto en la consola\nlibrary(ggridges)\npenguins |&gt; ggplot(aes(x = body_mass_g, y = species)) +\n  geom_density_ridges()\n\nPicking joint bandwidth of 153\n\n\n\n\n\n\n\n\n\nPodemos controlar el aspecto del gráfico con algunos parámetros que ya son familiares, tales como color, fill entre otros. El parámetro scale permite controlar la superposición de los gráficos, cuando toma el valor de 1, los gráficos apenas se tocan. Entre más cercano el valor a 0, más separados, entre más alto el valor, estarán más superpuestos\n\np1 &lt;- penguins |&gt; ggplot(aes(x = body_mass_g, y = species, fill = species)) +\n  labs(\n    title = \"Distribución del peso por especie\",\n    subtitle = \"Datos de pingüinos del archipiélago de Palmer\",\n    fill = \"Especie\",\n    y = \"Especie\",\n    x = \"Peso en gramos\"\n  )\np1 + geom_density_ridges(color = \"red\", \n                      alpha = 0.7, \n                      size = 1,\n                      scale = 0.5)\n\n\n\n\n\n\n\np1 + geom_density_ridges(alpha = 0.6,\n                      scale = 5)\n\n\n\n\n\n\n\n\nTeniendo en cuenta lo aprendido hasta ahora, intente reproducir el siguiente gráfico de cajas del peso de los pingüinoss por islas\n\n\nClick para mostrar el código\npenguins |&gt; ggplot(aes(x = island, y = body_mass_g, fill = island)) +\n  geom_boxplot(alpha = 0.8) +\n  labs(\n    title = \"Distribución del peso por isla\",\n    subtitle = \"Datos de pingüinos del archipiélago de Palmer\",\n    fill = \"Especie\",\n    x = \"Isla\",\n    y = \"Peso en gramos\"\n  )\n\n\n\n\n\n\n\n\n\n\nEjercicio 2.1 Invierta la orientación del gráfico anterior\n\n\nUn gráfico muy popular con un propósito similar al diagrama de cajas, es el gráfico de violín. Se puede construir usando ggplot2 con la misma sencillez que los anteriores gráficos\n\npenguins |&gt; ggplot(aes(x = island, y = body_mass_g, fill = island)) +\n  geom_violin() +\n  labs(\n    title = \"Distribución del peso por Isla\",\n    x = \"Isla\",\n    y = \"Peso en gramos\",\n    fill = \"Isla\"\n  )\n\n\n\n\n\n\n\n\nSería interesante agregar sobre el mismo sistema de coordenadas, gráfico de violín y gráfico de caja\n\npenguins |&gt; ggplot(aes(x = island, y = body_mass_g, fill = island)) +\n  geom_violin() +\n  geom_boxplot()+\n  labs(\n    title = \"Distribución del peso por Isla\",\n    x = \"Isla\",\n    y = \"Peso en gramos\",\n    fill = \"Isla\"\n  )\n\n\n\n\n\n\n\n\nEsta combinación de gráficos es bastante usual, aunque nuestro resultado no es el mejor, afinarlo requiere del uso de otras librerías. Más adelante volveremos a este tipo de gráficos.",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uso de `ggplot2`</span>"
    ]
  },
  {
    "objectID": "uso-ggplot2.html#caso-3.-dos-variables-numéricas",
    "href": "uso-ggplot2.html#caso-3.-dos-variables-numéricas",
    "title": "2  Uso de ggplot2",
    "section": "2.8 Caso 3. Dos variables numéricas",
    "text": "2.8 Caso 3. Dos variables numéricas\nEl gráfico más común para este caso es el diagrama de dispersión. Observemos la relación que hay entre el peso del pingüino y la longitud de la aleta\n\npenguins |&gt; ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\n\n\n\n\n\n\n\n\nCon algo más de trabajo, podemos personalizar8 el resultado, agregando una forma a los puntos con shape, modificando el tamaño de los puntos con size, coloreando con colour y fill, modificando el ancho del marco de los puntos con stroke\n8 Personalizar no quiere decir necesariamente mejorar\npenguins |&gt; ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(shape = 23, fill = \"white\", \n             colour = \"purple\", size = 3, \n             stroke = 3)\n\n\n\n\n\n\n\n\nEn R hay un total de 25 formas (argumento shape) básicas para representar estos puntos\n\ndata.frame(x = 1:25, y = 1:25) |&gt; ggplot(aes(x = x, y = y)) +\n  geom_point(shape = 1:25, size = 5)\n\n\n\n\n\n\n\n\nTodas estas formas admiten el argumento de color y de la forma 21 a 25 admiten además un argumento de relleno (fill). Reuniendo todo lo anterior, podemos crear un gráfico más detallado\n\npenguins |&gt; ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(shape = 22, fill = \"#37C790E6\", colour = \"black\",\n             size = 2, stroke = 1) +\n  labs(\n    title = \"Peso vs Longitud de la aleta\",\n    subtitle = \"Datos de 3 especies de pingüinos del archipiélago de Palmer\",\n    x = \"Longitudad de la aleta (mm)\",\n    y = \"Peso (gr)\"\n  )\n\n\n\n\n\n\n\n\nPodemos agregar distribuciones marginales a este diagrama de dispersión, usando el paquete ggExtra. Instale desde la consola: install.packages(\"ggExtra\"). Veamos un ejemplo\n\nlibrary(ggExtra)\np1 &lt;- penguins |&gt; ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(shape = 22, fill = \"#37C790E6\", colour = \"black\",\n             size = 2, stroke = 1) +\n  labs(\n    title = \"Peso vs Longitud de la aleta\",\n    subtitle = \"Datos de 3 especies de pingüinos del archipiélago de Palmer\",\n    x = \"Longitudad de la aleta (mm)\",\n    y = \"Peso (gr)\"\n  )\n\nggMarginal(p = p1)\n\n\n\n\n\n\n\n\nLa función ggMarginal del paquete ggExtra no funciona con el operador pipe, por lo que es necesario guardar el objeto ggplot2 (gráfico) en una variable para luego aplicar la función ggMarginal sobre esta variable. Cambiando algunos detalles del gráfico anterior, obtenemos\n\nlibrary(ggExtra)\np1 &lt;- penguins |&gt; ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(shape = 22, fill = \"#37C790E6\", colour = \"black\",\n             size = 2, stroke = 1) +\n  labs(\n    title = \"Peso vs Longitud de la aleta\",\n    subtitle = \"Datos de 3 especies de pingüinos del archipiélago de Palmer\",\n    x = \"Longitudad de la aleta (mm)\",\n    y = \"Peso (gr)\"\n  )\n\nggMarginal(p = p1, \n           type = \"histogram\", \n           size = 4, \n           fill = \"slateblue\",\n           bins = 20)\n\n\n\n\n\n\n\n\nOtro tipo de gráfico útil para estudiar la relación entre dos variables numéricas es el de densidad en dos dimensiones.\n\npenguins |&gt; ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_density_2d()\n\n\n\n\n\n\n\n\nPara modificar de forma manual la escala de valores de los ejes, agregamos xlim(inf_x, sup_x) y ylim(inf_y, sup_y)\n\npenguins |&gt; ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_density_2d() +\n  xlim(NA, 240) +\n  ylim(2000, 6500) +\n  labs(\n    title = \"Densidad conjunta\",\n    subtitle = \"Datos de 3 especies de pingüinos del archipiélago de Palmer\",\n    x = \"Longitudad de la aleta (mm)\",\n    y = \"Peso (gr)\"\n  )\n\n\n\n\n\n\n\n\nO en un intento de automatizar más el cálculo de la escala de los ejes\n\npenguins |&gt; ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_density_2d() +\n  xlim(0.98*min(penguins$flipper_length_mm), \n       1.02*max(penguins$flipper_length_mm)) +\n  ylim(0.95*min(penguins$body_mass_g), \n       1.02*max(penguins$body_mass_g)) +\n  labs(\n    title = \"Densidad conjunta\",\n    subtitle = \"Datos de 3 especies de pingüinos del archipiélago de Palmer\",\n    x = \"Longitudad de la aleta (mm)\",\n    y = \"Peso (gr)\"\n  )\n\n\n\n\n\n\n\n\nVolveremos después a revisar con más detalle estos gráficos.",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uso de `ggplot2`</span>"
    ]
  },
  {
    "objectID": "uso-ggplot2.html#caso-4.-dos-variables-numéricas-y-una-categórica",
    "href": "uso-ggplot2.html#caso-4.-dos-variables-numéricas-y-una-categórica",
    "title": "2  Uso de ggplot2",
    "section": "2.9 Caso 4. Dos variables numéricas y una categórica",
    "text": "2.9 Caso 4. Dos variables numéricas y una categórica\nEn el siguiente gráfico de dispersión, agregamos la información de la especie y además un modelo ajustado a los datos. Comparemos los dos resultados para comprender una característica fundamental de ggplot2\n\npenguins |&gt; ggplot(aes(x = flipper_length_mm, \n                       y = body_mass_g, \n                       colour = species)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\npenguins |&gt; ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(mapping = aes(colour = species)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLos valores de data y magpping = aes() declarados en la función ggplot() se consideran globales, cada geometría o capa agregada heredará estos valores, pero pueden ser reemplazados por declaraciones locales dentro de cada geometría. Eso quiere decir que cada geometría puede tener su propio conjunto de datos y su propia forma de mapear los datos en el gráfico, permitiendo así una gran flexibilidad.\nEn el primer código se mapeó el atributo color con la variable especie, por eso al aplicar geom_smooth() se crea una estimación para cada tipo de especie. En el segundo código, el mapeo de color se hace dentro de geom_point() por lo que solo aplica para esa geometría en particular, al aplicar geom_smooth() no se tiene en cuenta ya que no se ha declarado globalmente dentro de la función ggplot()\nExplique cada componente del siguiente gráfico\n\npenguins |&gt; ggplot(aes(x = flipper_length_mm, y = body_mass_g,)) +\n  geom_point(aes(color = species, shape = species), size = 2) +\n  geom_smooth(se = FALSE, color = \"#8A0200\", method = \"lm\") +\n  labs(\n    title = \"Peso Vs Longitud de la aleta\",\n    subtitle = \"Datos de 3 especies de pingüinos del archipiélago de Palmer\",\n    x = \"Longitudad de la aleta (mm)\",\n    y = \"Peso (gr)\",\n    color = \"Especie\",\n    shape = \"Especie\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nUsando el paquete ggthemes podemos mejorar la presentación del gráfico9\n9 Intente varios temas\nlibrary(ggthemes)\npenguins |&gt; ggplot(aes(x = flipper_length_mm, y = body_mass_g,)) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(se = FALSE, method = \"lm\") +\n  labs(\n    title = \"Peso Vs Longitud de la aleta\",\n    subtitle = \"Datos de 3 especies de pingüinos del archipiélago de Palmer\",\n    x = \"Longitudad de la aleta (mm)\",\n    y = \"Peso (gr)\",\n    color = \"Especie\",\n    shape = \"Especie\"\n  ) + ggthemes::scale_colour_colorblind()\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uso de `ggplot2`</span>"
    ]
  },
  {
    "objectID": "uso-ggplot2.html#geometrías",
    "href": "uso-ggplot2.html#geometrías",
    "title": "2  Uso de ggplot2",
    "section": "2.10 Geometrías",
    "text": "2.10 Geometrías\nYa que hemos revisado el funcionamiento básico de ggplot2, podemos echarle un vistazo a las geometrías disponibles. La apariencia de la mayoría de ellas puede deducirse de su nombre, podemos encontrar una explicación detallada en la página de referencia de ggplot2\n\nUtilizadas hasta ahora\n\ngeom_point()\ngeom_histogram()\ngeom_density()\ngeom_density_2d()\ngeom_boxplot()\ngeom_smooth()\n\nAlgunas que faltan\n\ngeom_abline(), geom_hline(), geom_vline(): Para agregar líneas rectas oblicuas (dado un intercepto y pendiente), horizontales y verticales\ngeom_bar(), geom_col(): Para construir diagramas de barras \ngeom_density_2d_filled: Para gráficas de contorno con relleno 🤯\ngeom_function()\ngeom_freqpoly()\ngeom_path(), geom_line()\n\n\nComo se ha dicho en varias ocasiones, gran parte del trabajo es aprender el tipo de geometría a usar dependiendo del tipo de variables involucradas. Observemos las siguientes gráficas a las que no se les agrega mayor detalle para revisar rápidamente las diferentes geometrías.\n\n2.10.1 geom_abline(), geom_hline(), geom_vline()\nLas líneas admiten parámetros de linewidth, color, linetype, alpha. El siguiente gráfico no tiene otro propósito diferente al de utilizar varias de estas opciones, aunque el gráfico resultante sea poco útil\n\npenguins |&gt; ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point() +\n  geom_hline(yintercept = c(3000, 4000, 6000),\n             color = \"red\",\n             linewidth = 1,\n             linetype = 1) +\n  geom_vline(xintercept = seq(170, 230, length.out = 6),\n             color = \"blue\", \n             linetype = 5) +\n  geom_abline(slope = 60,\n              intercept = c(-6000,-10000)) +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"blue\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nPara el caso de geom_hline() y geom_vline() se deben agregar las intercepciones en el eje Y y el eje X, respectivamente. Esto puede hacerse a través de un valor numérico o un vector. Para el caso de geom_abline() se requiere una pendiente (slope) y un intercepto (intercept), ggplot2 reciclará uno de los valores si es necesario.\nLos tipos de línea disponibles son: blank, solid, dashed, dotted, dotdash, longdash, twodash. Se pueden identificar de forma ordenada con los dígitos del 0 al 6, por lo que linetype = \"dashed\" es equivalente a linetype = 2\n\nggplot() +\n  geom_hline(yintercept = 1:7, linetype = 0:6)\n\n\n\n\n\n\n\n\nEl gráfico anterior muestra algo interesante, estas geometrías en realidad no necesitan una base de datos para poder crearse, ya que son reglas o funciones. Para los más exigentes en la personalización, se puede crear un patrón con hasta 8 dígitos hexadecimales (1,…,9,a,b,…,f) indicando un patrón de línea y espacio (así que deben ser 2,4,6 u 8 dígitos). El valor 1 es el menor y f el mayor. El patrón “5a3f” indica una línea de longitud 5 seguida de un espacio de longitud a luego una línea de longitud 3 y un espacio de longitud f, este patrón se repetirá hasta completar la gráfica.\n\nggplot() +\n  geom_hline(yintercept = 1:4,\n             color = \"red\",\n             linewidth = 2,\n             linetype = c(\"11\", \"f4f2\", \"224466\", \"a5b6c7d8\"))\n\n\n\n\n\n\n\n\n\n\n2.10.2 geom_bar(), geom_col()\ngeom_bar() y geom_col() son adecuados para hacer gráficas que incluyen variables categóricas, con geom_bar() se cuenta el número de casos de cada categoría y con geom_col() se grafican los valores en los datos para cada categoría.\n\npenguins |&gt; ggplot(aes(x = island)) +\n  geom_bar(fill = \"#ddffeeaa\", color = \"black\")\n\n\n\n\n\n\n\n\nSi agregamos una segunda variable categórica, las barras saldrán apiladas. Mapeamos por ejemplo fill = species\n\npenguins |&gt; ggplot(aes(x = island, fill = species)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nAlgunas veces, es más útil mostrar la proporción de una categoría dentro de otra. En este caso, cómo está distribuida la proporción de cada especie en cada isla\n\npenguins |&gt; ggplot(aes(x = island, fill = species)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\nPodemos agregar las barras por categoría, lado a lado usando position = \"dodge\" o position = \"dodge2\", en la segunda opción se agregará un espacio para separar los grupos en cada categoría\n\np1 &lt;- penguins |&gt; ggplot(aes(x = island, fill = species)) +\n  geom_bar(position = \"dodge\")\np2 &lt;- penguins |&gt; ggplot(aes(x = island, fill = species)) +\n  geom_bar(position = \"dodge2\")\np1; p2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPara mostrar el funcionamiento de geom_col() agrupamos los datos en penguins con la variable species y agregamos algunas variables nuevas10\n10 En el siguiente capítulo veremos con detalle estas funciones\ndata &lt;- penguins |&gt; group_by(species) |&gt; \n  summarise(cantidad = n(), \n            peso_promedio = mean(body_mass_g), \n            peso_total = sum(body_mass_g),\n            ind1 = mean(body_mass_g/flipper_length_mm))\ndata\n\n# A tibble: 3 × 5\n  species   cantidad peso_promedio peso_total  ind1\n  &lt;fct&gt;        &lt;int&gt;         &lt;dbl&gt;      &lt;int&gt; &lt;dbl&gt;\n1 Adelie         146         3706.     541100  19.5\n2 Chinstrap       68         3733.     253850  19.0\n3 Gentoo         119         5092.     606000  23.4\n\n\nQueremos observar en un gráfico de barras las diferencias en el peso promedio de los pingüinos según la especie a la que pertenecen\n\ndata |&gt; ggplot(aes(x = species, y = peso_promedio)) +\n  geom_col()\n\n\n\n\n\n\n\n\nEstudie el siguiente gráfico y explique cada parte del código\n\nn &lt;- 100\ndata1 &lt;- tibble(\n  fact1 = sample(c(\"a\", \"b\", \"c\", \"d\"), size = n, replace = T),\n  fact2 = sample(c(\"g1\", \"g2\", \"g3\"), size = n, replace = T),\n  num1 = rnorm(n, 20, 5)\n)\ndata1 &lt;- data1 |&gt; group_by(fact1, fact2) |&gt; \n  summarise(var_interesante = (mean(num1)))\n\n`summarise()` has grouped output by 'fact1'. You can override using the\n`.groups` argument.\n\ndata1\n\n# A tibble: 12 × 3\n# Groups:   fact1 [4]\n   fact1 fact2 var_interesante\n   &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 a     g1               24.1\n 2 a     g2               22.0\n 3 a     g3               16.3\n 4 b     g1               18.2\n 5 b     g2               20.4\n 6 b     g3               19.8\n 7 c     g1               18.8\n 8 c     g2               22.4\n 9 c     g3               20.2\n10 d     g1               18.7\n11 d     g2               17.6\n12 d     g3               20.7\n\ndata1 |&gt; ggplot(aes(x = fact1, y = var_interesante, fill = fact2)) +\n  geom_col(position = \"dodge2\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.10.3 geom_density_2d_filled()\nDe uso muy similar a geom_density_2d(), pero agretando un color de relleno al gráfico. Se puede combinar con geom_point() para ofrecer una gráfica más completa. Con todo lo visto hasta ahora, debe ser claro cómo funciona el siguiente código\n\npenguins |&gt; ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_density_2d_filled(alpha = 0.5, show.legend = FALSE) +\n  geom_density_2d(color = \"black\") +\n  xlim(NA, 240) +\n  ylim(2000, 6500) +\n  geom_point(size = 0.8, color = \"magenta\") +\n  labs(\n    title = \"Densidad conjunta\",\n    subtitle = \"Datos de 3 especies de pingüinos del archipiélago de Palmer\",\n    x = \"Longitudad de la aleta (mm)\",\n    y = \"Peso (gr)\",\n    \n  )\n\n\n\n\n\n\n\n\n\n\n2.10.4 geom_function()\nPodemos construir curvas a partir de funciones con geom_function(). En el siguiente código, construimos 2 funciones muy sencillas y las agregamos con diferente color, la capa coord_fixed nos sirve para mantener la relación 1 a 1 entre los valores de los ejes coordenados (la mayoría de veces, no es buena idea hacer esto, es mejor dejar que se reescalen los ejes)\n\nf1 &lt;- function(x) x^2\nf2 &lt;- function(x) x^3\nggplot() + geom_function(fun = f1, color = \"blue\") +\n  geom_function(fun = f2, color = \"red\") +\n  xlim(-1,1) +\n  coord_fixed()\n\n\n\n\n\n\n\n\nPodemos usar directamente las funciones de R. A continuación graficamos 3 distribuciones normales con la misma desviación y diferentes valores de la media\n\nggplot() +\n  xlim (-5,5) +\n  geom_function(fun = dnorm, \n                args = list(mean = -2, sd = 1),\n                color = \"blue\") +\n  geom_function(fun = dnorm, \n                args = list(mean = 0, sd = 1),\n                color = \"red\") +\n  geom_function(fun = dnorm, \n                args = list(mean = 2, sd = 1),\n                color = \"orange\")\n\n\n\n\n\n\n\n\n\n\n2.10.5 geom_freqpoly()\nUn polígono de frecuencia es un buen complemento para un histograma\n\npenguins |&gt; ggplot(aes(x = body_mass_g)) +\n  geom_histogram(fill = \"#C820387f\", \n                 color = \"black\",\n                 bins = 15) +\n  geom_freqpoly(bins = 15,\n                linewidth = 1,\n                color = \"#00B82ECC\")\n\n\n\n\n\n\n\n\n\n\n2.10.6 geom_path(), geom_line()\nEstas geometrías unen puntos con líneas, geom_line() lo hace en orden ascendente de la variable x, mientras que geom_path() lo hace en el orden mismo que aparecen los datos. geom_line() es bastante útil para representar series de tiempo y geom_path() puede bocetar formas irregulares. Usamos la base de datos de serie de tiempo economics_long11 que viene con ggplot2 para mostrar el uso de geom_line(). Agregamos una forma geométrica cualquiera para el uso de geom_path()\n11 Tanto economics como economics_long traen la misma información pero una está en formato “ancho” y la otra en formato “largo”, investigue a qué se refieren estos términos\n# Revisar los primeros valores de la base de datos\neconomics_long\n\n# A tibble: 2,870 × 4\n   date       variable value  value01\n   &lt;date&gt;     &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 1967-07-01 pce       507. 0       \n 2 1967-08-01 pce       510. 0.000265\n 3 1967-09-01 pce       516. 0.000762\n 4 1967-10-01 pce       512. 0.000471\n 5 1967-11-01 pce       517. 0.000916\n 6 1967-12-01 pce       525. 0.00157 \n 7 1968-01-01 pce       531. 0.00207 \n 8 1968-02-01 pce       534. 0.00230 \n 9 1968-03-01 pce       544. 0.00322 \n10 1968-04-01 pce       544  0.00319 \n# ℹ 2,860 more rows\n\n# graficar\neconomics_long |&gt; ggplot(aes(x = date, y = value01, color = variable)) +\n  geom_line(linewidth = 0.8)\n\n\n\n\n\n\n\n\n\ndata2 &lt;- tibble(\n  x = c(0.5, 3, 5.5, 0, 6, 0.5),\n  y = c(0, 6, 0, 4, 4, 0)\n)\ndata2 |&gt; ggplot(aes(x = x, y = y)) +\n  geom_path() +\n  theme(aspect.ratio = 1)",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uso de `ggplot2`</span>"
    ]
  },
  {
    "objectID": "uso-ggplot2.html#agregando-facetas-o-subplots",
    "href": "uso-ggplot2.html#agregando-facetas-o-subplots",
    "title": "2  Uso de ggplot2",
    "section": "2.11 Agregando facetas o subplots",
    "text": "2.11 Agregando facetas o subplots\nSi tenemos una variable categórica dentro de nuestro conjunto de variables, una forma de analizar su contribución o patrón es mapeando con un atributo del gráfico como fill o color, tal como lo hemos hecho en ocasiones anteriores\n\npenguins |&gt; ggplot(aes(x = bill_length_mm, \n                       y = body_mass_g,\n                       color = species)) +\n  geom_point()\n\n\n\n\n\n\n\n\nOtra alternativa es construir un gráfico por separado para cada grupo o valor de la variable categórica, para este ejemplo en particular, sería construir un diagrama de dispersión para cada subconjunto de datos según la especie. Esto es fácil de conseguir usando face_wrap() o face_grid(). La primera alternativa crea un subgráfico de forma lineal para una variable categórica que se incluya usando una fórmula12 y la segunda opción crea un cuadrícula usando 2 variables categóricas. Es más fácil viendo que diciendo\n12 revise el cocepto de fórmula en R\npenguins |&gt; ggplot(aes(x = flipper_length_mm,\n                       y = body_mass_g)) +\n  geom_point(aes(color = species)) + \n  geom_smooth() +\n  facet_wrap(facets = ~species, nrow = 2)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\npenguins |&gt; ggplot(aes(x = flipper_length_mm,\n                       y = body_mass_g)) +\n  geom_point(aes(color = species)) + \n  geom_smooth() +\n  facet_grid(species~island)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNote que mientras face_wrap() usa una fórmula de un solo lado, face_grid() usa una fórmula de 2 lados.\nPor defecto, todos los gráficos compartirán la misma escala en ambos ejes, se puede modificar este comportamiento con el argumento scales, cuyos valores posibles son: fixed (por defecto), free (ambos ejes se ajustan de manera automática según los valores en cada gráfico), free_x (se fija el eje y), free_y (se fija el eje x)\n\npenguins |&gt; ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species)) +\n  geom_smooth() +\n  facet_wrap(~species, nrow = 2, scales = \"free\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uso de `ggplot2`</span>"
    ]
  },
  {
    "objectID": "uso-ggplot2.html#más-allá-de-ggplot2",
    "href": "uso-ggplot2.html#más-allá-de-ggplot2",
    "title": "2  Uso de ggplot2",
    "section": "2.12 Más allá de ggplot2",
    "text": "2.12 Más allá de ggplot2\nUna de las grandes fortalezas de R para análisis de datos es su increíble cantidad de herramientas para hacer gráficas. Esta gran oferta puede resultar un poco abrumadora al principio, por lo que en este curso nos vamos a basar casi que exclusivamente en ggplot2 para hacer nuestras gráficas y ni siquiera usaremos con regularidad la función base plot. Puede parecer poco, pero ggplot2 es bastante amplio y seguramente no abarcaremos todas las opciones que ofrece.\nDicho eso, vamos a mencionar algunas librerías que agregan fortaleces a ggplot2 o funcionan de forma complementaria y algunas que son ecosistemas totalmente independientes, pero son tan geniales que debemos saber por lo menos que existen.\nYa hemos agregado algunas librerías: ggridges, ggExtra, ggthemes. Podemos encontrar una lista asombrosa de extras para ggplot2 aquí\n\n2.12.1 patchwork\nEsta librería permite combinar de forma sencilla varias gráficas individuales en una sola gráfica. Debemos instalar (install.packages(\"patchwork\")) y cargar (library(patchwork)) el paquete. Revise la página de patchwork para ver los detalles. Para revisar el uso básico, vamos a crear algunos gráficos individuales sencillos y combinarlos de diferentes formas\n\nlibrary(patchwork)\np1 &lt;- penguins |&gt; ggplot(aes(x = flipper_length_mm,\n                             y = body_mass_g,\n                             color = species)) +\n  geom_point(show.legend = F)\n\np2 &lt;- penguins |&gt; ggplot(aes(x = island, \n                             y = bill_depth_mm,\n                             color = island)) +\n  geom_boxplot(show.legend = F)\n\np3 &lt;- penguins |&gt; ggplot(aes(x = bill_length_mm,\n                             y = species,\n                             color = species,\n                             fill = species)) +\n  geom_density_ridges(alpha = 0.5, show.legend = F)\n\np4 &lt;- penguins |&gt; ggplot(aes(x = island, fill = species)) +\n  geom_bar(position = \"dodge2\", show.legend = F)\n\nPodemos agrupar los gráficos con el signo +, patchwork intentará distribuir los gráficos en filas y columnas de forma que el resultado sea lo más cuadrado posible, agregando los gráficos por filas (como si fuese una matriz en R). Podemos forzar la distribución usando plot_layout()\n\np1 + p2 + p3 + p4\n\nPicking joint bandwidth of 1.08\n\n\n\n\n\n\n\n\n\n\np1 + p2 + p3 + p4 + plot_layout(nrow = 3, ncol = 2, byrow = F)\n\nPicking joint bandwidth of 1.08\n\n\n\n\n\n\n\n\n\nSe puede forzar un layout con “|” para separar columnas y “/” para separar filas. Probemos algunas combinaciones\n\n(p1 / p2) | p3\n\nPicking joint bandwidth of 1.08\n\n\n\n\n\n\n\n\n(p1 | p4) / p2\n\n\n\n\n\n\n\np1 / p2 / p3\n\nPicking joint bandwidth of 1.08\n\n\n\n\n\n\n\n\n(p1 + p2) | p4\n\n\n\n\n\n\n\n\nSe pueden agregar etiquetas y títulos a cada gráfico de forma normal a cada gráfico individual, también se puede para el conjunto completo, usando plot_annotation(). Con plot_layout() se puede especificar la proporción de ancho y alto para cada columna y fila del arreglo.\n\n(p1 | (p2 / p3 + plot_layout(heights = c(2,1)))) +\n  plot_layout(widths = c(3,2)) +\n  plot_annotation(\n    title = \"Un grupo interesante de gráficas\",\n    subtitle = \"Gráficas bonitas\", \n    caption = \"Datos de Pingüinos del archipiélago de Palmer\"\n  )\n\nPicking joint bandwidth of 1.08\n\n\n\n\n\n\n\n\n\nPor último nombraremos 2 paquetes para gráficas pero que juegan en una liga diferente, están basados en librerías de JavaScript por lo que pueden generar gráficas interactivas y son especialmente adecuados para generar reportes con quarto y aplicaciones con shiny\n\n\n2.12.2 dygraphs\nEl paquete dygraphs de R es una implementación de la librería dygraphs de Javascript. Es bastante usado para graficar datos de series de tiempo. El siguiente ejemplo es tomado de la documentación\n\n# install.packages(\"dygraphs\") # ejecutar desde la consola\nlibrary(dygraphs)\nlungDeaths &lt;- cbind(mdeaths, fdeaths)\ndygraph(lungDeaths) |&gt; \n  dySeries(\"mdeaths\", label = \"Male\") |&gt; \n  dySeries(\"fdeaths\", label = \"Female\") |&gt; \n  dyOptions(stackedGraph = TRUE) |&gt; \n  dyRangeSelector(height = 20)\n\n\n\n\n\n\n\n2.12.3 plotly\nplotly es una librería declarativa y de alto nivel basada en D3.js y stack.gl para crear gráficos interactivos. Tiene versiones para python, R, Julia, Javascript, Matlab, F# (¡casi nada!). Ofrece muchos tipos de gráficos para 2 y 3 dimensiones y es un mundo completo para aprender. Por ahora solo nos interesa la función ggplotly que convierte cualquier gráfico estático ggplot en un gráfico interactivo plotly\n\n# install.packages(\"plotly\") # ejecutar desde la consola\nlibrary(plotly)\npp1 &lt;- penguins |&gt; ggplot(aes(x = flipper_length_mm,\n                             y = body_mass_g,\n                             color = species)) +\n  geom_point(show.legend = F)\nggplotly(pp1)",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uso de `ggplot2`</span>"
    ]
  },
  {
    "objectID": "uso-dplyr.html",
    "href": "uso-dplyr.html",
    "title": "3  Uso de dplyr",
    "section": "",
    "text": "3.1 Operaciones sobre filas\nNada que agregar, son operaciones o funciones que modifican de alguna forma las filas u observaciones en la base de datos. Dentro de estas funciones tenemos operaciones como filtrar y ordenar. Las funciones principales en este grupo son:\nPodemos ordenar las filas en diamonds en orden ascendente o descendente según el precio, o cualquier otra variable numérica\ndiamonds |&gt; arrange(price)\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\ndiamonds |&gt; arrange(desc(price))\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  2.29 Premium   I     VS2      60.8    60 18823  8.5   8.47  5.16\n 2  2    Very Good G     SI1      63.5    56 18818  7.9   7.97  5.04\n 3  1.51 Ideal     G     IF       61.7    55 18806  7.37  7.41  4.56\n 4  2.07 Ideal     G     SI2      62.5    55 18804  8.2   8.13  5.11\n 5  2    Very Good H     SI1      62.8    57 18803  7.95  8     5.01\n 6  2.29 Premium   I     SI1      61.8    59 18797  8.52  8.45  5.24\n 7  2.04 Premium   H     SI1      58.1    60 18795  8.37  8.28  4.84\n 8  2    Premium   I     VS1      60.8    59 18795  8.13  8.02  4.91\n 9  1.71 Premium   F     VS2      62.3    59 18791  7.57  7.53  4.7 \n10  2.15 Ideal     G     SI2      62.6    54 18791  8.29  8.35  5.21\n# ℹ 53,930 more rows\nSi queremos ver los valores únicos de las variables que son categóricas2 (cut, color y clarity) podemos usar distinct(cat1) o combinaciones únicas de varias de ellas con distinct(fact1, fact2)\ndiamonds |&gt; distinct(cut)\n\n# A tibble: 5 × 1\n  cut      \n  &lt;ord&gt;    \n1 Ideal    \n2 Premium  \n3 Good     \n4 Very Good\n5 Fair     \n\ndiamonds |&gt; distinct(color, clarity) |&gt; arrange(color, desc(clarity))\n\n# A tibble: 56 × 2\n   color clarity\n   &lt;ord&gt; &lt;ord&gt;  \n 1 D     IF     \n 2 D     VVS1   \n 3 D     VVS2   \n 4 D     VS1    \n 5 D     VS2    \n 6 D     SI1    \n 7 D     SI2    \n 8 D     I1     \n 9 E     IF     \n10 E     VVS1   \n# ℹ 46 more rows\nNote que en el último código, luego de captar las combinaciones únicas de color y clarity, se ordenan los datos de forma ascendente para color y para valores repetidos de color se ordenan de forma descendente para clarity.\nLa base de datos diamonds es un tibble. Un tibble es un data.frame pero con algunas características especiales. Su principal diferencia con un data.frame es que al momento de imprimir la base de datos, no se imprimen todos los datos sino solo las primeras 10 observaciones, algo bastante bueno si se está trabajando con bases de datos con muchas observaciones. Si deseamos imprimir todas las observaciones o una cantidad determinada de ellas, debemos solicitarlo3\nprint(diamonds, n = 20)\ndiamonds |&gt; distinct(color, clarity) |&gt; \n  arrange(color, desc(clarity)) |&gt; print(n = 56) # todas (mejórelo)\nSiempre que aplicamos distinct(), la base de datos resultante, no tendrá todas las variables originales, solo mantendrá aquellas involucradas en la operación (¿por qué eso puede ser deseable?)\nHay una forma de mantener todas las variables, con el argumento y valor .keep_all = TRUE (¿por qué es necesario poner un punto al inicio?). En este caso, se mantendrá la primera coincidencia para cada combinaicón única encontrada.\ndiamonds |&gt; distinct(clarity, color, .keep_all = TRUE)\n\n# A tibble: 56 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 46 more rows\nfilter() es quizás, la función más usada en filas. Nos permite seleccionar un subconjunto de los datos que cumplen uno o más criterios. La función requiere una base de datos como primer argumento y luego una serie de condiciones sobre los valores de una o más variables. Estas condiciones deben devolver un vector de valores lógicos que indicarán las filas a seleccionar.\nEs buen momento para recordar algunas operaciones con valores lógicos en R. Los valores lógicos son TRUE y FALSE o T y F. Podemos obtener estos valores al comparar con: &gt;, &gt;=, &lt;, &lt;=, ==, !=\n2 == 5 # FALSE\n\n[1] FALSE\n\n3 &gt;= 0 # TRUE\n\n[1] TRUE\n\n2 &lt; -3 # FALSE\n\n[1] FALSE\n\n3 != 6 # TRUE\n\n[1] TRUE\nEl operador “o” es |, el operador “y” es &, el\n(2 &gt; 0) & (10 &gt; 20) # TRUE y FALSE es FALSE\n\n[1] FALSE\n\n(2 &gt; 0) | (10 &gt; 20) # TRUE o FALSE es TRUE\n\n[1] TRUE\nClaramente, estas operaciones se pueden aplicar sobre vectores\nc(2,6,9,3) &gt;= 5\n\n[1] FALSE  TRUE  TRUE FALSE\n\n(c(7, 2, 9, 3, 1) &lt; 4) & (c(1, 2, 8, 2, 0))\n\n[1] FALSE  TRUE FALSE  TRUE FALSE\nComo se mencionó, el primer argumento en dplyr es una base de datos, y podemos pensar lo siguiente como un vector de valores lógicos donde se debe tener un valor de TRUE en cada posición que se quiera filtrar o extraer. Para entender con claridad el funcionamiento de filter(), vamos a crear una pequeña base de datos\nset.seed(123)\ndata_prueba &lt;- tibble(\n  var1 = sample(c(\"a\", \"b\", \"c\"), 10, replace = T),\n  var2 = runif(10, 3, 7),\n  var3 = runif(10)\n)\ndata_prueba\n\n# A tibble: 10 × 3\n   var1   var2  var3\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 c      6.83 0.890\n 2 c      4.81 0.693\n 3 c      5.71 0.641\n 4 b      5.29 0.994\n 5 c      3.41 0.656\n 6 b      6.60 0.709\n 7 b      3.98 0.544\n 8 b      3.17 0.594\n 9 c      4.31 0.289\n10 a      6.82 0.147\nSupongamos que, de estas 10 filas, queremos seleccionar o filtrar la fila 1 y 4.\ndata_prueba |&gt; filter(c(T,F,F,T,F,F,F,F,F,F))\n\n# A tibble: 2 × 3\n  var1   var2  var3\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 c      6.83 0.890\n2 b      5.29 0.994\nEsta es la forma en que opera filter(), aunque el vector de valores lógicos será resultado de algunas condiciones sobre los valores de las variables. Supongamos que queremos filtrar aquellos datos o filas en los que el valor de var3 es mayor a 0.7, lo que debemos escribir es\ndata_prueba |&gt; filter(var3 &gt; 0.7)\n\n# A tibble: 3 × 3\n  var1   var2  var3\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 c      6.83 0.890\n2 b      5.29 0.994\n3 b      6.60 0.709\nPodemos usar tantas combinaciones o condiciones como se requieran sobre las variables. En el siguiente ejemplo, filtramos los datos para los que var1 tiene el valor “c” y var3 es menor que 0.65\ndata_prueba |&gt; filter(var1 == \"c\" & var3 &lt; 0.65)\n\n# A tibble: 2 × 3\n  var1   var2  var3\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 c      5.71 0.641\n2 c      4.31 0.289\nVolvamos a trabajar con diamonds, para recordar sus variables volvamos a imprimir los nombres de las columnas\nglimpse(diamonds)\n\nRows: 53,940\nColumns: 10\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.…\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver…\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,…\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, …\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64…\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58…\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34…\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.…\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.…\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.…\nQueremos filtrar los datos para los diamantes que tienen las mejores cualidades en términos de cut (“Ideal”), color (“D”) y clarity (“IF”)\ndiamonds |&gt; \n  filter(cut == \"Ideal\" & color == \"D\" & clarity == \"IF\")\n\n# A tibble: 28 × 10\n   carat cut   color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.51 Ideal D     IF       62      56  3446  5.14  5.18  3.2 \n 2  0.51 Ideal D     IF       62.1    55  3446  5.12  5.13  3.19\n 3  0.53 Ideal D     IF       61.5    54  3517  5.27  5.21  3.22\n 4  0.53 Ideal D     IF       62.2    55  3812  5.17  5.19  3.22\n 5  0.63 Ideal D     IF       61.2    53  3832  5.55  5.6   3.41\n 6  0.59 Ideal D     IF       60.7    58  4161  5.45  5.49  3.32\n 7  0.59 Ideal D     IF       60.9    57  4208  5.4   5.43  3.3 \n 8  0.56 Ideal D     IF       62.4    56  4216  5.24  5.28  3.28\n 9  0.56 Ideal D     IF       61.9    57  4293  5.28  5.31  3.28\n10  0.56 Ideal D     IF       60.8    58  4632  5.35  5.31  3.24\n# ℹ 18 more rows\nSeleccionamos los diamantes más baratos y más caros (1%), y los ordenamos de forma descendente según su precio\nx_inf &lt;- quantile(diamonds$price, probs = 0.01)\nx_sup &lt;- quantile(diamonds$price, probs = 0.99)\ndiamonds |&gt; \n  filter(price &lt; x_inf | price &gt; x_sup) |&gt; \n  arrange(desc(price))\n\n# A tibble: 1,069 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  2.29 Premium   I     VS2      60.8    60 18823  8.5   8.47  5.16\n 2  2    Very Good G     SI1      63.5    56 18818  7.9   7.97  5.04\n 3  1.51 Ideal     G     IF       61.7    55 18806  7.37  7.41  4.56\n 4  2.07 Ideal     G     SI2      62.5    55 18804  8.2   8.13  5.11\n 5  2    Very Good H     SI1      62.8    57 18803  7.95  8     5.01\n 6  2.29 Premium   I     SI1      61.8    59 18797  8.52  8.45  5.24\n 7  2.04 Premium   H     SI1      58.1    60 18795  8.37  8.28  4.84\n 8  2    Premium   I     VS1      60.8    59 18795  8.13  8.02  4.91\n 9  1.71 Premium   F     VS2      62.3    59 18791  7.57  7.53  4.7 \n10  2.15 Ideal     G     SI2      62.6    54 18791  8.29  8.35  5.21\n# ℹ 1,059 more rows\nAl pasar las condiciones con las que queremos filtrar los datos, el operador & puede ser reemplazado por una , o dicho de otra forma, las condiciones separadas por , se interpretarán como &.\nSi queremos seleccionar los diamantes que tienen el atributo clarity más pobre (los 3 peores): “I1”, “SI2”, “SI1”\ndiamonds |&gt; \n  filter(clarity == \"I1\" | clarity == \"SI2\" | clarity == \"SI1\")\n\n# A tibble: 23,000 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 4  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 5  0.3  Good      J     SI1      64      55   339  4.25  4.28  2.73\n 6  0.22 Premium   F     SI1      60.4    61   342  3.88  3.84  2.33\n 7  0.31 Ideal     J     SI2      62.2    54   344  4.35  4.37  2.71\n 8  0.2  Premium   E     SI2      60.2    62   345  3.79  3.75  2.27\n 9  0.32 Premium   E     I1       60.9    58   345  4.38  4.42  2.68\n10  0.3  Ideal     I     SI2      62      54   348  4.31  4.34  2.68\n# ℹ 22,990 more rows\nCuando estamos evaluando la condición de que una variable tome alguno entre 2 o más valores, es más sencillo usar var %in% c(valor1, ..., valorn) en lugar de var == valor1 | var == \"valor2\" | ... var == \"valorn\". El siguiente código hace la misma operación que el anterior\ndiamonds |&gt; filter(clarity %in% c(\"I1\", \"SI2\", \"SI1\", \"VS2\"))\n\n# A tibble: 35,258 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 4  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 5  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 6  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n 7  0.3  Good      J     SI1      64      55   339  4.25  4.28  2.73\n 8  0.22 Premium   F     SI1      60.4    61   342  3.88  3.84  2.33\n 9  0.31 Ideal     J     SI2      62.2    54   344  4.35  4.37  2.71\n10  0.2  Premium   E     SI2      60.2    62   345  3.79  3.75  2.27\n# ℹ 35,248 more rows",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Uso de `dplyr`</span>"
    ]
  },
  {
    "objectID": "uso-dplyr.html#operaciones-sobre-filas",
    "href": "uso-dplyr.html#operaciones-sobre-filas",
    "title": "3  Uso de dplyr",
    "section": "",
    "text": "arrange(): Ordena las filas de forma ascendente o descendente para valores de una o más variables\nfilter(): Selecciona o filtra los datos que cumplen una o más condiciones\ndistinct(): Selecciona valores únicos\n\n\n\n\n2 No tiene mucho sentido hacerlo para variables continuas\n\n\n3 Omitimos la salida para facilitar la lectura del documento",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Uso de `dplyr`</span>"
    ]
  },
  {
    "objectID": "uso-dplyr.html#operaciones-sobre-columnas",
    "href": "uso-dplyr.html#operaciones-sobre-columnas",
    "title": "3  Uso de dplyr",
    "section": "3.2 Operaciones sobre columnas",
    "text": "3.2 Operaciones sobre columnas\nPara agregar nuevas variables a la base de datos se usa mutate(), estas pueden ser variables externas o creadas en el momento o resultado de cálculo de variables dentro de la base de datos misma4\n4 esto es algo bastante bueno, normalmente no podríamos hacerlo ya que las variables en la base de datos no están guardadas directamente en el entorno de variables de la sesión de trabajo\nvar_inventada &lt;- sample(c(\"c1\", \"c2\", \"c3\"), size = nrow(diamonds), replace = T)\ndiamonds |&gt; \n  mutate(\n    var_nueva = var_inventada, # una variable externa\n    var_nueva2 = runif(nrow(diamonds)), # una creada en el momento\n    var_nueva3 = x + y + y # una con las variables existentes en la base de datos\n  ) |&gt; glimpse()\n\nRows: 53,940\nColumns: 13\n$ carat      &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23,…\n$ cut        &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, …\n$ color      &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J,…\n$ clarity    &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS…\n$ depth      &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4,…\n$ table      &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62,…\n$ price      &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340,…\n$ x          &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00,…\n$ y          &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05,…\n$ z          &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39,…\n$ var_nueva  &lt;chr&gt; \"c1\", \"c2\", \"c3\", \"c2\", \"c1\", \"c3\", \"c3\", \"c1\", \"c3\", \"c2\",…\n$ var_nueva2 &lt;dbl&gt; 0.50870149, 0.09304586, 0.52525136, 0.24570402, 0.92814596,…\n$ var_nueva3 &lt;dbl&gt; 11.91, 11.57, 12.19, 12.66, 13.04, 11.86, 11.91, 12.29, 11.…\n\n\nPor defecto, las variables creadas serán agregadas “a la derecha” de la base de datos. Esto se puede controlar con:\n\n.before: El valor debe ser un número entero o nombre de variable, las nuevas variables se colocarán antes de ese número de columna o variable\n.after: 🧠\n\nTambién es posible especificar las variables que se quieren mantener con el argumento .keep, las opciones son: all (por defecto), used (solo las usadas), unused, none\n\ndiamonds |&gt; \n  mutate(\n    var_nueva = var_inventada,\n    var_nueva2 = runif(nrow(diamonds)),\n    var_nueva3 = x + y + z,\n    .keep = \"used\",\n    .after = x\n  )\n\n# A tibble: 53,940 × 6\n       x var_nueva var_nueva2 var_nueva3     y     z\n   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  3.95 c1            0.0301       10.4  3.98  2.43\n 2  3.89 c2            0.343        10.0  3.84  2.31\n 3  4.05 c3            0.561        10.4  4.07  2.31\n 4  4.2  c2            0.620        11.1  4.23  2.63\n 5  4.34 c1            0.0352       11.4  4.35  2.75\n 6  3.94 c3            0.170        10.4  3.96  2.48\n 7  3.95 c3            0.864        10.4  3.98  2.47\n 8  4.07 c1            0.469        10.7  4.11  2.53\n 9  3.87 c3            0.738        10.1  3.78  2.49\n10  4    c2            0.217        10.4  4.05  2.39\n# ℹ 53,930 more rows\n\n\nEs posible renombrar las variables con rename()\n\ndiamonds |&gt; \n  rename(\n    precio = price,\n    peso = carat,\n    corte = cut, \n    claridad = clarity\n  )\n\n# A tibble: 53,940 × 10\n    peso corte     color claridad depth table precio     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2       61.5    55    326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1       59.8    61    326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1       56.9    65    327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2       62.4    58    334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2       63.3    58    335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2      62.8    57    336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1      62.3    57    336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1       61.9    55    337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2       65.1    61    337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1       59.4    61    338  4     4.05  2.39\n# ℹ 53,930 more rows\n\n\nPodemos seleccionar un subconjunto de variables con select()",
    "crumbs": [
      "Análisis Exploratorio",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Uso de `dplyr`</span>"
    ]
  },
  {
    "objectID": "r-prob.html",
    "href": "r-prob.html",
    "title": "4  Probabilidad",
    "section": "",
    "text": "4.1 Definiciones básicas",
    "crumbs": [
      "Inferencia clásica",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "r-prob.html#probabilidad-de-eventos",
    "href": "r-prob.html#probabilidad-de-eventos",
    "title": "4  Probabilidad",
    "section": "4.2 Probabilidad de eventos",
    "text": "4.2 Probabilidad de eventos",
    "crumbs": [
      "Inferencia clásica",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "r-prob.html#variables-aleatorias",
    "href": "r-prob.html#variables-aleatorias",
    "title": "4  Probabilidad",
    "section": "4.3 Variables aleatorias",
    "text": "4.3 Variables aleatorias",
    "crumbs": [
      "Inferencia clásica",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "r-prob.html#distribuciones-de-la-muestra",
    "href": "r-prob.html#distribuciones-de-la-muestra",
    "title": "4  Probabilidad",
    "section": "4.4 Distribuciones de la muestra",
    "text": "4.4 Distribuciones de la muestra",
    "crumbs": [
      "Inferencia clásica",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "r-estimacion.html",
    "href": "r-estimacion.html",
    "title": "5  Estimación",
    "section": "",
    "text": "5.1 Estimacicón puntual",
    "crumbs": [
      "Inferencia clásica",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimación</span>"
    ]
  },
  {
    "objectID": "r-estimacion.html#estimacicón-puntual",
    "href": "r-estimacion.html#estimacicón-puntual",
    "title": "5  Estimación",
    "section": "",
    "text": "5.1.1 Estimación para la media\n\n\n5.1.2 Estimación para la varianza",
    "crumbs": [
      "Inferencia clásica",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimación</span>"
    ]
  },
  {
    "objectID": "r-estimacion.html#construcción-de-intervalos-de-confianza",
    "href": "r-estimacion.html#construcción-de-intervalos-de-confianza",
    "title": "5  Estimación",
    "section": "5.2 Construcción de intervalos de confianza",
    "text": "5.2 Construcción de intervalos de confianza",
    "crumbs": [
      "Inferencia clásica",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimación</span>"
    ]
  },
  {
    "objectID": "r-hipotesis.html",
    "href": "r-hipotesis.html",
    "title": "6  Pruebas de hipótesis",
    "section": "",
    "text": "6.1 Aspectos básicos, tipos de error",
    "crumbs": [
      "Inferencia clásica",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Pruebas de hipótesis</span>"
    ]
  },
  {
    "objectID": "r-hipotesis.html#prueba-de-hipótesis-sobre-la-media",
    "href": "r-hipotesis.html#prueba-de-hipótesis-sobre-la-media",
    "title": "6  Pruebas de hipótesis",
    "section": "6.2 Prueba de hipótesis sobre la media",
    "text": "6.2 Prueba de hipótesis sobre la media",
    "crumbs": [
      "Inferencia clásica",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Pruebas de hipótesis</span>"
    ]
  },
  {
    "objectID": "r-hipotesis.html#pruebas-sobre-la-varianza",
    "href": "r-hipotesis.html#pruebas-sobre-la-varianza",
    "title": "6  Pruebas de hipótesis",
    "section": "6.3 Pruebas sobre la varianza",
    "text": "6.3 Pruebas sobre la varianza",
    "crumbs": [
      "Inferencia clásica",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Pruebas de hipótesis</span>"
    ]
  },
  {
    "objectID": "ds-intro.html",
    "href": "ds-intro.html",
    "title": "7  Definiciones",
    "section": "",
    "text": "7.1 Ejemplo para discusión\n¿Cómo funcionaría un grupo de ciencia de datos en una empresa? Suponga un caso en el que una empresa pública quiere aplicar ciencia de datos a las PQR de los usuarios",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Definiciones</span>"
    ]
  },
  {
    "objectID": "ds-intro.html#modelo-predictivo",
    "href": "ds-intro.html#modelo-predictivo",
    "title": "7  Definiciones",
    "section": "7.2 Modelo predictivo",
    "text": "7.2 Modelo predictivo\nDadas unas condiciones particulares (valores de las variables predictoras \\(\\textbf{X}\\)), un modelo predictivo intenta determinar (pronosticar) de [la mejor manera posible] el valor de la variable de interés \\(Y\\) (aprendizaje supervisado). No todas las técnicas de ciencia de datos tienen como objetivo la predicción.",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Definiciones</span>"
    ]
  },
  {
    "objectID": "ds-intro.html#metodología",
    "href": "ds-intro.html#metodología",
    "title": "7  Definiciones",
    "section": "7.3 Metodología",
    "text": "7.3 Metodología\nSiempre que sea posible, los temas del curso se desarrollarán bajo el siguiente orden lógico\n\nDesarrollo teórico y conceptual (a mano)\nMétodo Montecarlo (bases de datos simuladas en R)\nAplicación de los métodos en bases de datos reales (en R)",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Definiciones</span>"
    ]
  },
  {
    "objectID": "ds-intro.html#aplicaciones",
    "href": "ds-intro.html#aplicaciones",
    "title": "7  Definiciones",
    "section": "7.4 Aplicaciones",
    "text": "7.4 Aplicaciones\n\nDadas unas condiciones particulares, ¿cuál es la mejor aproximación para la variable de respuesta?\n\n\nDado el historial de compras y de navegación de un cliente en el portal de un comercio electrónico, ¿cuál es la probabilidad de que compre un producto si se ofrece en promoción?\nDado el historial familiar, ¿cuál es la probabilidad que tiene un individuo de sufrir de diabetes en su edad adulta?\nDado el valor histórico de las acciones de una compañía en la bolsa de valores en los últimos años, ¿El valor de la acción subirá o bajará?\nDado un mapa de bits de un caracter numérico escrito a mano, ¿qué número es?",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Definiciones</span>"
    ]
  },
  {
    "objectID": "ds-intro.html#flexibilidad-del-modelo",
    "href": "ds-intro.html#flexibilidad-del-modelo",
    "title": "7  Definiciones",
    "section": "7.5 Flexibilidad del modelo",
    "text": "7.5 Flexibilidad del modelo\nEl concepto de flexibilidad será de vital importancia para nosotros, miremos qué podemos decir de forma sencilla. Supongamos que la demanda de un producto varía de forma inversa a su precio con la siguiente ecuación:\n\\[f(x)=20-x+100e^{-x}\\] Al considerar la parte aleatoria del problema (¿por qué es esto más realista?) podemos escribir\n\\[\ny_i=f(x_i)+\\epsilon_i\n\\\\ \\epsilon_i \\sim N (\\mu=0, \\ \\sigma=8)\n\\]\nGeneramos una muestra aleatoria de \\(n=200\\) datos (método Montecarlo). A partir de los datos, ajustamos un modelo de regresión local de segundo orden, cuyo parámetro de calibración se encuentra con el método de validación cruzada.\n\n7.5.1 Casos a considerar\n\nModelo real\nModelo muy flexible\nModelo poco flexible\nModelo Calibrado\n\n\n\n7.5.2 Modelo predictivo para la demanda usando regresión local\nPuede hacer zoom o seleccionar los modelos a mostrar en la gráfica\n\nlibrary(plotly)\nlibrary(stats)  # Función loess\n\nf &lt;- function(x){100*exp(-x)+20-x}\nn &lt;- 200 # tamaño de la muestra\nx &lt;- sort(runif(n, 0.1, 8))\ny &lt;- f(x)\ne &lt;- rnorm(n, 0, 8)\ny0 &lt;- y + e\nxy &lt;- data.frame(Precio=x,Demanda=y,y0)\n\n# Haciendo LOOCV (Leave One Out CV)\nflex &lt;- seq(from=0.1, to=1.3, length.out=20)\nv_cv &lt;- rep(0, length(flex))\n\nfor(j in 1:length(flex)){\n  cvs &lt;- rep(0, n)\n  for(i in 1:n){\n    xi &lt;- x[-i]\n    yi &lt;- y0[-i]\n    xiyi &lt;- data.frame(xi=xi, yi=yi)\n    d &lt;- loess(formula = yi~xi, data = xiyi,\n                           span=flex[j], degree = 2)\n    cvs[i] &lt;- (y0[i] - stats::predict(d, newdata=data.frame(xi=x[i])))^2\n  }\n  v_cv[j] &lt;- mean(cvs,na.rm=T)\n}\n\nbd_opt &lt;- flex[which.min(v_cv)]\n\nfit_1 &lt;- loess(formula = y0~x, span=0.09, degree = 2)  # Modelo muy flexible\nfit_2 &lt;- loess(formula = y0~x, span=9, degree = 2)  # Modelo poco flexible\nfit_opt &lt;- loess(formula = y0~x, span=bd_opt, degree = 2)  # Modelo calibrado con LOOCV\n\nypred_1 &lt;- predict(fit_1)\nypred_2 &lt;- predict(fit_2)\nypred_opt &lt;- predict(fit_opt)\n\n\ninf=-10; sup=120\nfig &lt;- plot_ly(xy, x=~Precio)\nfig &lt;- fig %&gt;% layout(yaxis=list(range=c(inf,sup)), xaxis=list(range=c(0,9)))\nfig &lt;- fig %&gt;% add_trace(y=~Demanda, type = \"scatter\", mode=\"lines\", name = \"Real\")\nfig &lt;- fig %&gt;% add_trace(y=~y0, type=\"scatter\", mode=\"markers\", \n                         marker=list(color=\"#ff7f0e\"), name=\"Datos\")\nfig &lt;- fig %&gt;% add_trace(y=~ypred_1, type = \"scatter\", mode=\"lines\", \n                         name = \"Modelo muy flexible\")\nfig &lt;- fig %&gt;% add_trace(y=~ypred_2, type = \"scatter\", mode=\"lines\", \n                         name = \"Modelo poco flexible\")\nfig &lt;- fig %&gt;% add_trace(y=~ypred_opt, type = \"scatter\", mode=\"lines\", \n                         name = \"Modelo calibrado\")\n\n\nfig",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Definiciones</span>"
    ]
  },
  {
    "objectID": "reg-simple.html",
    "href": "reg-simple.html",
    "title": "8  Regresión lineal simple",
    "section": "",
    "text": "8.1 Estimación de los parámetros",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "reg-simple.html#propiedades-de-los-estimadores",
    "href": "reg-simple.html#propiedades-de-los-estimadores",
    "title": "8  Regresión lineal simple",
    "section": "8.2 Propiedades de los estimadores",
    "text": "8.2 Propiedades de los estimadores",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "reg-simple.html#prueba-de-significancia",
    "href": "reg-simple.html#prueba-de-significancia",
    "title": "8  Regresión lineal simple",
    "section": "8.3 Prueba de significancia",
    "text": "8.3 Prueba de significancia",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regresión lineal simple</span>"
    ]
  },
  {
    "objectID": "reg-multiple.html",
    "href": "reg-multiple.html",
    "title": "9  Regresión lineal múltiple",
    "section": "",
    "text": "9.1 Estimación de los parámetros\nRecuerde que el modelo general es \\[y_i = f(x_i) + \\varepsilon_i\\] Y lo hemos reducido al caso muy especial de regresión lineal simple \\[y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\] La generalización natural de este modelo es \\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} +...+ \\beta_p x_{ip} + \\varepsilon_i\\] que se llama modelo de regresión lineal múltiple. En este caso, tenemos \\(p\\) variables y \\(p+1\\) parámetros1\nEl modelo con múltiples variables se puede escribir de forma concisa usando notación matricial\n\\[\\bf{y} = \\bf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\] Donde \\[\\bf{y} = \\begin{bmatrix}\ny_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n\\end{bmatrix}, \\hspace{10pt}\n\\bf{X} = \\begin{bmatrix}\n1 & x_{11} & \\dots & x_{1p} \\\\\n1 & x_{21} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & \\dots & x_{np} \\\\\n\\end{bmatrix}, \\hspace{10pt}\n\\boldsymbol{\\beta} = \\begin{bmatrix}\n\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p\n\\end{bmatrix}, \\hspace{10pt}\n\\boldsymbol{\\varepsilon} = \\begin{bmatrix}\n\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n\n\\end{bmatrix}\n\\] Desde este punto de vista, podemos plantear la suma de residuales al cuadrado como \\[\\mathrm{RSS} = \\boldsymbol{\\varepsilon}^T\\boldsymbol{\\varepsilon}\\] Y la estimación se hace a través del método de mínimos cuadrados \\[\n\\begin{equation}\n\\begin{split}\n\\boldsymbol{\\hat{\\beta}} & = \\arg\\min \\boldsymbol{\\varepsilon}^T\\boldsymbol{\\varepsilon} \\\\\n& = \\arg\\min \\left( \\bf{Y} - \\bf{X} \\boldsymbol{\\beta} \\right)^T \\left( \\bf{Y} - \\bf{X} \\boldsymbol{\\beta} \\right) \\\\\n\\end{split}\n\\end{equation}\n\\] Al tomar la derivada de \\(\\mathrm{RSS}\\) con respecto a \\(\\boldsymbol{\\hat{\\beta}}\\) e igualar a cero, obtenemos la solución \\[\n\\boldsymbol{\\hat{\\beta}} = \\left( \\textbf{X}^T\\bf{X}  \\right)^{-1}\\textbf{X}^T\\bf{y}\n\\]",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regresión lineal múltiple</span>"
    ]
  },
  {
    "objectID": "reg-multiple.html#estimación-de-los-parámetros",
    "href": "reg-multiple.html#estimación-de-los-parámetros",
    "title": "9  Regresión lineal múltiple",
    "section": "",
    "text": "1 algunos textos consideran \\(p\\) como el número de parámetros",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regresión lineal múltiple</span>"
    ]
  },
  {
    "objectID": "reg-multiple.html#valor-esperado-y-varianza",
    "href": "reg-multiple.html#valor-esperado-y-varianza",
    "title": "9  Regresión lineal múltiple",
    "section": "9.2 Valor esperado y Varianza",
    "text": "9.2 Valor esperado y Varianza\nLos supuestos de homocedasticidad y no autocorrelación para los términos del error en el modelo, se pueden resumir estableciendo la matriz de varianzas y covarianzas del error como \\[\\mathrm{Cov} \\left( \\boldsymbol{\\varepsilon}\\right) = \\sigma^2 \\mathbb{I}\\] Además, \\(\\mathrm{E}\\left( \\boldsymbol{\\varepsilon}\\right) = \\bf{0}\\) y se asume que la matriz \\(\\bf{X}\\) no es estocástica.\nComo consecuencia tenemos que\n\\[\\mathrm{E} \\left( \\bf{{y}} \\right) = \\mathrm{E}\\left( \\boldsymbol{{\\bf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}}}\\right) = \\mathrm{E} \\left( \\bf{X} \\boldsymbol{\\beta} \\right) + \\mathrm{E} \\left( \\boldsymbol{\\varepsilon} \\right)= \\bf{X}\\boldsymbol{\\beta} + \\bf{0} = \\bf{X}\\boldsymbol{\\beta}\\\\\n\\] \\[\n\\mathrm{Cov}\\left( \\bf{{y}}\\right) = \\mathrm{Cov}\\left( \\boldsymbol{{\\bf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}}}\\right) = \\bf{0} + \\mathrm{Cov} \\left( \\boldsymbol{\\varepsilon} \\right)= \\sigma^2\\mathbb{I}\\\\\\] Al aplicar propiedades del valor esperado y la varianza al vector de parámetros estimados (vector aleatorio) obtenemos\n\\[\n\\begin{equation}\n\\begin{split}\n\\mathrm{E} \\left( \\boldsymbol{\\hat\\beta}\\right)\n& = \\mathrm{E} \\left( \\left( \\textbf{X}^T\\bf{X}\\right)^{-1} \\textbf{X}^T\\bf{y} \\right) \\\\\n& = \\left( \\textbf{X}^T\\bf{X}\\right)^{-1} \\textbf{X}^T \\mathrm{E} \\left( \\bf{y} \\right) \\\\\n& = \\left( \\textbf{X}^T \\textbf{X}\\right)^{-1} \\textbf{X}^T \\textbf{X} \\boldsymbol{\\beta} \\\\\n& = \\mathbb{I}\\boldsymbol{\\beta} = \\boldsymbol{\\beta}\n\\end{split}\n\\end{equation}\n\\] \\[\n\\begin{equation}\n\\begin{split}\n\\mathrm{Cov} \\left( \\boldsymbol{\\hat\\beta}\\right)\n& = \\mathrm{Cov} \\left( \\left( \\textbf{X}^T\\bf{X}\\right)^{-1} \\textbf{X}^T\\bf{y} \\right) \\\\\n& = \\left( \\textbf{X}^T\\bf{X}\\right)^{-1} \\textbf{X}^T \\mathrm{Cov} \\left( \\bf{y} \\right)\n\\left( \\left( \\textbf{X}^T\\bf{X}\\right)^{-1} \\textbf{X}^T \\right)^T\\\\\n& = \\left( \\textbf{X}^T \\textbf{X}\\right)^{-1} \\textbf{X}^T \\left(\\sigma^2 \\mathbb{I}\\right)\n\\textbf{X} \\left( \\textbf{X}^T\\bf{X}\\right)^{-1} \\\\\n& = \\left( \\textbf{X}^T \\textbf{X}\\right)^{-1} \\textbf{X}^T\n\\textbf{X} \\left( \\textbf{X}^T\\bf{X}\\right)^{-1} \\sigma^2 \\\\\n& = \\left( \\textbf{X}^T \\textbf{X}\\right)^{-1} \\mathbb{I} \\sigma^2 \\\\\n& = \\left( \\textbf{X}^T \\textbf{X}\\right)^{-1} \\sigma^2 \\\\\n\\end{split}\n\\end{equation}\n\\] Nota: Bajo los supuestos indicados (valor esperado cero, varianza constante y no autocorrelación), los estimadores de mínmios cuadrados son los de menor varianza entre todos los estimadores lineales insesgados (Teorema de Gauss-Markov)\nSi queremos estimar la matriz de varianzas y covarianzas, debemos estimar la varianza del modelo, lo hacemos usando el estimador de mínimos cuadrados\n\\[\\hat\\sigma^2 = \\frac{\\boldsymbol{\\varepsilon}^T \\boldsymbol{\\varepsilon}}{n-p-1}\\]",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regresión lineal múltiple</span>"
    ]
  },
  {
    "objectID": "reg-multiple.html#predicción",
    "href": "reg-multiple.html#predicción",
    "title": "9  Regresión lineal múltiple",
    "section": "9.3 Predicción",
    "text": "9.3 Predicción\nAl igual que en modelo de regresión lineal simple, una predicción para un conjunto de datos nuevos de las variables independientes, se obtiene al reemplezar dichos valores en la ecuación lineal.\nSea \\(\\textbf{x}_0\\) el vector que contiene los valores para los cuáles se quiere predecir la variable \\(y\\). en notación matricial tenemos\n\\[\\hat{y} = \\textbf{x}_0^T \\boldsymbol{\\hat \\beta}\\] siendo, \\(\\textbf{x}_0^T = \\begin{bmatrix} 1 & x_{01} & x_{02} & \\dots & x_{0p} \\end{bmatrix}\\)\nSi calculamos el valor de \\(\\hat{y}\\) para cada observación en el conjunto de datos \\(\\bf{X}\\)\n\\[\\hat{\\bf{y}} = \\textbf{X}\\boldsymbol{\\hat \\beta} = \\textbf{X}\\left( \\textbf{X}^T \\textbf{X}\\right)^{-1} \\textbf{X}^T \\bf{y} = \\textbf{H}\\bf{y}\\] La matriz \\(\\textbf{H} = \\textbf{X}\\left( \\textbf{X}^T \\textbf{X}\\right)^{-1} \\textbf{X}^T\\) se suele llamar matriz sombrero, ya que le “pone” un sombrero al vector \\(\\bf{y}\\)\nCon el último resultado, salta a la vista un hecho importante, las predicciones son combinaciones lineales de los datos observados. El modelo de regresión lineal es un caso particular de una clase de modelos más generales que son suavizadores lineales (hay otros modelos diferentes a los de regresión lineal que son suavizadores lineales)",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regresión lineal múltiple</span>"
    ]
  },
  {
    "objectID": "reg-multiple.html#inferencia",
    "href": "reg-multiple.html#inferencia",
    "title": "9  Regresión lineal múltiple",
    "section": "9.4 Inferencia",
    "text": "9.4 Inferencia\nSi queremos ir más allá de la sola estimación de los parámetros y predicción puntual, debemos asumir una distribución para el error del modelo y obtener la distribución de los estimadores. Bajo el supuesto de normalidad, que es el más común\n\\[\\boldsymbol{\\varepsilon} \\sim N_n \\left( \\bf{0}, \\sigma^2 \\mathbb{I} \\right) \\Rightarrow\n\\textbf{y} \\sim N_n \\left( \\bf{X}\\boldsymbol{\\beta}, \\sigma^2 \\mathbb{I} \\right)\n\\] Teniendo en cuenta que los estimadores son combinaciones lineales de \\(\\bf{y}\\), y toda combinación lineal de distribuciones normales es también una distribución normal\n\\[\\boldsymbol{\\hat \\beta}\\sim N_{p+1} \\left( \\boldsymbol{\\beta}, \\left( \\textbf{X}^T \\textbf{X}\\right)^{-1} \\sigma^2 \\right)\\] Con esto ya podemos calcular intervalos de confianza y hacer algunas pruebas de hipótesis",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regresión lineal múltiple</span>"
    ]
  },
  {
    "objectID": "reg-multiple.html#pruebas-de-significancia",
    "href": "reg-multiple.html#pruebas-de-significancia",
    "title": "9  Regresión lineal múltiple",
    "section": "9.5 Pruebas de significancia",
    "text": "9.5 Pruebas de significancia\nCuando tenemos un modelo con múltiples variables, podemos hablar de la significancia global del modelo y de significancia individual de las variables.\nLa significancia global del modelo se usa para responder a la pregunta: ¿Al menos una de las variables es importante o explica en términos estadísticos a la variable \\(y\\)? Para esto, usamos una prueba de hipótesis y un estadístico F (recordar por qué una distribución F en lugar de una distribución T, es valioso pero no fundamental para este curso)\n\\[\n\\begin{equation}\n\\begin{split}\nH_0 & : \\beta_1 = \\beta_2 =  \\dots = \\beta_p = 0 \\\\\nH_a & : \\text{Al menos un beta es diferente de 0} \\\\ \\\\\nF_{cal} & = \\frac{\\left( \\mathrm{TSS - RSS}\\right)/p}{ \\mathrm{RSS}/(n-p-1)}\n\\end{split}\n\\end{equation}\n\\] Al hablar de significancia nos referimos a variables, pero las pruebas se hacen sobre los parámetros que acompañan dichas variables. Si un parámetro no es estadísticamente diferente de cero, entonces se puede concluir que los cambios de esa variable se ven anulados o no afectan a la variable \\(y\\), por esa razón, el \\(\\beta_0\\) no se incluye en la prueba de significancia.\nCon un poco de manipulación algebraica se puede poner el \\(F_{cal}\\) en términos del \\(\\mathrm{R^2}\\) del modelo (compruebe) \\(F_{cal} = \\frac{(n-p-1)\\mathrm{R}^2}{p(1-\\mathrm{R}^2)}\\)\nLa prueba de significancia individual para las variables se plantea como\n\\[\n\\begin{equation}\n\\begin{split}\n  H_0 & : \\beta_j = 0 \\\\\n  H_a & : \\beta_j \\neq 0 \\\\\n  T_{cal} & = \\frac{\\hat \\beta_j}{\\sqrt{\\hat{\\mathrm{Var}} (\\hat \\beta_j)}}\n\\end{split}\n\\end{equation}\n\\] para algún \\(j=1,2,3,...,p\\)",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regresión lineal múltiple</span>"
    ]
  },
  {
    "objectID": "reg-multiple.html#uso-de-r-para-los-cálculos",
    "href": "reg-multiple.html#uso-de-r-para-los-cálculos",
    "title": "9  Regresión lineal múltiple",
    "section": "9.6 Uso de R para los cálculos",
    "text": "9.6 Uso de R para los cálculos\nTrabajemos primero con un conjunto de datos simulados para observar algunos detalles de cálculo en el modelo de regresión lineal múltiple\nConsideremos el modelo lineal “real”\n\\[y = 1 + 2x_1 + 3x_2 + 4x_3 + \\varepsilon\\] Simulemos 100 observaciones de cada variable independiente (generadas uniformemente entre 0 y 1) y un error normalmente distribuido con media de 0 y desviación estándar de 1.5\n\nlibrary(tidyverse)\nset.seed(789)\nn &lt;- 100\nx1 &lt;- runif(n)\nx2 &lt;- runif(n)\nx3 &lt;- runif(n)\nerror &lt;- rnorm(n, 0, 1.5)\ny &lt;- 1 + 2*x1 + 3*x2 + 4*x3 + error\ndatos &lt;- tibble(var_dep = y, var1 = x1, var2 = x2, var3 = x3)\ndatos\n\n# A tibble: 100 × 4\n   var_dep   var1   var2   var3\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1    4.19 0.700  0.481  0.293 \n 2    1.79 0.0935 0.314  0.0935\n 3    2.88 0.0119 0.0643 0.345 \n 4    7.80 0.592  0.890  0.215 \n 5    6.08 0.492  0.787  0.721 \n 6    5.46 0.0202 0.164  0.927 \n 7    5.13 0.573  0.443  0.565 \n 8    4.94 0.166  0.429  0.831 \n 9    4.54 0.359  0.240  0.556 \n10    4.66 0.346  0.481  0.115 \n# ℹ 90 more rows\n\n\nAjustamos un modelo de regresión lineal usando las funciones de R2\n2 como haríamos en la práctica, no es necesario usar las ecuaciones matriciales que hemos visto. Ni siquiera R ajusta el modelo con esas ecuaciones matriciales internamente (¿Por qué?)\nmodelo1 &lt;- lm(formula = var_dep ~ var1 + var2 + var3, data = datos)\nsummary(modelo1)\n\n\nCall:\nlm(formula = var_dep ~ var1 + var2 + var3, data = datos)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5503 -0.9438  0.0341  1.0381  3.2040 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.02071    0.50151   0.041    0.967    \nvar1         2.77634    0.56757   4.892 4.02e-06 ***\nvar2         3.16818    0.54805   5.781 9.23e-08 ***\nvar3         4.67855    0.53282   8.781 6.17e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.543 on 96 degrees of freedom\nMultiple R-squared:  0.5838,    Adjusted R-squared:  0.5708 \nF-statistic: 44.89 on 3 and 96 DF,  p-value: &lt; 2.2e-16\n\n\nAl aplicar la función lm se guardan varios resultados, podemos acceder por el nombre de cada uno en el objeto modelo1 o en el objeto summary(modelo1)\n\nnames(modelo1)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\nnames(summary(modelo1))\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\n\nPodemos imprimir los coeficientes estimados o el \\(\\mathrm{R}^2\\)\n\nmodelo1$coefficients\n\n(Intercept)        var1        var2        var3 \n 0.02071457  2.77634472  3.16818077  4.67855453 \n\nsummary(modelo1)$r.squared\n\n[1] 0.5838244\n\n\nLa matriz de varianzas y covarianzas de los estimadores se obtiene con\n\nvcov(modelo1)\n\n            (Intercept)        var1        var2        var3\n(Intercept)   0.2515144 -0.14854787 -0.15860673 -0.14759946\nvar1         -0.1485479  0.32213475  0.01027060 -0.01353928\nvar2         -0.1586067  0.01027060  0.30036354  0.00552749\nvar3         -0.1475995 -0.01353928  0.00552749  0.28389997\n\n\nEs buen ejercicio intentar obtener estos resultados a partir de las ecuaciones matriciales del modelo\n\n# matriz X\nmx &lt;- cbind(rep(1,n), x1, x2, x3)\nbeta_est &lt;- solve(t(mx)%*%mx)%*%t(mx)%*%y\nH &lt;- mx%*%solve(t(mx)%*%mx)%*%t(mx)\nbeta_est\n\n         [,1]\n   0.02071457\nx1 2.77634472\nx2 3.16818077\nx3 4.67855453\n\n\nPredicciones, residuales, matriz de varianzas y covarianzas\n\ny_est &lt;- H%*%y\nres &lt;- y - y_est\nsigma2_est &lt;- sum(res*res)/(n-3-1)\ncov_beta &lt;- sigma2_est*(solve(t(mx)%*%mx))\nsummary(res)\n\n       V1          \n Min.   :-4.55030  \n 1st Qu.:-0.94376  \n Median : 0.03413  \n Mean   : 0.00000  \n 3rd Qu.: 1.03810  \n Max.   : 3.20398  \n\nsqrt(sigma2_est) # Residual standard error\n\n[1] 1.543416\n\ncov_beta\n\n                       x1          x2          x3\n    0.2515144 -0.14854787 -0.15860673 -0.14759946\nx1 -0.1485479  0.32213475  0.01027060 -0.01353928\nx2 -0.1586067  0.01027060  0.30036354  0.00552749\nx3 -0.1475995 -0.01353928  0.00552749  0.28389997\n\n\nEl \\(\\mathrm{R^2}\\) del modelo es por definición, \\(\\mathrm{R^2} = 1- \\frac{\\mathrm{SSE}}{\\mathrm{SST}}\\). El cálculo directo en R sería\n\nr2 &lt;- 1 - sum(res*res)/sum((y-mean(y))^2)\nr2\n\n[1] 0.5838244\n\n\nCalculemos el error estándar, valor de la distribución \\(t\\) y valor \\(p\\) para alguno de los \\(\\hat\\beta_i\\)\n\n# para el beta_1 que acompaña a la variable 1\ns.eb1 &lt;- sqrt(cov_beta[2,2]) # ¿por qué se elige el segundo valor en la diagonal?\ntb1 &lt;- beta_est[2]/s.eb1\np.b1 &lt;- 2*pt(q = tb1, df = n-3-1, lower.tail = F) # ¿por qué se multiplica por 2?\ns.eb1; tb1; p.b1 \n\n[1] 0.5675692\n\n\n[1] 4.891641\n\n\n[1] 4.019829e-06\n\n\nEl valor de la distribución F para la significancia global es\n\nf_cal &lt;- (n-3-1)*r2/(3*(1-r2))\nf_cal\n\n[1] 44.89062\n\n\nEl valor p es\n\n# ¿Por qué en este caso no se multiplica por 2?\np.sig.global &lt;- pf(q = f_cal, df1 = 3, df2 = n-3-1, lower.tail = FALSE)\np.sig.global\n\n[1] 3.220991e-18\n\n\nCompare todos los resultados con los entregados de forma directa por la función lm\nAlgo que se debe señalar, es el valor relativamente bajo del \\(\\mathrm{R}^2\\). Un \\(\\mathrm{R}^2\\) bajo no implica necesariamente que el modelo sea errado. En este caso, estamos ajustando sobre la verdadera forma del modelo “real” por lo que el modelo planteado no podría estar equivocado. Simulemos otro conjunto de datos en el que el error tenga una menor varianza o desviación estándar.\n\nx1 &lt;- runif(n)\nx2 &lt;- runif(n)\nx3 &lt;- runif(n)\nerror &lt;- rnorm(n, 0, 0.5)\ny &lt;- 1 + 2*x1 + 3*x2 + 4*x3 + error\ndatos &lt;- tibble(var_dep = y, var1 = x1, var2 = x2, var3 = x3)\n\nmodelo2 &lt;- lm(formula = var_dep ~ var1 + var2 + var3, data = datos)\n\nCompare el valor del \\(\\mathrm R^2\\) para cada caso. ¿Qué puede concluir?\n\nsummary(modelo2)$r.squared\n\n[1] 0.9135621\n\nsummary(modelo1)$r.squared\n\n[1] 0.5838244",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regresión lineal múltiple</span>"
    ]
  },
  {
    "objectID": "flex-lineal.html",
    "href": "flex-lineal.html",
    "title": "10  Controlando la flexibilidad del modelo de regresión lineal",
    "section": "",
    "text": "10.1 Selección de variables\nSuponga que se tiene un modelo lineal con \\(p\\) variables \\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} +...+ \\beta_p x_{ip} + \\epsilon_i\\]\nEl objetivo es determinar cuáles de estas variables deben ir en el modelo final. Para comparar modelos tenemos diversos criterios. El \\(R^2\\) es útil para comparar modelos con igual número de variables. En caso de que tengan diferente número de variables, tenemos alternativas como: \\(R_{adj}^2\\), \\(AIC\\), \\(C_p\\) y \\(BIC\\).",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Calibración del modelo lineal</span>"
    ]
  },
  {
    "objectID": "flex-lineal.html#selección-de-variables",
    "href": "flex-lineal.html#selección-de-variables",
    "title": "10  Controlando la flexibilidad del modelo de regresión lineal",
    "section": "",
    "text": "10.1.1 selección exhaustiva:\nSe ajustan todos los modelos posibles, desde el modelo con 0 variables (solo el intercepto) hasta el modelo con todas las \\(p\\) variables. Se comparan de forma adecuada para obtener el “mejor” modelo. Este método implica ajustar \\(2^p\\)1 submodelos, lo cuál puede fácilmente convertirse en algo imposible de llevar a cabo en la práctica, dado su costo computacional.\n1 corresponde a la suma de todos los modelos de 0, 1, 2, …, k, …, p variables \\[\\binom{p}{0} + \\binom{p}{1} + \\cdots + \\binom{p}{p} = \\sum_{k=0}^{p}\\binom{p}{k} = 2^p\\]\n\n10.1.2 Métodos heurísticos: Selección hacia adelante o hacia atrás.\nPara evitar el crecimiento exponencial en la cantidad de modelos a ajustar, se puede optar por un procedimiento heurístico de selección hacia adelante o hacia atrás. En el caso de selección hacia adelante, se inicia con el modelo que no contiene variables (\\(\\mathcal{M}_0\\)), y se forman \\(p\\) modelos univariados agregando cada una de las \\(p\\) variables, se escoge el mejor de estos modelos (que puede ser elegido con el \\(R^2\\) ya que tienen el mismo número de variables), llamemos a este modelo \\(\\mathcal{M}_1\\). A \\(\\mathcal{M}_1\\) se le agrega cada una de las \\(p-1\\) variables restantes y se elige el mejor modelo de 2 variables: \\(\\mathcal{M}_2\\). Al final se tendrán \\(1 + \\frac{p(p+1)}{2}\\)2 modelos: \\[\\mathcal{M}_0, \\mathcal{M}_1, \\mathcal{M}_2, \\cdots, \\mathcal{M}_p\\] Para elegir entre estos modelos, no es adecuado el \\(R^2\\) (¿por qué?) y debe recurrirse a medidas tales como: \\(R_{adj}^2\\), \\(AIC\\), \\(C_p\\) y \\(BIC\\), sin olvidar que para el \\(R_{adj}^2\\) se busca el modelo con mayor valor, y para el resto de métricas se busca el menor valor (¿por qué?). También es posible hacer Cross Validation (CV) y elegir el modelo con menor \\(MSE_{cv}\\), pero para el caso de regresión lineal, no es necesario hacerlo (¿por qué?).\n2 Es fácil comprobar que \\[1 + p + (p-1) + (p-2) + \\cdots + 1 = 1 + \\sum_{k=1}^{p}k = 1 + \\frac{p(p+1)}{2}\\]En la tabla, se muestra cómo crece la cantidad de modelos a ajustar según la metodología: exhaustivo o heurístico.\n\n\n   Variables       Exhaustivo Heurístico\n1          5               32         16\n2         10             1024         56\n3         15            32768        121\n4         20          1048576        211\n5         25         33554432        326\n6         30       1073741824        466\n7         35      34359738368        631\n8         40    1099511627776        821\n9         45   35184372088832       1036\n10        50 1125899906842624       1276\n\n\nEs claro que el método secuencial ofrece ventajas en cuánto a la cantidad de modelos que se deben ajustar, sin embargo, es probable que el resultado de estos procedimientos no sea el modelo óptimo (¿por qué?)\n\n\n10.1.3 Combinación de métodos\nEn algunos casos (cuando hay muchas variables), incluso un método secuencial puede resultar poco atractivo para la selección de variables. Para estos casos, se puede establecer un límite, por ejemplo, si se usa selección hacia adelante, se pone una cota \\(L\\) al número de variables a incluir, siendo \\(L&lt;&lt;p\\). También es posible combinar (piense bien cómo sería eso) los métodos de selección hacia adelante y hacia atrás, esto con el fin de aumentar la probabilidad de llegar al modelo óptimo.",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Calibración del modelo lineal</span>"
    ]
  },
  {
    "objectID": "flex-lineal.html#métodos-tipo-penalización",
    "href": "flex-lineal.html#métodos-tipo-penalización",
    "title": "10  Controlando la flexibilidad del modelo de regresión lineal",
    "section": "10.2 Métodos tipo penalización",
    "text": "10.2 Métodos tipo penalización\nUn enfoque alternativo a la selección directa de variables, para el control de la flexibilidad del modelo lineal, es la penalización. La idea es considerar el modelo completo con las \\(p\\) variables y encoger el efecto de dichas variables a través de una penalización. Recuerde que el método de mínimos cuadrados, resuelve el problema de optimización\n\\[\n\\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\beta} \\in \\mathbb{R}^{p+1}}{\\arg\\min} \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_j x_{ij}\\right)^2\n\\] Replanteando el problema como\n\\[\n\\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\beta} \\in \\mathbb{R}^{p+1}}{\\arg\\min} \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_j x_{ij}\\right)^2 + Penalización\n\\]\nAl agregar un término de penalización se espera un aumento del sesgo del modelo (¿por qué?) pero también una reducción significativa de la varianza (de hecho, ese es el truco u objetivo al agregar penalizaciones) por lo que al final, considerando la combinación de ambos efectos (¿cuáles?), podríamos tener una reducción en el error esperado de predicción de nuestro modelo.\nHay muchas formas de agregar la \\(Penalización\\) Veamos las 2 más comunes\n\n10.2.1 Regresión Ridge\nSe habla de Ridge Regression cuando la penalización en el problema de optimización se plantea con la suma de los coeficientes al cuadradro, es decir:\n\\[\n\\hat{\\boldsymbol{\\beta}}_{Ridge} = \\underset{\\boldsymbol{\\beta} \\in \\mathbb{R}^{p+1}}{\\arg\\min} \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_j x_{ij}\\right)^2 + \\lambda\\sum_{j=1}^{p}\\beta_j^2 \\hspace{2cm} \\lambda \\geq0\n\\]\nAlgunos apuntes sobre este enfoque\n\nLa penalización no incluye a \\(\\beta_0\\) ya que la idea es penalizar el efecto de las variables, y no hay variable asociada al intercetpo.\ncuando \\(\\lambda=0\\) el resultado es mínimos cuadrados ordinarios (el de toda la vida)\nLa solución al problema de optimización se puede expresar de forma cerrada3 \\[\\hat{\\boldsymbol{\\beta}}_{Ridge} = \\left( \\boldsymbol{X^TX} + \\lambda \\boldsymbol{I}\\right)^{-1} \\boldsymbol{X^TY}\\] Aquí debe hacerse más claro aún que cuando \\(\\lambda=0\\) se tiene el método de mínimos cuadrados ordinarios (¿por qué?).\n\n3 Bono para quien lo haga y lo explique¿Qué se ha ganado realmente?\nYa no es necesario seleccionar variables. En algunos casos, aunque la selección de variables se haga de forma metódica, no hay relaciones causales bien definidas en el modelo obtenido (¿cómo así, eso qué significa?).\n¿Qué debemos hacer entonces?\nHemos cambiado el problema de seleccionar variables a seleccionar un valor de \\(\\lambda\\) que sea un trade-off entre sesgo y varianza del modelo. Esto es lo que se conoce como calibración del modelo o selección del tuning parameter. Una forma de hacerlo es usando CV. Se selecciona un conjunto de valores para \\(\\lambda\\), por ejemplo: \\(\\lambda_1, \\lambda_2, \\lambda_3, \\cdots, \\lambda_m\\) se calcula el \\(MSE\\) con CV para cada caso y se busca el valor de \\(\\lambda\\) con mínimo \\(MSE\\) (asegúrese de entender bien este procedimiento, por ejemplo, ¿qué sería usar 10-fold cv para 15 valores de \\(\\lambda\\)?)\n\n\n10.2.2 Regresión Lasso\nUn modelo Lasso se obtiene al plantear la penalización con el valor absoluto:\n\\[\n\\hat{\\boldsymbol{\\beta}}_{Lasso} = \\underset{\\boldsymbol{\\beta} \\in \\mathbb{R}^{p+1}}{\\arg\\min} \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_j x_{ij}\\right)^2 + \\lambda\\sum_{j=1}^{p}|{\\beta_j}| \\hspace{2cm} \\lambda \\geq0\n\\]\nPara este caso no hay solución cerrada para el problema de optimización, pero no es problema, existen algoritmos numéricos muy eficientes para hacer la tarea.\nTenga en cuenta el otro punto de vista para estos dos modelos: Ridge y Lasso. El que considera la función de mínimos cuadrados ordinaria pero agregando una restricción.4\n4 Una o varias preguntas del examen se referirán a la interpretación de esto",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Calibración del modelo lineal</span>"
    ]
  },
  {
    "objectID": "flex-lineal.html#reducción-de-la-dimensionalidad",
    "href": "flex-lineal.html#reducción-de-la-dimensionalidad",
    "title": "10  Controlando la flexibilidad del modelo de regresión lineal",
    "section": "10.3 Reducción de la dimensionalidad 5",
    "text": "10.3 Reducción de la dimensionalidad 5\n\n10.3.1 Regresión por componentes principales (PCR)\n\n\n10.3.2 Cuadrados mínimos parciales (PLS)",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Calibración del modelo lineal</span>"
    ]
  },
  {
    "objectID": "flex-lineal.html#cargamos-las-librerías-necesarias",
    "href": "flex-lineal.html#cargamos-las-librerías-necesarias",
    "title": "10  Controlando la flexibilidad del modelo de regresión lineal",
    "section": "11.1 Cargamos las librerías necesarias",
    "text": "11.1 Cargamos las librerías necesarias\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(ggcorrplot)\nlibrary(broom)\n\n\n\n\n\n\n\nBroom\n\n\n\nLa librería broom trae funciones que permiten convertir diversos objetos de R en data.frame. Es muy útil para trabajar con el tidyverse",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Calibración del modelo lineal</span>"
    ]
  },
  {
    "objectID": "flex-lineal.html#breve-descripción-de-la-base-de-datos",
    "href": "flex-lineal.html#breve-descripción-de-la-base-de-datos",
    "title": "10  Controlando la flexibilidad del modelo de regresión lineal",
    "section": "11.2 Breve descripción de la base de datos",
    "text": "11.2 Breve descripción de la base de datos\nLa base de datos contiene información sobre diferentes características físicas y químicas de más de 1000 muestras de vino.\n\ndatos &lt;- read.csv(\"WineQT.csv\", header = T) |&gt; as_tibble()\nglimpse(datos)\n\nRows: 1,143\nColumns: 13\n$ fixed.acidity        &lt;dbl&gt; 7.4, 7.8, 7.8, 11.2, 7.4, 7.4, 7.9, 7.3, 7.8, 6.7…\n$ volatile.acidity     &lt;dbl&gt; 0.700, 0.880, 0.760, 0.280, 0.700, 0.660, 0.600, …\n$ citric.acid          &lt;dbl&gt; 0.00, 0.00, 0.04, 0.56, 0.00, 0.00, 0.06, 0.00, 0…\n$ residual.sugar       &lt;dbl&gt; 1.9, 2.6, 2.3, 1.9, 1.9, 1.8, 1.6, 1.2, 2.0, 1.8,…\n$ chlorides            &lt;dbl&gt; 0.076, 0.098, 0.092, 0.075, 0.076, 0.075, 0.069, …\n$ free.sulfur.dioxide  &lt;dbl&gt; 11, 25, 15, 17, 11, 13, 15, 15, 9, 15, 16, 9, 35,…\n$ total.sulfur.dioxide &lt;dbl&gt; 34, 67, 54, 60, 34, 40, 59, 21, 18, 65, 59, 29, 1…\n$ density              &lt;dbl&gt; 0.9978, 0.9968, 0.9970, 0.9980, 0.9978, 0.9978, 0…\n$ pH                   &lt;dbl&gt; 3.51, 3.20, 3.26, 3.16, 3.51, 3.51, 3.30, 3.39, 3…\n$ sulphates            &lt;dbl&gt; 0.56, 0.68, 0.65, 0.58, 0.56, 0.56, 0.46, 0.47, 0…\n$ alcohol              &lt;dbl&gt; 9.4, 9.8, 9.8, 9.8, 9.4, 9.4, 9.4, 10.0, 9.5, 9.2…\n$ quality              &lt;int&gt; 5, 5, 5, 6, 5, 5, 5, 7, 7, 5, 5, 5, 7, 6, 5, 5, 5…\n$ Id                   &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 13, 16, 19, 21…\n\nsummary(datos)\n\n fixed.acidity    volatile.acidity  citric.acid     residual.sugar  \n Min.   : 4.600   Min.   :0.1200   Min.   :0.0000   Min.   : 0.900  \n 1st Qu.: 7.100   1st Qu.:0.3925   1st Qu.:0.0900   1st Qu.: 1.900  \n Median : 7.900   Median :0.5200   Median :0.2500   Median : 2.200  \n Mean   : 8.311   Mean   :0.5313   Mean   :0.2684   Mean   : 2.532  \n 3rd Qu.: 9.100   3rd Qu.:0.6400   3rd Qu.:0.4200   3rd Qu.: 2.600  \n Max.   :15.900   Max.   :1.5800   Max.   :1.0000   Max.   :15.500  \n   chlorides       free.sulfur.dioxide total.sulfur.dioxide    density      \n Min.   :0.01200   Min.   : 1.00       Min.   :  6.00       Min.   :0.9901  \n 1st Qu.:0.07000   1st Qu.: 7.00       1st Qu.: 21.00       1st Qu.:0.9956  \n Median :0.07900   Median :13.00       Median : 37.00       Median :0.9967  \n Mean   :0.08693   Mean   :15.62       Mean   : 45.91       Mean   :0.9967  \n 3rd Qu.:0.09000   3rd Qu.:21.00       3rd Qu.: 61.00       3rd Qu.:0.9978  \n Max.   :0.61100   Max.   :68.00       Max.   :289.00       Max.   :1.0037  \n       pH          sulphates         alcohol         quality     \n Min.   :2.740   Min.   :0.3300   Min.   : 8.40   Min.   :3.000  \n 1st Qu.:3.205   1st Qu.:0.5500   1st Qu.: 9.50   1st Qu.:5.000  \n Median :3.310   Median :0.6200   Median :10.20   Median :6.000  \n Mean   :3.311   Mean   :0.6577   Mean   :10.44   Mean   :5.657  \n 3rd Qu.:3.400   3rd Qu.:0.7300   3rd Qu.:11.10   3rd Qu.:6.000  \n Max.   :4.010   Max.   :2.0000   Max.   :14.90   Max.   :8.000  \n       Id      \n Min.   :   0  \n 1st Qu.: 411  \n Median : 794  \n Mean   : 805  \n 3rd Qu.:1210  \n Max.   :1597  \n\n\n\n11.2.1 Algunas gráficas y resúmenes interesantes\nObservemos una gráfica de la correlación entre cada par de variables, omitiendo ID\n\ndatos |&gt; select(!Id) |&gt; cor() |&gt; \n  ggcorrplot()\n\n\n\n\n\n\n\n\nGraficamos la variable densidad contra cada una de las variables:\n\nacidez fija\nalcohol\nacidez volátil\n\n\np1 &lt;- datos |&gt; ggplot(aes(x = fixed.acidity,\n                          y = density)) +\n  geom_point(color = \"#FF7857\") +\n  geom_smooth(method = \"lm\")\n\np2 &lt;- datos |&gt; ggplot(aes(x = alcohol,\n                          y = density)) +\n  geom_point(color = \"#A480E7\") +\n  geom_smooth(method = \"lm\")\n\np3 &lt;- datos |&gt; ggplot(aes(x = volatile.acidity,\n                          y = density)) +\n  geom_point(color = \"green\") +\n  geom_smooth(method = \"lm\")\n\n(p1|p2)/p3\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Calibración del modelo lineal</span>"
    ]
  },
  {
    "objectID": "flex-lineal.html#validación-cruzada",
    "href": "flex-lineal.html#validación-cruzada",
    "title": "10  Controlando la flexibilidad del modelo de regresión lineal",
    "section": "11.3 Validación cruzada",
    "text": "11.3 Validación cruzada\n\n11.3.1 Ajsute de modelos lineales\nSeleccionamos solo las 3 variables que hemos decidido trabajar y las renombramos usano funciones de dplyr\n\ndatos_sub &lt;- datos |&gt; select(densidad = density,\n                             acidez.f = fixed.acidity,\n                             acidez.v = volatile.acidity,\n                             alcohol)\n\nAjustamos un modelo simple para cada variable considerada\n\nmod1 &lt;- lm(densidad ~ acidez.f, data = datos_sub)\nsummary(mod1)\n\n\nCall:\nlm(formula = densidad ~ acidez.f, data = datos_sub)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0064269 -0.0007442  0.0000533  0.0009530  0.0055416 \n\nCoefficients:\n              Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) 0.99049119 0.00020268 4886.97 &lt;0.0000000000000002 ***\nacidez.f    0.00075071 0.00002387   31.46 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.001409 on 1141 degrees of freedom\nMultiple R-squared:  0.4644,    Adjusted R-squared:  0.464 \nF-statistic: 989.5 on 1 and 1141 DF,  p-value: &lt; 0.00000000000000022\n\ntidy(summary(mod1))\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) 0.990    0.000203     4887.  0        \n2 acidez.f    0.000751 0.0000239      31.5 6.65e-157\n\n\nObserve la diferencia cuando se aplica la función tidy del paquete broom. Es bastante útil para usar combinando con el operador pipe\nAjustamos los dos modelos restantes\n\nmod2 &lt;- lm(densidad ~ alcohol, data = datos_sub)\nmod3 &lt;- lm(densidad ~ acidez.v, data = datos_sub)\n\nVerificoamos el \\(R^2\\) de cada modelo\n\nr2_1 &lt;- summary(mod1)$r.squared\nr2_2 &lt;- summary(mod2)$r.squared\nr2_3 &lt;- summary(mod3)$r.squared\nr2_1\n\n[1] 0.4644435\n\nr2_2\n\n[1] 0.2447547\n\nr2_3\n\n[1] 0.0002726303\n\n\n\n\n11.3.2 Validación cruzada\nQueremos ver cómo funciona el método de validación cruzada para comparar modelos, recuerde que esto no es necesario para un modelo de regresión lineal pero lo haremos en el contexto de modelos lineales por la familiaridad del tema\nEstablecemos unos valores para la cantidad de particiones\n\nset.seed(22)\nk_fold &lt;- 10\nn &lt;- nrow(datos_sub)\ncelda &lt;- ceiling(n/k_fold)\n\nEvaluamos o calculamos el \\(MSE\\) para el modelo 1. Para esto, creamos una partición aleatoria de los datos y creamos un vector para guardar el \\(MSE\\) de cada iteración\n\npermutacion &lt;- sample(n)\nmse &lt;- vector(mode = \"numeric\", length = k_fold)\nfor(i in 1:k_fold){\n  test &lt;- permutacion[((i-1)*celda + 1):(min(celda*i, n))]\n  datos_test &lt;- datos_sub[test,]\n  datos_train &lt;- datos_sub[-test,]\n  modelo &lt;- lm(densidad ~ acidez.f, data = datos_train)\n  mse[i] &lt;- mean((datos_test$densidad - \n                    predict(modelo, newdata = datos_test))^2)\n}\nmse_1 &lt;- mean(mse)\n\nHacemos lo propio para los 2 modelos restantes\nModelo con la variable alcohol\n\npermutacion &lt;- sample(n)\nmse &lt;- vector(mode = \"numeric\", length = k_fold)\nfor(i in 1:k_fold){\n  test &lt;- permutacion[((i-1)*celda + 1):(min(celda*i, n))]\n  datos_test &lt;- datos_sub[test,]\n  datos_train &lt;- datos_sub[-test,]\n  modelo &lt;- lm(densidad ~ alcohol, data = datos_train)\n  mse[i] &lt;- mean((datos_test$densidad - \n                    predict(modelo, newdata = datos_test))^2)\n}\nmse_2 &lt;- mean(mse)\n\nModelo con la variable acidez volátil\n\npermutacion &lt;- sample(n)\nmse &lt;- vector(mode = \"numeric\", length = k_fold)\nfor(i in 1:k_fold){\n  test &lt;- permutacion[((i-1)*celda + 1):(min(celda*i, n))]\n  datos_test &lt;- datos_sub[test,]\n  datos_train &lt;- datos_sub[-test,]\n  modelo &lt;- lm(densidad ~ acidez.v, data = datos_train)\n  mse[i] &lt;- mean((datos_test$densidad - \n                    predict(modelo, newdata = datos_test))^2)\n}\nmse_3 &lt;- mean(mse)\n\nComparamos el resultado de cada modelo\n\nmse_1\n\n[1] 0.000001992099\n\nmse_2\n\n[1] 0.000002803126\n\nmse_3\n\n[1] 0.000003718297\n\n\nOrganizamos los resultados\n\nresultados &lt;- tibble(\n  Modelo = c(\"Acidez Fija\", \"Alcohol\", \"Acidez Volátil\"),\n  R2 = c(r2_1, r2_2, r2_3),\n  MSE_cv = c(mse_1, mse_2, mse_3)\n)\nresultados \n\n# A tibble: 3 × 3\n  Modelo               R2     MSE_cv\n  &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;\n1 Acidez Fija    0.464    0.00000199\n2 Alcohol        0.245    0.00000280\n3 Acidez Volátil 0.000273 0.00000372\n\nglimpse(resultados)\n\nRows: 3\nColumns: 3\n$ Modelo &lt;chr&gt; \"Acidez Fija\", \"Alcohol\", \"Acidez Volátil\"\n$ R2     &lt;dbl&gt; 0.4644434531, 0.2447547038, 0.0002726303\n$ MSE_cv &lt;dbl&gt; 0.000001992099, 0.000002803126, 0.000003718297\n\n\n\n\n\n\n\n\nEjercicio para la casa, para el aburrimiento\n\n\n\n\nProgramar en R una función que reciba el parámetro k_fold, la base de datos y las variables y devuelva el MSE de validación cruzada\nUse la función del punto 1 para calcular LOOCV de los modelos resueltos en clase\nHacer validación cruzada usando librerías de R. Investigue\n\n\n\nAplicamos LOOCV para el modelo con acidez fija\n\nk_fold &lt;- n\npermutacion &lt;- sample(n)\nmse &lt;- vector(mode = \"numeric\", length = k_fold)\nfor(i in 1:k_fold){\n  # test &lt;- permutacion[((i-1)*celda + 1):(min(celda*i, n))]\n  datos_test &lt;- datos_sub[i,]\n  datos_train &lt;- datos_sub[-i,]\n  modelo &lt;- lm(densidad ~ acidez.f, data = datos_train)\n  mse[i] &lt;- mean((datos_test$densidad - \n                    predict(modelo, newdata = datos_test))^2)\n}\nmse_1_loocv &lt;- mean(mse)\nmse_1_loocv\n\n[1] 0.000001990474\n\n\nAhora usamos directamente las librerías de R para facilitar el trabajo de evaluación de los modelos. Usaremos caret\n\nlibrary(caret)\n\nCargando paquete requerido: lattice\n\n\n\nAdjuntando el paquete: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\ntrain_control &lt;- trainControl(method = \"LOOCV\")\nmod1.caret &lt;- train(densidad ~ acidez.f, data = datos_sub,\n                    method = \"lm\", trControl = train_control)\nmod1.caret\n\nLinear Regression \n\n1143 samples\n   1 predictor\n\nNo pre-processing\nResampling: Leave-One-Out Cross-Validation \nSummary of sample sizes: 1142, 1142, 1142, 1142, 1142, 1142, ... \nResampling results:\n\n  RMSE         Rsquared   MAE        \n  0.001410841  0.4624204  0.001073912\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nVeamos los resultados del ajuste y evluación del modelo\n\nmod1.caret$results\n\n  intercept        RMSE  Rsquared         MAE\n1      TRUE 0.001410841 0.4624204 0.001073912\n\n\nExtraemos el MSE (por LOOCV) del modelo\n\nmse.loocv.caret &lt;- mod1.caret$results[2]^2\nmse.loocv.caret\n\n            RMSE\n1 0.000001990474\n\n\n\n\n11.3.3 Matriz de proyección\nLa matriz de proyección es la que convierte los valores observados en predicciones. Cuando los modelos tienen esta propiedad, se suelen llamar suavizadores lineales. El modelo de regresión lineal es uno de ellos.\nLa matriz de proyección también se llama: matriz sombrero (hat), matriz de influencia. Los elementos de la diagonal principal de esta matriz son los llamados leverages y se pueden interpretar como la influencia individual de cada punto en las predicciones. Algunas formas de analizar datos atípicos revisan estos leverages.\nCalculemos la matriz de proyección para el modelo de acidez fija\n\nmod_ac.f &lt;- lm(densidad ~ acidez.f, data = datos_sub)\n\nmx &lt;- cbind(rep(1, n), datos_sub$acidez.f)\nmh &lt;- mx%*%solve(t(mx)%*%mx)%*%t(mx) \ntraza &lt;- sum(diag(mh))\nmse_loocv_mh &lt;- mean((mod_ac.f$residuals/(1-diag(mh)))^2)\nmse_loocv_mh\n\n[1] 0.000001990474\n\n\nPodemos hacer esto mismo de una forma un poco más sencilla\n\nleverage.mod_ac.f &lt;-  lm.influence(mod_ac.f)$hat\nsum(leverage.mod_ac.f)\n\n[1] 2\n\nmse_loocv_mh_r &lt;- mean((mod_ac.f$residuals/(1-leverage.mod_ac.f))^2)\nmse_loocv_mh_r\n\n[1] 0.000001990474",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Calibración del modelo lineal</span>"
    ]
  },
  {
    "objectID": "flex-lineal.html#selección-de-variables-1",
    "href": "flex-lineal.html#selección-de-variables-1",
    "title": "10  Controlando la flexibilidad del modelo de regresión lineal",
    "section": "11.4 Selección de variables",
    "text": "11.4 Selección de variables\ncomo siempre, hay varias alternativas y niveles de detalle para trabajar el problema de selección de variables. La función básica para construir modelos lineales es lm(). Podemos programar algoritmos para hacer selección de variables, pero haremos uso directo de la librería olsrr\n\nlibrary(olsrr)\n\n\nAdjuntando el paquete: 'olsrr'\n\n\nThe following object is masked from 'package:datasets':\n\n    rivers\n\n\nEsta librería es para facilitar el trabajo con mínimos cuadrados ordinarios (OLS: Ordinary Least Squares). Para un estudio más detallado de las funciones y opciones, visite la documentación\nVolvamos a trabajar con todas las variables\n\ndatos_sub2 &lt;- datos |&gt; select(\n  densidad = density,\n acidez.f = fixed.acidity,\n acidez.v = volatile.acidity,\n acido.c = citric.acid,\n azucar = residual.sugar,\n cloruro = chlorides,\n sulfuro.l = free.sulfur.dioxide,\n sulfuro.t = total.sulfur.dioxide,\n ph = pH,\n sulfato = sulphates,\n alcohol,\n calidad = quality\n)\ndatos_sub2\n\n# A tibble: 1,143 × 12\n   densidad acidez.f acidez.v acido.c azucar cloruro sulfuro.l sulfuro.t    ph\n      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1    0.998      7.4     0.7     0       1.9   0.076        11        34  3.51\n 2    0.997      7.8     0.88    0       2.6   0.098        25        67  3.2 \n 3    0.997      7.8     0.76    0.04    2.3   0.092        15        54  3.26\n 4    0.998     11.2     0.28    0.56    1.9   0.075        17        60  3.16\n 5    0.998      7.4     0.7     0       1.9   0.076        11        34  3.51\n 6    0.998      7.4     0.66    0       1.8   0.075        13        40  3.51\n 7    0.996      7.9     0.6     0.06    1.6   0.069        15        59  3.3 \n 8    0.995      7.3     0.65    0       1.2   0.065        15        21  3.39\n 9    0.997      7.8     0.58    0.02    2     0.073         9        18  3.36\n10    0.996      6.7     0.58    0.08    1.8   0.097        15        65  3.28\n# ℹ 1,133 more rows\n# ℹ 3 more variables: sulfato &lt;dbl&gt;, alcohol &lt;dbl&gt;, calidad &lt;int&gt;\n\n\nDebemos verificar la información de cada variable, hacer algunos resúmenes y algunas gráficas de forma marginal, antes de ver la posible relación con otras variables. Esto incluye estudiar los metadatos que acompañan a la base de datos6. Tomemos por ejemplo la variable azúcar\n6 Aquí normalmente, viene información relevante del tipo de varibles, unidades, y otros apuntes importantes\ndatos_sub2 |&gt; ggplot(aes(x = azucar)) +\n  geom_histogram(bins = 40, fill = \"orange\", color = \"darkblue\") +\n  labs(\n    title = \"Niveles de Azúcar\",\n    subtitle = \"Datos sobre la calidad del vino, tomado de &lt;internet&gt;\",\n    x = \"Azúcar (unidades)\"\n  )\n\n\n\n\n\n\n\ndatos_sub2 |&gt; select(azucar) |&gt; summary()\n\n     azucar      \n Min.   : 0.900  \n 1st Qu.: 1.900  \n Median : 2.200  \n Mean   : 2.532  \n 3rd Qu.: 2.600  \n Max.   :15.500  \n\ndatos_sub2 &lt;- datos_sub2 |&gt; select(-calidad)\n\nAjustemos modelos lineales por mínimos cuadrados\n\nmod_lineal1 &lt;- ols_regress(\n  densidad ~ .,\n  data = datos_sub2\n)\nmodelo.lineal &lt;- lm(densidad ~ ., data = datos_sub2)\nmod_lineal1\n\n                          Model Summary                            \n------------------------------------------------------------------\nR                       0.921       RMSE                    0.001 \nR-Squared               0.848       MSE                     0.000 \nAdj. R-Squared          0.847       Coef. Var               0.076 \nPred R-Squared          0.844       AIC                -13183.269 \nMAE                     0.001       SBC                -13122.772 \n------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                ANOVA                                  \n----------------------------------------------------------------------\n               Sum of                                                 \n              Squares          DF    Mean Square       F         Sig. \n----------------------------------------------------------------------\nRegression      0.004          10          0.000    633.367    0.0000 \nResidual        0.001        1132          0.000                      \nTotal           0.004        1142                                     \n----------------------------------------------------------------------\n\n                                   Parameter Estimates                                     \n------------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta       t         Sig      lower     upper \n------------------------------------------------------------------------------------------\n(Intercept)     0.979         0.001                 1180.973    0.000     0.978     0.981 \n   acidez.f     0.001         0.000        0.851      42.367    0.000     0.001     0.001 \n   acidez.v     0.001         0.000        0.070       4.574    0.000     0.000     0.001 \n    acido.c     0.000         0.000        0.020       0.970    0.332     0.000     0.001 \n     azucar     0.000         0.000        0.308      25.144    0.000     0.000     0.000 \n    cloruro     0.001         0.001        0.036       2.517    0.012     0.000     0.003 \n  sulfuro.l     0.000         0.000       -0.041      -2.568    0.010     0.000     0.000 \n  sulfuro.t     0.000         0.000        0.026       1.575    0.116     0.000     0.000 \n         ph     0.005         0.000        0.417      24.057    0.000     0.005     0.006 \n    sulfato     0.001         0.000        0.117       8.665    0.000     0.001     0.002 \n    alcohol    -0.001         0.000       -0.530     -39.754    0.000    -0.001    -0.001 \n------------------------------------------------------------------------------------------\n\nnames(mod_lineal1)\n\n [1] \"r\"          \"rsq\"        \"adjr\"       \"rmse\"       \"cv\"        \n [6] \"mse\"        \"mae\"        \"aic\"        \"sbc\"        \"sbic\"      \n[11] \"prsq\"       \"error_df\"   \"model_df\"   \"total_df\"   \"ess\"       \n[16] \"rss\"        \"tss\"        \"rms\"        \"ems\"        \"f\"         \n[21] \"p\"          \"n\"          \"betas\"      \"sbetas\"     \"std_errors\"\n[26] \"tvalues\"    \"pvalues\"    \"df\"         \"conf_lm\"    \"title\"     \n[31] \"dependent\"  \"predictors\" \"mvars\"      \"model\"     \n\n\nPodemos, fácilmente, ajustar todos los posibles modelos de regresión (cuidado, es costoso computacionalmente)\n\nmod_todos1 &lt;- ols_step_all_possible(\n  model = modelo.lineal\n)\n# mod_todos1\nplot(mod_todos1)\n\nUna alternativa más prudente es establecer una cantidad máxima de variables a considerar, con el argumento max_order\n\nmod_todos2 &lt;- ols_step_all_possible(model = modelo.lineal, \n                                    max_order = 3)\nmod_todos2\n\n    Index N                   Predictors     R-Square Adj. R-Square\n1       1 1                     acidez.f 0.4644434531  0.4639740784\n10      2 1                      alcohol 0.2447547038  0.2440927885\n4       3 1                       azucar 0.1445117049  0.1437619343\n3       4 1                      acido.c 0.1408075045  0.1400544875\n8       5 1                           ph 0.1244499359  0.1236825826\n5       6 1                      cloruro 0.0436395069  0.0428013295\n9       7 1                      sulfato 0.0204888563  0.0196303891\n6       8 1                    sulfuro.l 0.0029322570  0.0020584027\n7       9 1                    sulfuro.t 0.0025175133  0.0016432955\n2      10 1                     acidez.v 0.0002726303 -0.0006035550\n19     11 2             acidez.f alcohol 0.6623186398  0.6617262164\n13     12 2              acidez.f azucar 0.5357408599  0.5349263702\n11     13 2            acidez.f acidez.v 0.5019112799  0.5010374400\n17     14 2                  acidez.f ph 0.4890093394  0.4881128646\n14     15 2             acidez.f cloruro 0.4829855522  0.4820785093\n16     16 2           acidez.f sulfuro.t 0.4804061772  0.4794946090\n12     17 2             acidez.f acido.c 0.4771973516  0.4762801540\n15     18 2           acidez.f sulfuro.l 0.4679231400  0.4669896718\n18     19 2             acidez.f sulfato 0.4650452520  0.4641067349\n34     20 2              acido.c alcohol 0.4298642551  0.4288640170\n40     21 2               azucar alcohol 0.4126489669  0.4116185265\n54     22 2                   ph alcohol 0.3060954767  0.3048781004\n55     23 2              sulfato alcohol 0.2811227043  0.2798615161\n45     24 2              cloruro alcohol 0.2543144355  0.2530062152\n27     25 2             acidez.v alcohol 0.2521814395  0.2508694771\n49     26 2            sulfuro.l alcohol 0.2507664467  0.2494520018\n52     27 2            sulfuro.t alcohol 0.2466640337  0.2453423916\n28     28 2               acido.c azucar 0.2426609411  0.2413322761\n38     29 2                    azucar ph 0.2408869920  0.2395552148\n20     30 2             acidez.v acido.c 0.2100181857  0.2086322527\n35     31 2               azucar cloruro 0.1777890940  0.1763466187\n32     32 2                   acido.c ph 0.1719321506  0.1704794000\n39     33 2               azucar sulfato 0.1631486660  0.1616805057\n36     34 2             azucar sulfuro.l 0.1585862452  0.1571100807\n29     35 2              acido.c cloruro 0.1553354230  0.1538535554\n37     36 2             azucar sulfuro.t 0.1450302561  0.1435303092\n21     37 2              acidez.v azucar 0.1448613235  0.1433610802\n31     38 2            acido.c sulfuro.t 0.1421298391  0.1406248037\n30     39 2            acido.c sulfuro.l 0.1418699053  0.1403644139\n33     40 2              acido.c sulfato 0.1412064800  0.1396998247\n43     41 2                   cloruro ph 0.1377803758  0.1362677098\n25     42 2                  acidez.v ph 0.1338703514  0.1323508257\n53     43 2                   ph sulfato 0.1307023617  0.1291772781\n50     44 2                 sulfuro.t ph 0.1253124242  0.1237778846\n47     45 2                 sulfuro.l ph 0.1252646252  0.1237300017\n44     46 2              cloruro sulfato 0.0485317806  0.0468625381\n41     47 2            cloruro sulfuro.l 0.0469284276  0.0452563722\n42     48 2            cloruro sulfuro.t 0.0452523418  0.0435773459\n22     49 2             acidez.v cloruro 0.0436620730  0.0419842872\n48     50 2            sulfuro.l sulfato 0.0239835399  0.0222712303\n26     51 2             acidez.v sulfato 0.0238871446  0.0221746659\n51     52 2            sulfuro.t sulfato 0.0226364416  0.0209217687\n46     53 2          sulfuro.l sulfuro.t 0.0160618641  0.0143356568\n23     54 2           acidez.v sulfuro.l 0.0032013902  0.0014526208\n24     55 2           acidez.v sulfuro.t 0.0026775061  0.0009278175\n76     56 3      acidez.f azucar alcohol 0.7523797950  0.7517275908\n90     57 3          acidez.f ph alcohol 0.7358250254  0.7351292177\n63     58 3    acidez.f acidez.v alcohol 0.6710520310  0.6701856184\n91     59 3     acidez.f sulfato alcohol 0.6677486859  0.6668735727\n88     60 3   acidez.f sulfuro.t alcohol 0.6638343940  0.6629489710\n81     61 3     acidez.f cloruro alcohol 0.6637316494  0.6628459557\n85     62 3   acidez.f sulfuro.l alcohol 0.6633524266  0.6624657342\n70     63 3     acidez.f acido.c alcohol 0.6626717631  0.6617832779\n57     64 3     acidez.f acidez.v azucar 0.5693228934  0.5681885376\n74     65 3           acidez.f azucar ph 0.5602166978  0.5590583573\n64     66 3      acidez.f acido.c azucar 0.5540850429  0.5529105523\n71     67 3      acidez.f azucar cloruro 0.5506439071  0.5494603529\n125    68 3       acido.c azucar alcohol 0.5463183914  0.5451234442\n73     69 3    acidez.f azucar sulfuro.t 0.5407493699  0.5395397545\n75     70 3      acidez.f azucar sulfato 0.5365238488  0.5353031039\n72     71 3    acidez.f azucar sulfuro.l 0.5357755236  0.5345528077\n79     72 3          acidez.f cloruro ph 0.5243105246  0.5230576111\n61     73 3         acidez.f acidez.v ph 0.5224886201  0.5212309079\n58     74 3    acidez.f acidez.v cloruro 0.5162749935  0.5150009153\n60     75 3  acidez.f acidez.v sulfuro.t 0.5154694739  0.5141932741\n86     76 3        acidez.f sulfuro.t ph 0.5140778190  0.5127979537\n62     77 3    acidez.f acidez.v sulfato 0.5073777029  0.5060801903\n59     78 3  acidez.f acidez.v sulfuro.l 0.5065128240  0.5052130333\n65     79 3     acidez.f acido.c cloruro 0.5052131015  0.5039098875\n56     80 3    acidez.f acidez.v acido.c 0.5020913794  0.5007799432\n67     81 3   acidez.f acido.c sulfuro.t 0.4982592795  0.4969377500\n68     82 3          acidez.f acido.c ph 0.4969816121  0.4956567173\n78     83 3   acidez.f cloruro sulfuro.t 0.4969757547  0.4956508445\n83     84 3        acidez.f sulfuro.l ph 0.4936129922  0.4922792248\n89     85 3          acidez.f ph sulfato 0.4905376019  0.4891957343\n77     86 3   acidez.f cloruro sulfuro.l 0.4859479926  0.4845940365\n80     87 3     acidez.f cloruro sulfato 0.4837047807  0.4823449162\n66     88 3   acidez.f acido.c sulfuro.l 0.4817445293  0.4803795018\n82     89 3 acidez.f sulfuro.l sulfuro.t 0.4814056683  0.4800397482\n69     90 3     acidez.f acido.c sulfato 0.4808374989  0.4794700823\n87     91 3   acidez.f sulfuro.t sulfato 0.4807516346  0.4793839918\n84     92 3   acidez.f sulfuro.l sulfato 0.4683530971  0.4669527980\n98     93 3     acidez.v acido.c alcohol 0.4593033288  0.4578791936\n154    94 3            azucar ph alcohol 0.4505187737  0.4490715010\n155    95 3       azucar sulfato alcohol 0.4471658030  0.4457096989\n149    96 3     azucar sulfuro.l alcohol 0.4347704545  0.4332817024\n137    97 3    acido.c sulfuro.t alcohol 0.4346066053  0.4331174216\n134    98 3    acido.c sulfuro.l alcohol 0.4328709996  0.4313772446\n140    99 3      acido.c sulfato alcohol 0.4327826856  0.4312886979\n130   100 3      acido.c cloruro alcohol 0.4303964061  0.4288961332\n139   101 3           acido.c ph alcohol 0.4299122150  0.4284106669\n152   102 3     azucar sulfuro.t alcohol 0.4297640058  0.4282620673\n104   103 3      acidez.v azucar alcohol 0.4205279255  0.4190016602\n145   104 3       azucar cloruro alcohol 0.4165499828  0.4150132400\n175   105 3           ph sulfato alcohol 0.3260185697  0.3242433771\n170   106 3         sulfuro.l ph alcohol 0.3092894279  0.3074701726\n173   107 3         sulfuro.t ph alcohol 0.3084026411  0.3065810502\n164   108 3           cloruro ph alcohol 0.3076877354  0.3058642615\n118   109 3          acidez.v ph alcohol 0.3063994315  0.3045725643\n92    110 3      acidez.v acido.c azucar 0.2954210821  0.2935652992\n171   111 3    sulfuro.l sulfato alcohol 0.2883577208  0.2864833338\n174   112 3    sulfuro.t sulfato alcohol 0.2838748996  0.2819887053\n119   113 3     acidez.v sulfato alcohol 0.2825096393  0.2806198490\n165   114 3      cloruro sulfato alcohol 0.2815903335  0.2796981219\n123   115 3            acido.c azucar ph 0.2710142725  0.2690942047\n109   116 3     acidez.v cloruro alcohol 0.2619100788  0.2599660316\n159   117 3    cloruro sulfuro.l alcohol 0.2603959510  0.2584479157\n113   118 3   acidez.v sulfuro.l alcohol 0.2583531363  0.2563997205\n162   119 3    cloruro sulfuro.t alcohol 0.2562678794  0.2543089713\n120   120 3       acido.c azucar cloruro 0.2550496035  0.2530874866\n116   121 3   acidez.v sulfuro.t alcohol 0.2537974870  0.2518320721\n143   122 3            azucar cloruro ph 0.2512531017  0.2492809852\n121   123 3     acido.c azucar sulfuro.l 0.2509485225  0.2489756038\n168   124 3  sulfuro.l sulfuro.t alcohol 0.2508769407  0.2489038334\n102   125 3           acidez.v azucar ph 0.2489828843  0.2470047883\n147   126 3          azucar sulfuro.l ph 0.2489564478  0.2469782821\n153   127 3            azucar ph sulfato 0.2473751372  0.2453928066\n124   128 3       acido.c azucar sulfato 0.2438177722  0.2418260718\n122   129 3     acido.c azucar sulfuro.t 0.2432297923  0.2412365433\n150   130 3          azucar sulfuro.t ph 0.2420785130  0.2400822316\n96    131 3          acidez.v acido.c ph 0.2321909249  0.2301686008\n93    132 3     acidez.v acido.c cloruro 0.2137158265  0.2116448409\n97    133 3     acidez.v acido.c sulfato 0.2127435482  0.2106700018\n94    134 3   acidez.v acido.c sulfuro.l 0.2105088975  0.2084294653\n95    135 3   acidez.v acido.c sulfuro.t 0.2100507939  0.2079701551\n141   136 3     azucar cloruro sulfuro.l 0.1920210686  0.1898929415\n144   137 3       azucar cloruro sulfato 0.1831981143  0.1810467485\n128   138 3           acido.c cloruro ph 0.1801649123  0.1780055574\n142   139 3     azucar cloruro sulfuro.t 0.1786443146  0.1764809545\n148   140 3     azucar sulfuro.l sulfato 0.1782943766  0.1761300948\n99    141 3      acidez.v azucar cloruro 0.1778585507  0.1756931211\n135   142 3         acido.c sulfuro.t ph 0.1727261476  0.1705471997\n132   143 3         acido.c sulfuro.l ph 0.1725034279  0.1703238935\n138   144 3           acido.c ph sulfato 0.1722917263  0.1701116343\n103   145 3      acidez.v azucar sulfato 0.1665895580  0.1643944471\n151   146 3     azucar sulfuro.t sulfato 0.1638275927  0.1616252071\n146   147 3   azucar sulfuro.l sulfuro.t 0.1637091467  0.1615064491\n100   148 3    acidez.v azucar sulfuro.l 0.1589313283  0.1567160464\n126   149 3    acido.c cloruro sulfuro.l 0.1566511980  0.1544299105\n127   150 3    acido.c cloruro sulfuro.t 0.1563291046  0.1541069688\n129   151 3      acido.c cloruro sulfato 0.1557254095  0.1535016836\n131   152 3  acido.c sulfuro.l sulfuro.t 0.1479043252  0.1456599995\n101   153 3    acidez.v azucar sulfuro.t 0.1454543563  0.1432035776\n117   154 3          acidez.v ph sulfato 0.1451306781  0.1428790469\n107   155 3          acidez.v cloruro ph 0.1447016428  0.1424488816\n136   156 3    acido.c sulfuro.t sulfato 0.1425066084  0.1402480657\n133   157 3    acido.c sulfuro.l sulfato 0.1423478348  0.1400888739\n163   158 3           cloruro ph sulfato 0.1395509907  0.1372846632\n157   159 3         cloruro sulfuro.l ph 0.1388590769  0.1365909269\n160   160 3         cloruro sulfuro.t ph 0.1384337246  0.1361644543\n111   161 3        acidez.v sulfuro.l ph 0.1345855016  0.1323060956\n114   162 3        acidez.v sulfuro.t ph 0.1342864943  0.1320063007\n169   163 3         sulfuro.l ph sulfato 0.1317553892  0.1294685290\n172   164 3         sulfuro.t ph sulfato 0.1314913012  0.1292037454\n166   165 3       sulfuro.l sulfuro.t ph 0.1295059879  0.1272132030\n156   166 3  cloruro sulfuro.l sulfuro.t 0.0577604435  0.0552786888\n158   167 3    cloruro sulfuro.l sulfato 0.0520773349  0.0495806115\n161   168 3    cloruro sulfuro.t sulfato 0.0500915565  0.0475896028\n108   169 3     acidez.v cloruro sulfato 0.0493570977  0.0468532094\n105   170 3   acidez.v cloruro sulfuro.l 0.0469494792  0.0444392496\n106   171 3   acidez.v cloruro sulfuro.t 0.0452553471  0.0427406553\n167   172 3  sulfuro.l sulfuro.t sulfato 0.0369313852  0.0343947690\n112   173 3   acidez.v sulfuro.l sulfato 0.0274364093  0.0248747844\n115   174 3   acidez.v sulfuro.t sulfato 0.0255958950  0.0230294224\n110   175 3 acidez.v sulfuro.l sulfuro.t 0.0160808558  0.0134893216\n     Mallow's Cp\n1    0.462417881\n10   0.241387167\n4    0.140929865\n3    0.137658610\n8    0.120844743\n5    0.038181558\n9    0.017392261\n6   -0.001242004\n7   -0.001815567\n2   -0.003237354\n19   0.660147872\n13   0.532514579\n11   0.498795933\n17   0.485579622\n14   0.479409257\n16   0.476145127\n12   0.474060157\n15   0.464318876\n18   0.462141085\n34   0.426238322\n40   0.407685460\n54   0.301436889\n55   0.276048963\n45   0.248809275\n27   0.247646323\n49   0.245674152\n52   0.241996182\n28   0.237761598\n38   0.235572044\n20   0.205085575\n35   0.171984904\n32   0.167116074\n39   0.158664601\n36   0.153634512\n29   0.148374687\n37   0.138444178\n21   0.139799766\n31   0.135849149\n30   0.136440138\n33   0.136759986\n43   0.130883095\n25   0.128757296\n53   0.125299413\n50   0.118664008\n47   0.119296314\n44   0.041458662\n41   0.039327328\n42   0.037524484\n22   0.036516046\n48   0.018517329\n26   0.018922311\n51   0.017059924\n46   0.008806660\n23  -0.002716248\n24  -0.003442441\n76   0.749530020\n90   0.733271045\n63   0.668247077\n91   0.664793183\n88   0.660618997\n81   0.660673772\n85   0.660163093\n70   0.659828770\n57   0.565379181\n74   0.555789141\n64   0.549867194\n71   0.546601229\n125  0.541499255\n73   0.535014822\n75   0.532602025\n72   0.531688711\n79   0.519538560\n61   0.518078520\n58   0.511848489\n60   0.510477427\n86   0.508732422\n62   0.503359546\n59   0.501990114\n65   0.500866756\n56   0.497977567\n67   0.493138024\n68   0.492506947\n78   0.491401026\n83   0.488647882\n89   0.486078703\n77   0.481048571\n80   0.479247996\n66   0.477085323\n82   0.475348881\n69   0.476748085\n87   0.475636991\n84   0.463867372\n98   0.454381287\n154  0.444556369\n155  0.441015263\n149  0.428987136\n137  0.429644518\n134  0.427741115\n140  0.427767831\n130  0.424923282\n139  0.425058079\n152  0.423409075\n104  0.414472769\n145  0.410553604\n175  0.319255856\n170  0.302895686\n173  0.302228453\n164  0.301021738\n118  0.300510816\n92   0.289245993\n171  0.281705662\n174  0.277724651\n119  0.276232799\n165  0.274232435\n123  0.264622763\n109  0.255178118\n159  0.253194891\n113  0.252097538\n162  0.249507441\n120  0.247645446\n116  0.247908352\n143  0.243852099\n121  0.244735841\n168  0.244352731\n102  0.242516054\n147  0.242247199\n153  0.240725292\n124  0.237800992\n122  0.234956900\n150  0.233512101\n96   0.225682103\n93   0.205541674\n97   0.206328861\n94   0.203534578\n95   0.202381560\n141  0.185035225\n144  0.176133208\n128  0.171812375\n142  0.170137318\n148  0.172536801\n99   0.170629706\n135  0.164657437\n132  0.165386267\n138  0.166011843\n103  0.160621204\n151  0.156527226\n146  0.154498671\n100  0.152528852\n126  0.147585957\n127  0.146484222\n129  0.147454093\n131  0.138936751\n101  0.137339327\n117  0.137818597\n107  0.136504986\n136  0.134929093\n133  0.135620186\n163  0.130904254\n157  0.129773727\n160  0.128722255\n111  0.127170187\n114  0.126260129\n169  0.124032893\n172  0.123094224\n166  0.120094281\n156  0.047390681\n158  0.042858431\n161  0.040767649\n108  0.040487813\n105  0.037664175\n106  0.035797000\n167  0.028348030\n112  0.020152280\n115  0.018212730\n110  0.007031116\n\nplot(mod_todos2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPodemos solicitar directamente la selección de modelos\n\nmod_todos3 &lt;- ols_step_best_subset(\n  model = modelo.lineal, \n  max_order = 5,\n  metric = \"aic\"\n)\nmod_todos3\nplot(mod_todos3)\n\nDe igual forma, podemos hacer selección tipo forward y backward\n\nmod_todos_f &lt;- ols_step_forward_adj_r2(\n  model = modelo.lineal\n)\nmod_todos_f\n\n\n                                  Stepwise Summary                                  \n----------------------------------------------------------------------------------\nStep    Variable         AIC           SBC           SBIC         R2       Adj. R2 \n----------------------------------------------------------------------------------\n 0      Base Model    -11047.195    -11037.112    -14294.017    0.00000    0.00000 \n 1      acidez.f      -11758.940    -11743.816    -15006.510    0.46444    0.46397 \n 2      alcohol       -12284.096    -12263.930    -15531.667    0.66232    0.66173 \n 3      azucar        -12636.662    -12611.455    -15883.701    0.75238    0.75173 \n 4      ph            -13055.656    -13025.407    -16300.435    0.82867    0.82807 \n 5      sulfato       -13145.133    -13109.843    -16389.211    0.84185    0.84116 \n 6      acidez.v      -13174.465    -13134.133    -16418.228    0.84613    0.84531 \n 7      cloruro       -13180.810    -13135.437    -16424.467    0.84725    0.84630 \n 8      sulfuro.l     -13182.748    -13132.334    -16426.338    0.84777    0.84670 \n 9      sulfuro.t     -13184.320    -13128.864    -16427.836    0.84825    0.84704 \n----------------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                          Model Summary                            \n------------------------------------------------------------------\nR                       0.921       RMSE                    0.001 \nR-Squared               0.848       MSE                     0.000 \nAdj. R-Squared          0.847       Coef. Var               0.076 \nPred R-Squared          0.844       AIC                -13184.320 \nMAE                     0.001       SBC                -13128.864 \n------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                ANOVA                                  \n----------------------------------------------------------------------\n               Sum of                                                 \n              Squares          DF    Mean Square       F         Sig. \n----------------------------------------------------------------------\nRegression      0.004           9          0.000    703.673    0.0000 \nResidual        0.001        1133          0.000                      \nTotal           0.004        1142                                     \n----------------------------------------------------------------------\n\n                                   Parameter Estimates                                     \n------------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta       t         Sig      lower     upper \n------------------------------------------------------------------------------------------\n(Intercept)     0.979         0.001                 1182.937    0.000     0.978     0.981 \n   acidez.f     0.001         0.000        0.861      51.217    0.000     0.001     0.001 \n    alcohol    -0.001         0.000       -0.527     -40.720    0.000    -0.001    -0.001 \n     azucar     0.000         0.000        0.308      25.232    0.000     0.000     0.000 \n         ph     0.005         0.000        0.416      24.040    0.000     0.005     0.006 \n    sulfato     0.001         0.000        0.117       8.711    0.000     0.001     0.002 \n   acidez.v     0.001         0.000        0.062       4.790    0.000     0.000     0.001 \n    cloruro     0.002         0.001        0.040       2.963    0.003     0.001     0.003 \n  sulfuro.l     0.000         0.000       -0.043      -2.728    0.006     0.000     0.000 \n  sulfuro.t     0.000         0.000        0.031       1.883    0.060     0.000     0.000 \n------------------------------------------------------------------------------------------\n\nplot1 &lt;- plot(mod_todos_f)\n\n\n\n\n\n\n\nplot1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEjercicio para reducir el estrés\n\n\n\nGraficar directamente con ggplot2 y mejorar los resultados gráficos\n\n\nUna alternativa es convertir a un gráfico interactivo con plotly\n\nlibrary(plotly)\n\n\nAdjuntando el paquete: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nggplotly(plot1)\n\n\n\n\n\nCon selección tipo backward, obtenemos\n\nmod_todos4 &lt;- ols_step_backward_adj_r2(modelo.lineal)\nmod_todos4\n\n\n                                  Stepwise Summary                                  \n----------------------------------------------------------------------------------\nStep    Variable         AIC           SBC           SBIC         R2       Adj. R2 \n----------------------------------------------------------------------------------\n 0      Full Model    -13183.269    -13122.772    -16426.749    0.84837    0.84703 \n 1      acido.c       -13184.320    -13128.864    -16427.837    0.84825    0.84704 \n----------------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                          Model Summary                            \n------------------------------------------------------------------\nR                       0.921       RMSE                    0.001 \nR-Squared               0.848       MSE                     0.000 \nAdj. R-Squared          0.847       Coef. Var               0.076 \nPred R-Squared          0.844       AIC                -13184.320 \nMAE                     0.001       SBC                -13128.864 \n------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                ANOVA                                  \n----------------------------------------------------------------------\n               Sum of                                                 \n              Squares          DF    Mean Square       F         Sig. \n----------------------------------------------------------------------\nRegression      0.004           9          0.000    703.673    0.0000 \nResidual        0.001        1133          0.000                      \nTotal           0.004        1142                                     \n----------------------------------------------------------------------\n\n                                   Parameter Estimates                                     \n------------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta       t         Sig      lower     upper \n------------------------------------------------------------------------------------------\n(Intercept)     0.979         0.001                 1182.937    0.000     0.978     0.981 \n   acidez.f     0.001         0.000        0.861      51.217    0.000     0.001     0.001 \n   acidez.v     0.001         0.000        0.062       4.790    0.000     0.000     0.001 \n     azucar     0.000         0.000        0.308      25.232    0.000     0.000     0.000 \n    cloruro     0.002         0.001        0.040       2.963    0.003     0.001     0.003 \n  sulfuro.l     0.000         0.000       -0.043      -2.728    0.006     0.000     0.000 \n  sulfuro.t     0.000         0.000        0.031       1.883    0.060     0.000     0.000 \n         ph     0.005         0.000        0.416      24.040    0.000     0.005     0.006 \n    sulfato     0.001         0.000        0.117       8.711    0.000     0.001     0.002 \n    alcohol    -0.001         0.000       -0.527     -40.720    0.000    -0.001    -0.001 \n------------------------------------------------------------------------------------------\n\nplot2 &lt;- plot(mod_todos4)\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n\n\nplot2\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n\n\nggplotly(plot2)",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Calibración del modelo lineal</span>"
    ]
  },
  {
    "objectID": "no-lineales.html",
    "href": "no-lineales.html",
    "title": "11  Modelos no lineales",
    "section": "",
    "text": "12 Modelos no lineales en los predictores\nVeamos ahora el control de flexibilidad del modelo desde el punto de vista del aumento de la familia de funciones consideradas y no del número de variables involucradas.",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modelos no lineales</span>"
    ]
  },
  {
    "objectID": "no-lineales.html#regresión-polinómica",
    "href": "no-lineales.html#regresión-polinómica",
    "title": "11  Modelos no lineales",
    "section": "12.1 Regresión polinómica",
    "text": "12.1 Regresión polinómica\nConsideremos un modelo lineal en una sola variable \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\), al pensar en aumentar la flexibilidad del modelo para abordar problemas más complejos, podemos pensar en agregar términos cuadráticos, cúbicos y polinómicos de alto grado:\n\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1 x_{i} + \\beta_2 x_{i}^2 +...+ \\beta_p x_{i}^p + \\epsilon_i\\)\n\nEstos modelos pueden llamarse modelos cuadráticos, cúbicos; y en general: modelos polinómicos. Las ecuaciones son no lineales en \\(x\\) pero siguen siendo modelos lineales en los parámetros, por lo que se resuelven de la forma habitual usando cuadrados mínimos ordinarios (OLS)\nPodemos agregar tantos términos al polinomio como sea necesario para obtener un modelo bastante flexible, pero este enfoque tiene muchos inconvenientes y suele evitarse en la práctica. Para ver por qué, hagamos algunas simulaciones.\n\n12.1.1 Datos simulados\nConsideremos la función\n\\[f(x)=2 + 1.5 \\mathrm{sen}(2x) + 0.07x^2\\]\nY un ruido aleatorio \\(\\varepsilon_i\\) con una distribución normal estándar. Los datos se generan mediante la ecuación:\n\\[y_i=2 + 1.5\\mathbb{sen}(2x_i) + 0.07x_i^2 + \\varepsilon_i\\]\n\nlibrary(tidyverse)\nlibrary(plotly)\n\nset.seed(789)\nn &lt;- 200\nf &lt;- function(x) 2 + 1.5*sin(2*x) + 0.07*x^2\nx &lt;- runif(n, 0, 8)\nerror = rnorm(n)\ny &lt;- f(x) + error\n\ndatos &lt;- tibble(y = y, x = x)\n\nplot1 &lt;- datos |&gt; ggplot(aes(x,y)) + \n  geom_point() +\n  geom_function(fun = f, colour = \"red\", linewidth = 1) +\n  labs(title = \"Función y datos simulados\")\nplot1\n\n\n\n\n\n\n\n\nEn la gráfica, podemos ver la función \\(f(x)\\) que debemos estimar a partir de los datos\n\n\n12.1.2 Extrapolación\nVamos a resaltar primero, lo deficiente y poco recomendable que es usar regresión polinómica para extrapolar predicciones fuera del rango de valores observados (esto no solo es cierto para regresión polinómica sino para la mayoría de métodos). Para hacerlo, usemos solo la parte interna de los datos y miremos cómo se comporta más allá de este intervalo al ajustar polinomios de grados superiores. Para facilitar la tarea, usamos (entre otras cosas) la función poly para crear de forma rápida los valores de los polinomios requeridos para el ajuste. Veamos un pequeño ejemplo\n\nxx &lt;- c(1,2,3,4)\npoly(x = xx, degree = 3)\n\n              1    2          3\n[1,] -0.6708204  0.5 -0.2236068\n[2,] -0.2236068 -0.5  0.6708204\n[3,]  0.2236068 -0.5 -0.6708204\n[4,]  0.6708204  0.5  0.2236068\nattr(,\"coefs\")\nattr(,\"coefs\")$alpha\n[1] 2.5 2.5 2.5\n\nattr(,\"coefs\")$norm2\n[1] 1.0 4.0 5.0 4.0 1.8\n\nattr(,\"degree\")\n[1] 1 2 3\nattr(,\"class\")\n[1] \"poly\"   \"matrix\"\n\n\nPor defecto genera polinomios ortogonales, para evitarlo usamos el argumento raw = T\n\npoly(xx, 3, raw = T, simple = T)\n\n     1  2  3\n[1,] 1  1  1\n[2,] 2  4  8\n[3,] 3  9 27\n[4,] 4 16 64\n\n\nCon esto, podemos ajustar modelos polinómicos de forma más sencilla\n\nlibrary(randomcoloR) # para generar colores de forma aleatoria\nlibrary(patchwork)\npg &lt;- floor(n/5):floor(4*n/5)\ndatos_sub &lt;- datos |&gt; arrange(x) |&gt; slice(pg)\n\nplot2 &lt;- plot1 + geom_point(data = datos_sub, mapping = aes(x, y),\n                   colour = \"blue\")\n\nfor(i in 3:6){\n  ajuste.lineal &lt;- lm(y ~ poly(x, i, raw = T),\n                      data = datos_sub)\n  y_pred &lt;- predict(ajuste.lineal, newdata = datos)\n  plot3 &lt;- plot2 + geom_line(aes(x=x, y=y_pred), size=1, colour = randomColor())\n  print(plot3)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalice las gráficas y concluya al respecto.\n\n\n12.1.3 Varianza de los modelos\nPodemos evaluar el desempeño de un polinomio de alto grado desde otro punto de vista: Cómo cambia la estimación al cambiar los datos\n\n\nd &lt;- 20\nfor(i in 1:4){\n  x &lt;- runif(n, 0, 8)\n  error = rnorm(n)\n  y &lt;- f(x) + error\n  assign(paste0(\"data\", i), tibble(x = x, y = y))\n  modelo.lineal &lt;- lm(y ~ poly(x, degree = d, raw = T))\n  assign(paste0(\"y_pred\",i), predict(modelo.lineal))\n}\n\nplot4 &lt;- ggplot() +\n  geom_function(fun = f, color = \"red\", linewidth = 1) +\n  geom_line(aes(x, y = y_pred1), data = data1,\n            linewidth = 0.8, color = randomColor()) +\n  geom_line(aes(x, y = y_pred2), data = data2, \n            linewidth = 0.8, color = randomColor()) +\n  geom_line(aes(x, y = y_pred3), data = data3, \n            linewidth = 0.8, color = randomColor()) +\n  geom_line(aes(x, y = y_pred4), data = data4, \n            linewidth = 0.8, color = randomColor())\n# plot4\nggplotly(plot4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Qué puede decir sobre estos resultados?",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modelos no lineales</span>"
    ]
  },
  {
    "objectID": "no-lineales.html#regression-splines",
    "href": "no-lineales.html#regression-splines",
    "title": "11  Modelos no lineales",
    "section": "12.2 Regression Splines",
    "text": "12.2 Regression Splines\nAsí que aumentar la flexibilidad del modelo a través del grado del polinomio no es el camino a seguir. En su lugar podemos ajustar varios modelos sencillos en subintervalos.\nEste enfoque nos trae de vuelta a un problema expresado en bases funcionales (¿por qué se da eso? explique), donde debemos estimar unos parámetros lineales (entonces, ¿cuál es la ventaja de estos métodos?) para resolver el modelo.\nLas bases dependerán del tipo de modelo ajustado y de la cantidad y posición de los nodos. Como el objetivo es facilitar la tarea, por lo común, los nodos o puntos de corte se ponen de forma uniforme en todo el intervalo (piense en qué situaciones podría ser ventajoso hacerlo de otra manera).1\n1 Bono para quien deduzca las bases funcionales de un cubic spline (ver ejercicio 1 del cap 7 del libro guía)La flexibilidad del modelo resultante se puede controlar de 2 formas:\n\nVariando la complejidad del modelo ajustado en cada subintervalo.\nVariando la cantidad de nodos.\n\nPero el lema es “Ajustar varios modelos sencillos”, entonces lo más recomendado (y lo que mejor funciona en la práctica) es variar la cantidad de nodos. Cuando estos modelos sencillos son polinomios de bajo grado y tienen ciertas restricciones en los puntos de corte (¿cuáles son esas restricciones o condiciones?) se obtienen los modelos conocidos como regression splines\n\n12.2.1 Simulaciones\nVeamos cómo resulta en las simulaciones. Para facilitar la tarea, usaremos el paquete splines en R\n\nlibrary(splines)\n\nx &lt;- runif(n, 0, 8)\nerror = rnorm(n)\ny &lt;- f(x) + error\n\ndatos &lt;- tibble(y = y, x = x)\n\nplot1 &lt;- datos |&gt; ggplot(aes(x,y)) + \n  geom_point() +\n  geom_function(fun = f, colour = \"red\", linewidth = 1) +\n  labs(title = \"Función y datos simulados\")\nplot1\n\n\n\n\n\n\n\nd &lt;- 4\n# con 3 nodos\nnodos &lt;- seq(0, 8, length.out = 5)\nnodos &lt;- nodos[-c(1,length(nodos))]\nd_spline &lt;- lm(y ~ bs(x, knots = nodos, degree = d))\nds_pred1 &lt;- predict(object = d_spline)\n\n# con 1 nodo\nnodos &lt;- seq(0, 8, length.out = 3)\nnodos &lt;- nodos[-c(1,length(nodos))]\nd_spline &lt;- lm(y ~ bs(x, knots = nodos, degree = d))\nds_pred2 &lt;- predict(object = d_spline)\n\n# con 20 nodos\nnodos &lt;- seq(0, 8, length.out = 22)\nnodos &lt;- nodos[-c(1,length(nodos))]\nd_spline &lt;- lm(y ~ bs(x, knots = nodos, degree = d))\nds_pred3 &lt;- predict(object = d_spline)\n\nplot5 &lt;- plot1 +\n  geom_line(aes(x = x, y = ds_pred1), linewidth = 0.5, color = \"purple\") +\n  geom_line(aes(x = x, y = ds_pred2), linewidth = 0.5, color = \"orange\") +\n  geom_line(aes(x = x, y = ds_pred3), linewidth = 0.5, color = \"blue\")\n\nplot5\n\n\n\n\n\n\n\nggplotly(plot5)\n\n\n\n\n\n\n\n\n\n\n\nEjercicio para la casa para reducir los niveles de cortisol\n\n\n\ntomar un vector de cantidad de puntos de corte (nodos) y elegir el mejor valor con validación cruzada.",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modelos no lineales</span>"
    ]
  },
  {
    "objectID": "ssplines.html",
    "href": "ssplines.html",
    "title": "12  Smoothing Splines",
    "section": "",
    "text": "12.1 Modelo simulado\nConsidere el siguiente conjunto de datos simulados a partir de un modelo del tipo\n\\[\ny_i = f(x_i) + \\varepsilon_i \\hspace{2cm} i=1,...,n\n\\]\nDonde \\(\\varepsilon_i\\) tiene una distribución normal \\(\\varepsilon_i \\sim \\mathrm{N} (0, \\sigma^2)\\)",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>_Smoothing Splines_</span>"
    ]
  },
  {
    "objectID": "ssplines.html#modelo-simulado",
    "href": "ssplines.html#modelo-simulado",
    "title": "12  Smoothing Splines",
    "section": "",
    "text": "Intente aproximar una forma funcional adecuada para estos datos\n¿Cuántas opciones tiene disponible?\n¿Cómo saber si la elección es adecuada o no?",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>_Smoothing Splines_</span>"
    ]
  },
  {
    "objectID": "ssplines.html#agreguemos-algo-de-información",
    "href": "ssplines.html#agreguemos-algo-de-información",
    "title": "12  Smoothing Splines",
    "section": "12.2 Agreguemos algo de información",
    "text": "12.2 Agreguemos algo de información\nEs muy difícil hacerse una idea de la forma funcional real solo con la gráfica anterior.\nSuponga que \\(\\varepsilon_i\\) tiene una distribución normal con media 0 y desviación 0.2.\nAdemás, se muestra la función real en color azul.\n\n\n\n\n\n\n\n¿Ahora sí?\n¿es polinómica?, ¿es trigonométrica, exponencial?, ¿es racional?, ¿Es alguna combinación de funciones elementales?\nEstá bien, sigue siendo difícil, miremos la ecuación de la función con la que se simularon los datos",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>_Smoothing Splines_</span>"
    ]
  },
  {
    "objectID": "ssplines.html#forma-funcional",
    "href": "ssplines.html#forma-funcional",
    "title": "12  Smoothing Splines",
    "section": "12.3 Forma funcional",
    "text": "12.3 Forma funcional\nLa función real a partir de la cual se simularon los datos tiene la forma\n\\[f(x) = \\frac{\\left( \\cos(ae^{bx}) \\right) ^2}{c\\sqrt{x}+d}\\]\n\nAhora sí está fácil, ¿o no?, tenemos la forma funcional. Solo debemos encontrar los valores de a, b, c y d.\n¿Cómo estimamos los parámetros?\n¿Mínimos cuadrados, máxima verosimilitud?\nHay una dificultad, el modelo planteado no es lineal en los parámetros, la función de mínimos cuadrados ya no tendrá esa forma sencilla que podíamos incluso resolver de forma matricial (¿se acuerdan de aquella solución elegante \\(\\hat{\\boldsymbol{\\beta}} = (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{Y}\\)?)\nNi qué hablar de máxima verosimilitud, de por sí ya es más complicado de manejar numéricamente que el método de mínimos cuadrados.\nLa forma real es: \\[f(x) = \\frac{\\left( \\cos(2.5e^{1.1x}) \\right) ^2}{2.1\\sqrt{x}+ e/\\pi}\\]\n¡Tampoco estaba tan difícil!\n\n\n12.3.1 Haga la simulación en su computador\n\nlibrary(tidyverse)\nlibrary(plotly)\nset.seed(1005)\nn &lt;- 500\nx &lt;- runif(n, 0, 1.5)\nf &lt;- function(x) (cos(2.5*exp(1.1*x))^2/(2.1*sqrt(x)+exp(1)/pi))\nerror &lt;- rnorm(n, 0, 0.2)\ny &lt;- f(x) + error\ndatos &lt;- tibble(\n  x = x,\n  y = y\n)\nplot1 &lt;- datos |&gt; ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  geom_function(fun = f, color = \"blue\",\n                linewidth = 1)\nggplotly(plot1)",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>_Smoothing Splines_</span>"
    ]
  },
  {
    "objectID": "ssplines.html#métodos-de-regularización",
    "href": "ssplines.html#métodos-de-regularización",
    "title": "12  Smoothing Splines",
    "section": "12.4 Métodos de regularización",
    "text": "12.4 Métodos de regularización\n\n12.4.1 Características\n\nEn la práctica, es muy difícil encontrar una forma funcional a priori adecuada para un modelo. Algunas veces contamos con suficiente evidencia empírica o teórica (por ejemplo, la popular función de Cobb Douglas para modelos econométricos) para respaldar nuestra elección, pero estos casos son más bien rarezas o los supuestos y simplificaciones son poco convenientes en algunos contextos.\nHemos visto algunas alternativas que intentan evitar esta difícil elección, como los regression splines. Sin embargo, seguimos en la necesidad de parametrizar nuestros modelos aunque ahora lo hagamos de manera más sencilla y en pequeños intervalos.\nLos regression splines necesitan establecer un conjunto de knots para controlar la flexibilidad del modelo. Se puede abordar el problema de regresión desde un punto de vista más general, una familia de métodos no paramétricos. Los llamados modelos de regularización plantean problemas de optimización sobre espacios funcionales bastante generales y controlan la flexiblidad del modelo a través de penalizaciones\n\n\n\n12.4.2 Planteamiento matemático\nMatemáticamente, se puede escribir como\n\\[\n\\hat{f}(x) = \\underset{f \\in \\mathcal{H}}{\\arg \\min} \\left\\{ \\sum_{i=1}^{n} L(y_i, f(x_i)) + \\lambda J(f) \\right\\}\n\\]\nEsto es un problema bastante general y podemos comentar algunas cosas sin entrar en el detalle matemático:\n\n\\(\\mathcal{H}\\) Es un espacio funcional de infinitas dimensiones (entiéndase como un conjunto muy grande de funciones).\n\\(L(y_i, f(x_i))\\) es una función de pérdida y mide qué tan bien se ajusta la función estimada a los datos.\n\\(J(f)\\) Es una penalización funcional definida sobre el espacio \\(\\mathcal{H}\\). Este término en el problema mide qué tan flexible es la función estimada.\n\\(\\lambda \\geq0\\) es el tuning parameter y es el que controla el trade-off entre sesgo y varianza del modelo. A medida que aumenta el valor de \\(\\lambda\\) el sesgo aumenta y la varianza disminuye (¿por qué?)\nCuando \\(\\lambda  = 0\\), la solución \\(\\hat{f}\\) es cualquier función que interpole los datos. El espacio de funciones es tan grande que siempre podemos encontrar una función que haga cero la función objetivo (¿cómo se llama eso?)",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>_Smoothing Splines_</span>"
    ]
  },
  {
    "objectID": "ssplines.html#smoothing-spline",
    "href": "ssplines.html#smoothing-spline",
    "title": "12  Smoothing Splines",
    "section": "12.5 Smoothing Spline",
    "text": "12.5 Smoothing Spline\nUn Smoothing Spline, es un caso particular del problema más general planteado anteriormente con las siguientes características:\n\nEl espacio \\(\\mathcal{H}\\) Es un espacio de funciones con segundas derivadas continuas y con norma finita, es decir, un espacio de Sobolev de orden 2 (piense en una función que no cumpla este requisito).\nLa función de pérdida es la de mínimos cuadrados.\nEl término de penalización está expresado en términos de la segunda derivada.\nBajo estas condiciones, el problema de regularización se reduce a: \\[\n\\hat{f}(x) = \\underset{f \\in \\mathcal{H}}{\\arg \\min} \\left\\{ \\sum_{i=1}^{n} \\left( y_i - f(x_i) \\right)^2 + \\lambda \\int (f''(t))^2dt \\right\\}\\] Y es conocido como Smoothing Splines\n\n\n12.5.1 ¿Cómo es la solución para un smoothing spline?\nEl problema sigue siendo bastante general, pero…\n\nSe puede hacer uso de algunas propiedades del espacio \\(\\mathcal{H}\\). Resulta que es un espacio de Hilbert con Kernel reproducible. ¿Cómo así? 😪\nEl kernel trick nos permite expresar la solución en términos de un número finito de bases (parámetros) ¡un resultado bastante impresionante!\nDe esa manera ya podemos optimizar y encontrar nuestra \\(\\hat{f}\\)\nComo siempre \\(\\lambda\\) debe ser calibrado para ajustar la flexibilidad del modelo. Es posible hacerlo con CV\n¿qué pasa si \\(\\lambda=0\\)?\n¿y si \\(\\lambda \\to \\infty\\)?\n\n\n\n12.5.2 Calibración\nComo se mencionó, debemos controlar la flexibilidad de la estimación para evitar overfitting. En este caso, debemos encontrar el valor de \\(\\lambda\\) que minimiza el MSE calculado con CV. Hay varios paquetes disponibles en R para hacer la tarea. Usaremos gam que viene de Generalized Additive Models 🥱\n\n\n12.5.3 Un modelo muy flexible: \\(\\lambda = 0.1\\)\n\n# install.packages(\"gam\")\nlibrary(gam)\nlambda &lt;- 0.1\nmod &lt;- gam(formula = y ~ s(x, spar = lambda), data = datos)\ny_pred &lt;- mod$fitted.values\nsmooth1 &lt;- plot1 + geom_line(aes(x = x, y = y_pred),\n                             color =  \"red\", linewidth = 1)\nggplotly(smooth1)\n\n\n\n\n\n\n\n12.5.4 Un modelo poco flexible: \\(\\lambda = 2\\)\n\nlambda &lt;- 2\nmod &lt;- gam(formula = y ~ s(x, spar = lambda), data = datos)\ny_pred &lt;- mod$fitted.values\nsmooth2 &lt;- plot1 + geom_line(aes(x = x, y = y_pred),\n                             color =  \"red\", linewidth = 1)\nggplotly(smooth2)\n\n\n\n\n\n\n\n12.5.5 Un modelo calibrado (a ojo): \\(\\lambda = 0.7\\)\n\nlambda &lt;- 0.7\nmod &lt;- gam(formula = y ~ s(x, spar = lambda), data = datos)\ny_pred &lt;- mod$fitted.values\nsmooth3 &lt;- plot1 + geom_line(aes(x = x, y = y_pred),\n                             color =  \"red\", linewidth = 1)\nggplotly(smooth3)\n\n\n\n\n\n\n\n\n\n\n\n\nEjercicio en casa para reducir la fatiga emocional debida al ocio\n\n\n\nImplemente una función que calibre el valor de la pelnalización a través de validación cruzada",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>_Smoothing Splines_</span>"
    ]
  },
  {
    "objectID": "locales.html",
    "href": "locales.html",
    "title": "13  Suavizadores locales",
    "section": "",
    "text": "13.1 Kernel Smoothing\nOtra gran familia de modelos predictivos son los conocidos métodos de suavizado con kernel. A diferencia de los modelos vistos hasta ahora, estos predicen de forma local. Caen dentro de los métodos no paramétricos. Ya hemos visto uno de estos algoritmos: KNN.",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Suavizadores locales</span>"
    ]
  },
  {
    "objectID": "locales.html#kernel-smoothing",
    "href": "locales.html#kernel-smoothing",
    "title": "13  Suavizadores locales",
    "section": "",
    "text": "Recordemos el algoritmo de \\(k\\) vecinos más cercanos \\[\\hat{f}(x_0)=\\frac{1}{k} \\sum_{i:x_i \\in \\mathcal{N}_k(x_0)} y_i\\]\nLa predicción se hace punto a punto (de forma local).\n\\(\\mathcal{N}_k(x_0)\\) Es el vecindario del punto \\(x_0\\), es decir, el conjunto de los \\(k\\) puntos más cercanos a \\(x_0\\).\nLa predicción es sencillamente el promedio (de la variable \\(y\\) claro está) para aquellos \\(k\\) puntos más cercanos a \\(x_0\\).\nSencillo, pero aún así muy efectivo para algunos casos.",
    "crumbs": [
      "Ciencia de datos",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Suavizadores locales</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referencias",
    "section": "",
    "text": "Alexander, Rohan. 2023. Telling Stories with\nData. CRC Press. https://tellingstorieswithdata.com/.\n\n\nEmaasit, Daniel. n.d. “Ggplot2 Extensions.” https://exts.ggplot2.tidyverse.org/gallery/.\n\n\nGarcía-Portugués, Eduardo. n.d.a. Notes for\nNonparametric Statistics. https://bookdown.org/egarpor/NP-UC3M/.\n\n\n———. n.d.b. Notes for Predictive\nModeling. https://bookdown.org/egarpor/PM-UC3M/.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The\nElements of Statistical\nLearning. Springer Series in\nStatistics. New York, NY: Springer. https://doi.org/10.1007/978-0-387-84858-7.\n\n\nHealy, Conor, and Yan Holtz. n.d. “From Data to Viz\n Find the Graphic You Need.” https://www.data-to-viz.com/data-to-viz.com.\n\n\nHealy, Kieran. 2018. Data Visualization. A\nPractical Introduction. Princeton\nUniversity Press. https://socviz.co/index.html#preface.\n\n\nHoltz, Yan. n.d. “The R Graph\nGallery – Help and Inspiration for\nR Charts.” The R Graph Gallery. https://r-graph-gallery.com/index.html.\n\n\nHuang, Shuai, and Houtao Deng. n.d. Data Analytics:\nA Small Data\nApproach. CRC Press. https://dataanalyticsbook.info/index.html#cover.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2021. An Introduction to Statistical\nLearning: With Applications in\nR. 2nd ed. Springer Texts in\nStatistics. New York, NY: Springer US. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nKabacoff, Rob. n.d. Data Visualization with\nR. https://rkabacoff.github.io/datavis/.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. n.d.\nR for Data Science (2e). 2nd ed.\nO’REILLY. https://r4ds.hadley.nz/.",
    "crumbs": [
      "Referencias"
    ]
  }
]